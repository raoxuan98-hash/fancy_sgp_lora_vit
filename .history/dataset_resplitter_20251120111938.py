#!/usr/bin/env python3
"""
Cross-Domain数据集重新划分工具
将每个类别的样本数量限制到128，确保数据集平衡
"""

import os
import sys
import json
import shutil
import logging
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict, Counter
from pathlib import Path
import random

# 添加项目根目录到Python路径
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils.data1 import get_dataset
from utils.cross_domain_data_manager import CrossDomainDataManagerCore


class MetadataManager:
    """元数据管理器，记录原始分布和采样过程"""
    
    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.metadata_dir = self.output_dir / "metadata"
        self.metadata_dir.mkdir(parents=True, exist_ok=True)
        
        self.original_distribution = {}
        self.balanced_distribution = {}
        self.sampling_config = {}
        self.dataset_statistics = {}
        
    def record_original_distribution(self, dataset_name: str, 
                                   train_counts: Dict[int, int], 
                                   test_counts: Dict[int, int],
                                   class_names: List[str]):
        """记录原始数据分布"""
        self.original_distribution[dataset_name] = {
            'train_counts': train_counts,
            'test_counts': test_counts,
            'class_names': class_names,
            'total_train_samples': sum(train_counts.values()),
            'total_test_samples': sum(test_counts.values()),
            'num_classes': len(class_names)
        }
    
    def record_balanced_distribution(self, dataset_name: str,
                                  train_counts: Dict[int, int],
                                  test_counts: Dict[int, int],
                                  sampling_info: Dict[str, Any]):
        """记录平衡后的数据分布"""
        self.balanced_distribution[dataset_name] = {
            'train_counts': train_counts,
            'test_counts': test_counts,
            'total_train_samples': sum(train_counts.values()),
            'total_test_samples': sum(test_counts.values()),
            'sampling_info': sampling_info
        }
    
    def record_sampling_config(self, config: Dict[str, Any]):
        """记录采样配置"""
        self.sampling_config = config
    
    def record_dataset_statistics(self, stats: Dict[str, Any]):
        """记录数据集统计信息"""
        self.dataset_statistics = stats
    
    def save_all_metadata(self):
        """保存所有元数据到文件"""
        # 保存原始分布
        with open(self.metadata_dir / "original_distribution.json", 'w') as f:
            json.dump(self.original_distribution, f, indent=2)
        
        # 保存平衡后分布
        with open(self.metadata_dir / "balanced_distribution.json", 'w') as f:
            json.dump(self.balanced_distribution, f, indent=2)
        
        # 保存采样配置
        with open(self.metadata_dir / "sampling_config.json", 'w') as f:
            json.dump(self.sampling_config, f, indent=2)
        
        # 保存统计信息
        with open(self.metadata_dir / "dataset_statistics.json", 'w') as f:
            json.dump(self.dataset_statistics, f, indent=2)
        
        logging.info(f"元数据已保存到 {self.metadata_dir}")


class DatasetResplitter:
    """数据集重新划分器"""
    
    def __init__(self, 
                 max_samples_per_class: int = 128,
                 seed: int = 42,
                 output_dir: str = "balanced_datasets"):
        """
        初始化数据集重新划分器
        
        Args:
            max_samples_per_class: 每个类别的最大样本数
            seed: 随机种子
            output_dir: 输出目录
        """
        self.max_samples_per_class = max_samples_per_class
        self.seed = seed
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # 设置随机种子
        random.seed(seed)
        np.random.seed(seed)
        
        # 初始化元数据管理器
        self.metadata_manager = MetadataManager(str(self.output_dir))
        
        # 记录采样配置
        self.metadata_manager.record_sampling_config({
            'max_samples_per_class': max_samples_per_class,
            'seed': seed,
            'output_dir': str(output_dir)
        })
        
        # 设置日志
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        
    def resplit_single_dataset(self, dataset_name: str) -> Dict[str, Any]:
        """
        重新划分单个数据集
        
        Args:
            dataset_name: 数据集名称
            
        Returns:
            采样信息字典
        """
        logging.info(f"开始处理数据集: {dataset_name}")
        
        # 获取数据集
        try:
            dataset = get_dataset(dataset_name)
        except Exception as e:
            logging.error(f"无法加载数据集 {dataset_name}: {str(e)}")
            return {}
        
        # 获取原始数据
        train_data = getattr(dataset, 'train_data', None)
        train_targets = getattr(dataset, 'train_targets', None)
        test_data = getattr(dataset, 'test_data', None)
        test_targets = getattr(dataset, 'test_targets', None)
        class_names = getattr(dataset, 'class_names', [])
        use_path = getattr(dataset, 'use_path', False)
        
        if any(x is None for x in [train_data, train_targets, test_data, test_targets]):
            logging.error(f"数据集 {dataset_name} 缺少必要属性")
            return {}
        
        # 统计原始分布
        train_counts = self._count_samples_by_class(train_targets)
        test_counts = self._count_samples_by_class(test_targets)
        
        # 记录原始分布
        self.metadata_manager.record_original_distribution(
            dataset_name, train_counts, test_counts, class_names
        )
        
        # 执行平衡采样
        balanced_train_data, balanced_train_targets, balanced_test_data, balanced_test_targets, sampling_info = self._balance_dataset(
            train_data, train_targets, test_data, test_targets, len(class_names)
        )
        
        # 统计平衡后分布
        balanced_train_counts = self._count_samples_by_class(balanced_train_targets)
        balanced_test_counts = self._count_samples_by_class(balanced_test_targets)
        
        # 记录平衡后分布
        self.metadata_manager.record_balanced_distribution(
            dataset_name, balanced_train_counts, balanced_test_counts, sampling_info
        )
        
        # 保存平衡后的数据集
        self._save_balanced_dataset(
            dataset_name,
            balanced_train_data, balanced_train_targets,
            balanced_test_data, balanced_test_targets,
            class_names, use_path
        )
        
        logging.info(f"数据集 {dataset_name} 处理完成")
        return sampling_info
    
    def _count_samples_by_class(self, targets: np.ndarray) -> Dict[int, int]:
        """统计每个类别的样本数量"""
        counts = Counter(targets)
        return dict(counts)
    
    def _balance_dataset(self, 
                        train_data: np.ndarray, train_targets: np.ndarray,
                        test_data: np.ndarray, test_targets: np.ndarray,
                        num_classes: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, Any]]:
        """
        平衡数据集
        
        Returns:
            (balanced_train_data, balanced_train_targets, balanced_test_data, balanced_test_targets, sampling_info)
        """
        balanced_train_data = []
        balanced_train_targets = []
        balanced_test_data = []
        balanced_test_targets = []
        
        sampling_info = {
            'classes_processed': 0,
            'classes_with_insufficient_samples': [],
            'samples_moved_from_train_to_test': defaultdict(int),
            'final_train_counts': {},
            'final_test_counts': {}
        }
        
        for class_id in range(num_classes):
            # 获取当前类别的训练和测试样本索引
            train_indices = np.where(train_targets == class_id)[0]
            test_indices = np.where(test_targets == class_id)[0]
            
            train_count = len(train_indices)
            test_count = len(test_indices)
            total_count = train_count + test_count
            
            logging.debug(f"类别 {class_id}: 训练样本={train_count}, 测试样本={test_count}, 总计={total_count}")
            
            if test_count >= self.max_samples_per_class:
                # 测试样本足够，直接采样
                sampled_test_indices = np.random.choice(
                    test_indices, 
                    size=self.max_samples_per_class, 
                    replace=False
                )
                balanced_test_data.extend(test_data[sampled_test_indices])
                balanced_test_targets.extend([class_id] * self.max_samples_per_class)
                
                # 训练样本处理
                if train_count >= self.max_samples_per_class:
                    sampled_train_indices = np.random.choice(
                        train_indices, 
                        size=self.max_samples_per_class, 
                        replace=False
                    )
                    balanced_train_data.extend(train_data[sampled_train_indices])
                    balanced_train_targets.extend([class_id] * self.max_samples_per_class)
                else:
                    # 训练样本不足，保留所有
                    balanced_train_data.extend(train_data[train_indices])
                    balanced_train_targets.extend([class_id] * train_count)
                    
            else:
                # 测试样本不足，需要从训练集中补充
                needed_samples = self.max_samples_per_class - test_count
                
                if train_count >= needed_samples:
                    # 训练样本足够补充
                    # 随机选择需要移动到测试集的训练样本
                    move_indices = np.random.choice(train_indices, size=needed_samples, replace=False)
                    balanced_test_data.extend(test_data[test_indices])
                    balanced_test_data.extend(train_data[move_indices])
                    balanced_test_targets.extend([class_id] * self.max_samples_per_class)
                    
                    # 记录移动的样本数
                    sampling_info['samples_moved_from_train_to_test'][class_id] = needed_samples
                    
                    # 剩余训练样本
                    remaining_train_indices = np.setdiff1d(train_indices, move_indices)
                    if len(remaining_train_indices) >= self.max_samples_per_class:
                        sampled_remaining = np.random.choice(
                            remaining_train_indices, 
                            size=self.max_samples_per_class, 
                            replace=False
                        )
                        balanced_train_data.extend(train_data[sampled_remaining])
                        balanced_train_targets.extend([class_id] * self.max_samples_per_class)
                    else:
                        balanced_train_data.extend(train_data[remaining_train_indices])
                        balanced_train_targets.extend([class_id] * len(remaining_train_indices))
                        
                else:
                    # 即使所有训练样本都移动到测试集也不够
                    balanced_test_data.extend(test_data[test_indices])
                    balanced_test_data.extend(train_data[train_indices])
                    balanced_test_targets.extend([class_id] * total_count)
                    
                    # 记录样本不足的类别
                    sampling_info['classes_with_insufficient_samples'].append({
                        'class_id': class_id,
                        'total_samples': total_count,
                        'desired_samples': self.max_samples_per_class
                    })
                    
                    # 训练集为空
                    balanced_train_targets.extend([class_id] * 0)  # 保持一致性
            
            sampling_info['classes_processed'] += 1
        
        # 转换为numpy数组
        balanced_train_data = np.array(balanced_train_data)
        balanced_train_targets = np.array(balanced_train_targets)
        balanced_test_data = np.array(balanced_test_data)
        balanced_test_targets = np.array(balanced_test_targets)
        
        # 记录最终分布
        sampling_info['final_train_counts'] = self._count_samples_by_class(balanced_train_targets)
        sampling_info['final_test_counts'] = self._count_samples_by_class(balanced_test_targets)
        
        return balanced_train_data, balanced_train_targets, balanced_test_data, balanced_test_targets, sampling_info
    
    def _save_balanced_dataset(self, 
                             dataset_name: str,
                             train_data: np.ndarray, train_targets: np.ndarray,
                             test_data: np.ndarray, test_targets: np.ndarray,
                             class_names: List[str], use_path: bool):
        """保存平衡后的数据集"""
        dataset_dir = self.output_dir / dataset_name
        train_dir = dataset_dir / "train"
        test_dir = dataset_dir / "test"
        
        # 创建目录结构
        train_dir.mkdir(parents=True, exist_ok=True)
        test_dir.mkdir(parents=True, exist_ok=True)
        
        # 创建类别目录
        for i, class_name in enumerate(class_names):
            (train_dir / str(i)).mkdir(exist_ok=True)
            (test_dir / str(i)).mkdir(exist_ok=True)
        
        # 保存训练数据
        self._save_data_to_directory(train_data, train_targets, train_dir, use_path)
        
        # 保存测试数据
        self._save_data_to_directory(test_data, test_targets, test_dir, use_path)
        
        # 保存类别标签文件
        label_file = dataset_dir / "label.txt"
        with open(label_file, 'w') as f:
            for class_name in class_names:
                f.write(f"{class_name}\n")
        
        logging.info(f"平衡后的数据集已保存到 {dataset_dir}")
    
    def _save_data_to_directory(self, data: np.ndarray, targets: np.ndarray, 
                               base_dir: Path, use_path: bool):
        """将数据保存到目录结构中"""
        if use_path:
            # 如果数据是路径，直接复制文件
            for i, (sample, label) in enumerate(zip(data, targets)):
                src_path = Path(sample)
                dst_dir = base_dir / str(label)
                dst_path = dst_dir / f"{i}_{src_path.name}"
                shutil.copy2(src_path, dst_path)
        else:
            # 如果数据是数组，保存为图像文件
            from PIL import Image
            for i, (sample, label) in enumerate(zip(data, targets)):
                dst_dir = base_dir / str(label)
                dst_path = dst_dir / f"{i:06d}.png"
                if isinstance(sample, np.ndarray):
                    img = Image.fromarray(sample)
                else:
                    # 假设是PIL图像
                    img = sample
                img.save(dst_path)
    
    def resplit_all_datasets(self, dataset_names: List[str]) -> Dict[str, Any]:
        """重新划分所有数据集"""
        logging.info(f"开始处理 {len(dataset_names)} 个数据集")
        
        all_sampling_info = {}
        
        for dataset_name in dataset_names:
            try:
                sampling_info = self.resplit_single_dataset(dataset_name)
                all_sampling_info[dataset_name] = sampling_info
            except Exception as e:
                logging.error(f"处理数据集 {dataset_name} 时出错: {str(e)}")
                all_sampling_info[dataset_name] = {'error': str(e)}
        
        # 生成统计信息
        statistics = self._generate_statistics(all_sampling_info)
        self.metadata_manager.record_dataset_statistics(statistics)
        
        # 保存所有元数据
        self.metadata_manager.save_all_metadata()
        
        logging.info(f"所有数据集处理完成，结果保存在 {self.output_dir}")
        return all_sampling_info
    
    def _generate_statistics(self, all_sampling_info: Dict[str, Any]) -> Dict[str, Any]:
        """生成统计信息"""
        stats = {
            'total_datasets_processed': len(all_sampling_info),
            'datasets_with_errors': 0,
            'classes_with_insufficient_samples': 0,
            'total_samples_moved': 0,
            'balance_improvement': {}
        }
        
        for dataset_name, info in all_sampling_info.items():
            if 'error' in info:
                stats['datasets_with_errors'] += 1
                continue
            
            if 'classes_with_insufficient_samples' in info:
                stats['classes_with_insufficient_samples'] += len(info['classes_with_insufficient_samples'])
            
            if 'samples_moved_from_train_to_test' in info:
                stats['total_samples_moved'] += sum(info['samples_moved_from_train_to_test'].values())
        
        return stats


def main():
    """主函数"""
    # 默认数据集列表
    default_datasets = [
        'cifar100_224', 'cub200_224', 'resisc45', 'imagenet-r', 'caltech-101', 
        'dtd', 'fgvc-aircraft-2013b-variants102', 'food-101', 'mnist', 
        'oxford-flower-102', 'oxford-iiit-pets', 'cars196_224'
    ]
    
    # 创建重新划分器
    resplitter = DatasetResplitter(
        max_samples_per_class=128,
        seed=42,
        output_dir="balanced_datasets"
    )
    
    # 处理所有数据集
    results = resplitter.resplit_all_datasets(default_datasets)
    
    print("数据集重新划分完成！")
    print(f"结果保存在: balanced_datasets/")
    print(f"元数据保存在: balanced_datasets/metadata/")


if __name__ == "__main__":
    main()