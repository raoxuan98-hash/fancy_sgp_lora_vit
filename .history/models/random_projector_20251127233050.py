from __future__ import annotations

import copy
import logging
import time
from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, SubsetRandomSampler, Subset
from torchvision import datasets, transforms

from models.base import BaseLearner
from classifier.classifier_builder import ClassifierReconstructor
from compensator.distribution_compensator import DistributionCompensator
from models.distillator import Distiller
from utils.inc_net import RandomNet
from lora import compute_covariances
import math


class EMASmooth:
    def __init__(self, alpha=0.95):
        self.alpha = alpha
        self.value = None
    def update(self, new_value):
        if self.value is None:
            self.value = new_value
        else:
            self.value = self.alpha * self.value + (1 - self.alpha) * new_value
        return self.value
    def get(self):
        return self.value if self.value is not None else 0.0

@dataclass
class Timing:
    train: float = 0.0
    drift: float = 0.0
    total: float = 0.0

class RandomProjector(BaseLearner):
    def __init__(self, args: Dict[str, Any]) -> None:
        super().__init__(args)
        self._device = torch.device("cuda")
        self.network = RandomNet(args, pretrained=True).to(self._device)
        self.args = args

        self._timings: Timing = Timing()
        self.time_history: List[Dict[str, float]] = []

        self.batch_size: int = args["batch_size"]
        self.iterations: int = args["iterations"]
        self.warmup_steps: int = int(args["warmup_ratio"] * self.iterations)
        self.lrate: float = args["lrate"]
        self.weight_decay: float = args["weight_decay"]
        self.optimizer_type: str = args["optimizer"]
        self.first_section_adaptation: bool = args["first_section_adaptation"]


        self.drift_compensator = DistributionCompensator(compensator_types=["SeqFT"])
        
        self.classifier_reconstructor = ClassifierReconstructor(
            device=self._device,
            lda_reg_alpha=args['lda_reg_alpha'],
            qda_reg_alpha1=args['qda_reg_alpha1'],
            qda_reg_alpha2=args['qda_reg_alpha2'],
            qda_reg_alpha3=args['qda_reg_alpha3'])
        
        self.seed: int = args["seed"]
        self.task_count: int = 0
        self.current_task_id = 0
        
        self.loss_smoother = EMASmooth(alpha=0.98)
        self.acc_smoother = EMASmooth(alpha=0.98)
        

    def handle_drift_compensation(self) -> None:
        """Handle the drift compensation and update classifiers."""
        drift_start = time.time()

        self.drift_compensator.build_current_only(
            self.current_task_id,
            self.network,
            self.train_loader_test_mode,
            low_rank=True)
        
        self._timings.drift = time.time() - drift_start

    def refine_classifiers(self):
        logging.info(f"Building classifiers from {len(self.drift_compensator.variants)} variants...")
        variant_names = list(self.drift_compensator.variants.keys())
        logging.info(f"Available variants: {variant_names}")
        
        self.fc_dict = self.classifier_reconstructor.build_classifiers(self.drift_compensator.variants)
        logging.info(f"Built {len(self.fc_dict)} classifiers: {list(self.fc_dict.keys())}")

    def after_task(self) -> None:
        self._known_classes = self._total_classes
        self.task_count += 1
    def incremental_train(self, data_manager) -> None:
        start_time = time.time()
        task_id = self.current_task_id
        task_size = data_manager.get_task_size(task_id)

        self._total_classes = self._known_classes + task_size
        self.current_task_id += 1
        self.topk = min(self._total_classes, 5)

        train_set = data_manager.get_incremental_subset(task=task_id, source="train", cumulative=False, mode="train")
        test_set = data_manager.get_incremental_subset(task=task_id, source="test", cumulative=True, mode="test")
        train_set_test_mode = data_manager.get_incremental_subset(task=task_id, source="train", cumulative=False, mode="test")

        self.train_loader = DataLoader(train_set, batch_size=self.batch_size, shuffle=True, num_workers=4, pin_memory=True, persistent_workers=False)
        self.test_loader = DataLoader(test_set, batch_size=self.batch_size * 4, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=False)

        self.train_loader_test_mode = DataLoader(
            train_set_test_mode,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=3,
            pin_memory=True,
            persistent_workers=True)


        # è·å–å½“å‰ä»»åŠ¡çš„æ•°æ®é›†ä¿¡æ¯ï¼Œå…¼å®¹å¢é‡æ‹†åˆ†å’Œæ™®é€šæ¨¡å¼
        dataset_info = data_manager.datasets[task_id]
        dataset_name = dataset_info.get('original_dataset_name', dataset_info['name'])
        
        # === åˆå§‹åŒ– DriftCompensator æ‰€éœ€çš„ loaderï¼Œä½†ä¸ç”¨äº KD ===
        if self.current_task_id  == 1 and self.first_section_adaptation:
            self.network.update_fc(task_size)
            self.network.fc.to(self._device)

            logging.info("System training on classes %d-%d (%s)", self._known_classes, self._total_classes, dataset_name.lower())
            
            self.print_parameter_statistics(task_id)
            self.system_training()

        self.handle_drift_compensation()
        self._timings.total = time.time() - start_time

        logging.info("Task %d finished total: %.2f s | train: %.2f s | drift: %.2f s", self.current_task_id, self._timings.total, self._timings.train, self._timings.drift)

        
    def make_optimizer(
        self,
        lora_params: List[torch.nn.Parameter],
        fc_params: List[torch.nn.Parameter]) -> optim.Optimizer:

        param_groups = [
            {"params": lora_params, "lr": self.lrate, "weight_decay": self.weight_decay},
            {"params": fc_params, "lr": 1e-3 if self.optimizer_type == "adamw" else 5e-3, "weight_decay": self.weight_decay}]

        if self.optimizer_type == "sgd":
            optimizer = optim.SGD(param_groups, momentum=0.9)
        elif self.optimizer_type == "adamw":
            optimizer = optim.AdamW(param_groups)
        elif self.optimizer_type == "rmsprop":
            optimizer = optim.RMSprop(param_groups)
        else:
            raise ValueError(f"Unsupported optimizer: {self.optimizer_type}")

        if self.warmup_steps > 0:
            def lora_lr_lambda(step):
                if step < self.warmup_steps:
                    return step / max(1, self.warmup_steps)
                else:
                    progress = (step - self.warmup_steps) / max(1, self.iterations - self.warmup_steps)
                    initial_lr = self.lrate
                    eta_min = getattr(self, 'eta_min', self.lrate * 0.3)
                    lr_ratio = eta_min / initial_lr
                    cosine_decay = 0.5 * (1.0 + math.cos(math.pi * progress))
                    return lr_ratio + cosine_decay * (1.0 - lr_ratio)
            
            def const_lr_lambda(step):
                return 1.0
            
            lr_lambdas = [lora_lr_lambda, const_lr_lambda]
            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambdas, last_epoch=-1)

        return optimizer, scheduler


    def system_training(self) -> None:
        """Train ViT + new classifier head for ``self.epochs`` epochs."""
        fc_params = self.network.fc.parameters()
        # å¤„ç†ä¸åŒæ¨¡å‹ç±»å‹çš„å‚æ•°è·å–
        if hasattr(self.network.vit, 'get_param_groups'):
            lora_params = self.network.vit.get_param_groups()
        else:
            lora_params = [p for p in self.network.vit.parameters() if p.requires_grad]
        optimizer, scheduler = self.make_optimizer(lora_params, fc_params)
        
        start = time.time()
        self.network.train()
        
        step = 0
        done = False
        while True:
            for batch in self.train_loader:
                inputs, targets = batch[0], batch[1]
                loss, n_corr, kd_term = self.process_batch(inputs, targets, optimizer)
                batch_acc = n_corr / inputs.size(0)
                smoothed_loss = self.loss_smoother.update(loss)
                smoothed_acc = self.acc_smoother.update(batch_acc)
                if (step + 1) % 50 == 0:
                    logging.info('step: %d, loss: %.4f, acc: %.4f', step, smoothed_loss, smoothed_acc)
                scheduler.step()
                step += 1
                if step == self.iterations:
                    done = True
                    break
            if done:
                break
        self._timings.train = time.time() - start

    def process_batch(self, inputs, targets, optimizer):
        inputs, targets = inputs.to(self._device), targets.to(self._device)
        feats = self.network.forward_features(inputs, random_projection=False)
        logits = self.network.fc(feats)
        
        new_targets_rel = torch.where(
            targets - self._known_classes >= 0,
            targets - self._known_classes, -100)
        new_logits = logits[:, self._known_classes:]
        
        sce = F.cross_entropy(new_logits, new_targets_rel, label_smoothing=0.1)
        loss = sce

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        with torch.no_grad():
            pred = logits.argmax(dim=1)
            n_correct = (pred == targets).sum().item()

        kd_raw = 0.0
        return loss.item(), n_correct, kd_raw

    def evaluate(
        self,
        loader: DataLoader,
        fc_dict):
        """
        Evaluate model on test data.
        For cross-domain scenarios, compute accuracy per dataset and average across datasets.
        For regular scenarios, compute overall accuracy.
        """
        self.network.eval()
        
        is_cross_domain = self.args['cross_domain']
        
        if is_cross_domain:
            result = self._evaluate_cross_domain(loader, fc_dict)
        else:
            result = self._evaluate_regular(loader, fc_dict)
           
        return result
    
    def _evaluate_regular(self, loader: DataLoader, fc_dict):
        """Regular evaluation: compute overall accuracy across all samples"""
        self.network.eval()
        total = 0
        corrects = {}
        for name, fc in fc_dict.items():
            corrects[name] = 0
            fc.to(self._device)

        with torch.no_grad():
            for batch in loader:
                inputs = batch[0].to(self._device)
                targets = batch[1]
                
                feats = self.network.forward_features(inputs)
                for name, fc in fc_dict.items():
                    preds = fc(feats).argmax(dim=1).cpu()
                    corrects[name] += (preds == targets).sum().item()
                total += targets.size(0)
        
        for name, correct in corrects.items():
            corrects[name] = float(np.around(100 * correct / total, 2))
        return corrects
    

    def _evaluate_cross_domain(self, loader: DataLoader, fc_dict):
        self.network.eval()
        for name, fc in fc_dict.items():
            fc.to(self._device)
        
        targets_all = []
        preds_all = {}
        for name in fc_dict.keys():
            preds_all[name] = []

        with torch.no_grad():
            for batch_idx, batch in enumerate(loader):
                inputs = batch[0].to(self._device)
                targets = batch[1]
                
                targets_all.append(targets)
                feats = self.network.forward_features(inputs)
                
                for name, fc in fc_dict.items():
                    preds = fc(feats).argmax(dim=1).cpu()
                    preds_all[name].append(preds)

            targets_all = torch.cat(targets_all, dim=0)
            for name in fc_dict.keys():
                preds_all[name] = torch.cat(preds_all[name], dim=0)
        
        """ Calculate overall accuracy across all samples """
        overall_accs = {}
        for name, preds in preds_all.items():
            overall_accs[name] = float(np.around(100 * (preds == targets_all).sum().item() / targets_all.size(0), 2))

        """ Calculate task-wise average accuracy"""
        overall_task_wise_avg_accs = {}
        for name in fc_dict.keys():
            overall_task_wise_avg_accs[name] = {}
        
        for task_id in range(self.current_task_id):
            task_start_label = self.data_manager.global_label_offset[task_id]
            # å¯¹äºæœ€åä¸€ä¸ªä»»åŠ¡ï¼Œä½¿ç”¨æ€»ç±»åˆ«æ•°ä½œä¸ºç»“æŸæ ‡ç­¾
            if task_id + 1 < len(self.data_manager.global_label_offset):
                task_end_label = self.data_manager.global_label_offset[task_id + 1]
            else:
                task_end_label = self._total_classes
            # print(task_start_label)
            # print(task_end_label)
            mask = (targets_all >= task_start_label) & (targets_all < task_end_label)
            for name, preds in preds_all.items():
                task_acc = float(np.around(100 * (preds[mask] == targets_all[mask]).sum().item() / mask.sum().item(), 2))
                overall_task_wise_avg_accs[name][task_id] = task_acc


        """ Calculate class-wise average accuracy """
        overall_class_wise_accs = {}
        for name in fc_dict.keys():
            overall_class_wise_accs[name] = {}
        unique_labels = torch.unique(targets_all)
        for label in unique_labels:
            mask = (targets_all == label)
            for name, preds in preds_all.items():
                class_wise_accs = float(np.around(100 * (preds[mask] == targets_all[mask]).sum().item() / mask.sum().item(), 2))
                overall_class_wise_accs[name][label.item()] = class_wise_accs
        overall_class_wise_avg_accs = {}
        for name in fc_dict.keys():
            overall_class_wise_avg_accs[name] = np.mean(list(overall_class_wise_accs[name].values()))

        task_stats = {
            'average_accs': overall_accs,
            'average_task_wise_accs': overall_task_wise_avg_accs,
            'average_class_wise_accs': overall_class_wise_avg_accs}
        
        return task_stats
    
    def eval_task(self):
        logging.info(f"Evaluating with {len(self.fc_dict)} classifiers...")
        
        results = self.evaluate(
            self.test_loader,
            fc_dict=self.fc_dict)
    
        self.all_task_results[self.current_task_id] = results
        logging.info(f"[GPU Memory] Peak memory usage up to task {self.current_task_id}: {peak_memory:.2f}GB")
        return results

    def update_projection_matrices(self):
        if hasattr(self.network.vit, 'use_projection') and self.network.vit.use_projection:
            if self.current_task_id >= 0:
                new_covs = compute_covariances(self.network.vit, self.train_loader_test_mode)
                # å°†æ–°çš„åæ–¹å·®çŸ©é˜µç§»åˆ°CPUä»¥èŠ‚çœGPUæ˜¾å­˜
                new_covs_cpu = {k: v.cpu() for k, v in new_covs.items()}
                
                if self.covariances is None:
                    self.covariances = new_covs_cpu
                else:
                    for k in self.covariances:
                        self.covariances[k] = 0.9 * self.covariances[k] + new_covs_cpu[k] + 1e-7 * torch.eye(self.covariances[k].size(0))
                # åªåœ¨éœ€è¦æ—¶å†å°†covariancesç§»åˆ°GPU
                covariances_gpu = {k: v.to(self._device) for k, v in self.covariances.items()}
                self.network.update_projection_matrices(covariances_gpu)
                # æ¸…ç†GPUä¸Šçš„ä¸´æ—¶covariances
                del covariances_gpu
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

    def loop(self, data_manager) -> Dict[str, Any]:
        self.data_manager = data_manager
        self.all_task_results = {}
        
        # è·å–evaluate_final_onlyå‚æ•°
        evaluate_final_only = self.args.get('evaluate_final_only', True)
        total_tasks = data_manager.nb_tasks
        
        for task_idx in range(total_tasks):
            self.incremental_train(data_manager)
            self.refine_classifiers()
            
            # æ ¹æ®evaluate_final_onlyå‚æ•°å†³å®šæ˜¯å¦è¿›è¡Œè¯„ä¼°
            if not evaluate_final_only or (task_idx == total_tasks - 1):
                # å½“evaluate_final_only=Falseæ—¶ï¼Œæ¯ä¸ªä»»åŠ¡åéƒ½è¯„ä¼°
                # å½“evaluate_final_only=Trueæ—¶ï¼Œåªåœ¨æœ€åä¸€ä¸ªä»»åŠ¡åè¯„ä¼°
                logging.info(f"Evaluating after task {self.current_task_id}...")
                self.eval_task()
                print(self.all_task_results)
                print(self.all_task_results.keys())
                dataset_names = getattr(self.data_manager, 'dataset_names', None)
                final_analysis = self.analyze_task_results(self.all_task_results, dataset_names)
            else:
                # è·³è¿‡ä¸­é—´ä»»åŠ¡çš„è¯„ä¼°ï¼Œåªè®°å½•åŸºæœ¬ä¿¡æ¯
                logging.info(f"Task {self.current_task_id} completed (evaluation skipped due to evaluate_final_only=True)")
                final_analysis = None
                
            self.after_task()

        return final_analysis
    def analyze_task_results(self, all_task_results, dataset_names):
        task_ids = sorted(all_task_results.keys())
        last_task_id = task_ids[-1]
        # æå– variant åç§°
        variant_names = set()
        for task_dict in all_task_results.values():
            if 'average_accs' in task_dict:
                variant_names.update(task_dict['average_accs'].keys())
        variant_names = sorted(variant_names)
        # è®¡ç®—æ•°æ®é›†çº§åˆ«å‡†ç¡®ç‡ï¼ˆdata-wiseï¼‰
        data_wise_accuracies = {}
        if 'average_accs' in all_task_results[last_task_id]:
            data_wise_accuracies = all_task_results[last_task_id]['average_accs'].copy()
        # è®¡ç®—ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡ï¼ˆdata-wiseï¼‰
        cumulative_average_accuracies = {}
        for variant in variant_names:
            accs = []
            for task_id in task_ids:
                task_dict = all_task_results[task_id]
                if 'average_accs' in task_dict and variant in task_dict['average_accs']:
                    acc_value = task_dict['average_accs'][variant]
                    if isinstance(acc_value, (int, float)):
                        accs.append(acc_value)
            
            if accs:
                cumulative_average_accuracies[variant] = float(np.mean(accs))
            else:
                cumulative_average_accuracies[variant] = 0.0
        
        # è®¡ç®—ä»»åŠ¡çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡ï¼ˆtask-wiseï¼‰
        cumulative_task_wise_accuracies = {}
        for variant in variant_names:
            task_wise_accs = []
            for task_id in task_ids:
                task_dict = all_task_results[task_id]
                if 'average_task_wise_accs' in task_dict and variant in task_dict['average_task_wise_accs']:
                    task_accs = list(task_dict['average_task_wise_accs'][variant].values())
                    if task_accs:
                        task_wise_accs.append(np.mean(task_accs))
            
            if task_wise_accs:
                cumulative_task_wise_accuracies[variant] = float(np.mean(task_wise_accs))
            else:
                cumulative_task_wise_accuracies[variant] = 0.0
        
        # è®¡ç®—ç±»åˆ«çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡ï¼ˆclass-wiseï¼‰
        cumulative_class_wise_accuracies = {}
        for variant in variant_names:
            class_wise_accs = []
            for task_id in task_ids:
                task_dict = all_task_results[task_id]
                if 'average_class_wise_accs' in task_dict and variant in task_dict['average_class_wise_accs']:
                    class_acc = task_dict['average_class_wise_accs'][variant]
                    if isinstance(class_acc, (int, float)):
                        class_wise_accs.append(class_acc)
            
            if class_wise_accs:
                cumulative_class_wise_accuracies[variant] = float(np.mean(class_wise_accs))
            else:
                cumulative_class_wise_accuracies[variant] = 0.0
        is_cross_domain = self.args['cross_domain']
        
        if is_cross_domain:
            logging.info("ğŸ“Š Cross-domain Evaluation Results:")
            logging.info("   â”€â”€ Overall Performance Summary â”€â”€")
            
            # åˆ›å»ºç»Ÿä¸€çš„æ€§èƒ½è¡¨æ ¼
            performance_data = []
            for variant in variant_names:
                current_acc = data_wise_accuracies.get(variant, 0.0)
                task_wise_avg = 0.0
                class_wise_avg = 0.0
                
                # è·å–ä»»åŠ¡çº§åˆ«å¹³å‡å‡†ç¡®ç‡
                if 'average_task_wise_accs' in all_task_results[last_task_id]:
                    task_wise_data = all_task_results[last_task_id]['average_task_wise_accs']
                    if variant in task_wise_data:
                        task_accs = list(task_wise_data[variant].values())
                        if task_accs:
                            task_wise_avg = float(np.mean(task_accs))
                
                # è·å–ç±»åˆ«çº§åˆ«å¹³å‡å‡†ç¡®ç‡
                if 'average_class_wise_accs' in all_task_results[last_task_id]:
                    class_wise_data = all_task_results[last_task_id]['average_class_wise_accs']
                    if variant in class_wise_data:
                        class_wise_avg = class_wise_data[variant]
                
                performance_data.append({
                    'variant': variant,
                    'current': current_acc,
                    'task_wise': task_wise_avg,
                    'class_wise': class_wise_avg
                })
            
            # æŒ‰å½“å‰å‡†ç¡®ç‡æ’åº
            performance_data.sort(key=lambda x: x['current'], reverse=True)
            
            # è¾“å‡ºç»Ÿä¸€è¡¨æ ¼
            logging.info("   Method                         â”‚ Dataset â”‚ Per-Task â”‚ Per-Class")
            logging.info("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
            for data in performance_data:
                logging.info(f"   {data['variant']:<30} â”‚ {data['current']:6.2f}% â”‚ {data['task_wise']:8.2f}% â”‚ {data['class_wise']:9.2f}%")
            
            # è¯¦ç»†ä»»åŠ¡ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œå¦‚æœéœ€è¦çš„è¯ï¼‰
            if 'average_task_wise_accs' in all_task_results[last_task_id]:
                logging.info("")
                logging.info("   â”€â”€ Detailed Per-Task Performance â”€â”€")
                task_wise_data = all_task_results[last_task_id]['average_task_wise_accs']
                
                # æŒ‰æ€§èƒ½æ’åº variants
                sorted_variants = sorted(variant_names,
                                    key=lambda v: data_wise_accuracies.get(v, 0.0),
                                    reverse=True)
                
                # æŒ‰æ•°æ®é›†åç§°åˆ†ç»„ç»Ÿè®¡ç»“æœ
                for variant in sorted_variants:
                    if variant in task_wise_data:
                        # ç¬¬ä¸€æ­¥ï¼šæŒ‰æ•°æ®é›†åç§°åˆ†ç»„æ‰€æœ‰ä»»åŠ¡ç»“æœ
                        dataset_groups = {}  # dataset_name -> [(task_id, acc, test_samples), ...]
                        
                        for task_id in sorted(task_wise_data[variant].keys()):
                            # è·å–æ•°æ®é›†åç§°
                            if hasattr(self, 'data_manager') and self.data_manager:
                                dataset_info = self.data_manager.datasets[task_id] if task_id < len(self.data_manager.datasets) else None
                                if dataset_info and 'original_dataset_name' in dataset_info:
                                    dataset_name = dataset_info['original_dataset_name']
                                elif dataset_info and 'name' in dataset_info:
                                    dataset_name = dataset_info['name']
                                elif dataset_names and task_id < len(dataset_names):
                                    dataset_name = dataset_names[task_id]
                                else:
                                    dataset_name = f"Task {task_id}"
                            else:
                                dataset_name = dataset_names[task_id] if dataset_names and task_id < len(dataset_names) else f"Task {task_id}"
                            
                            # æ¸…ç†æ•°æ®é›†åç§°
                            if dataset_name.endswith('_split_0') or dataset_name.endswith('_split_1'):
                                dataset_name = dataset_name.split('_split_')[0]
                            elif dataset_name.endswith('_224'):
                                pass  # ä¿æŒ_224åç¼€ä»¥åŒºåˆ†ä¸åŒåˆ†è¾¨ç‡
                            
                            acc = task_wise_data[variant][task_id]
                            
                            # è·å–æµ‹è¯•æ ·æœ¬æ•°é‡ä½œä¸ºæƒé‡
                            if hasattr(self, 'data_manager') and self.data_manager:
                                dataset_info = self.data_manager.datasets[task_id] if task_id < len(self.data_manager.datasets) else None
                                test_samples = len(dataset_info['test_data']) if dataset_info and 'test_data' in dataset_info else 1
                            else:
                                test_samples = 1
                            
                            # æŒ‰æ•°æ®é›†åç§°åˆ†ç»„
                            if dataset_name not in dataset_groups:
                                dataset_groups[dataset_name] = []
                            dataset_groups[dataset_name].append((task_id, acc, test_samples))
                        
                        # ç¬¬äºŒæ­¥ï¼šæ˜¾ç¤ºåˆ†ç»„ç»“æœ
                        logging.info(f"   ğŸ“ˆ {variant}:")
                        for dataset_name, task_results in dataset_groups.items():
                            if len(task_results) > 1:
                                # æœ‰å¤šä¸ªå­é›†ï¼Œè®¡ç®—åŠ æƒå¹³å‡
                                accuracies = [result[1] for result in task_results]
                                weights = [result[2] for result in task_results]
                                weighted_avg = np.average(accuracies, weights=weights)
                                total_samples = sum(weights)
                                
                                # æ˜¾ç¤ºæ¯ä¸ªå­é›†çš„ç»“æœ
                                for task_id, acc, samples in task_results:
                                    logging.info(f"        {dataset_name} (task {task_id}) : {acc:.2f}% ({samples} samples)")
                                
                                # æ˜¾ç¤ºåŠ æƒå¹³å‡ç»“æœ
                                logging.info(f"        {dataset_name} (weighted avg) : {weighted_avg:.2f}% (total: {total_samples} samples)")
                            else:
                                # åªæœ‰ä¸€ä¸ªå­é›†ï¼Œç›´æ¥æ˜¾ç¤º
                                task_id, acc, samples = task_results[0]
                                logging.info(f"        {dataset_name:<20} : {acc:.2f}% ({samples} samples)")
            
            # æ€§èƒ½æ€»ç»“
            logging.info("")
            logging.info("   â”€â”€ Performance Summary â”€â”€")
            best_variant = max(performance_data, key=lambda x: x['current'])
            logging.info(f"   ğŸ† Best Performing Method: '{best_variant['variant']}'")
            logging.info(f"   ğŸ“Š Best Accuracy: {best_variant['current']:.2f}%")
            
            return {
                "last_task_id": last_task_id,
                "last_task_accuracies": data_wise_accuracies,  # ä¿æŒå‘åå…¼å®¹æ€§
                "data_wise_accuracies": data_wise_accuracies,
                "average_accuracies": cumulative_average_accuracies,  # ä¿æŒå‘åå…¼å®¹æ€§
                "cumulative_average_accuracies": cumulative_average_accuracies,
                "cumulative_task_wise_accuracies": cumulative_task_wise_accuracies,
                "cumulative_class_wise_accuracies": cumulative_class_wise_accuracies,
                "task_wise_accuracies": {data['variant']: data['task_wise'] for data in performance_data},
                "class_wise_accuracies": {data['variant']: data['class_wise'] for data in performance_data}}
             
        else:
            # For within-domain datasets: output only average_accuracy
            logging.info("ğŸ“Š Within-domain Evaluation Results:")
            logging.info("   â”€â”€ Average Accuracy Across Tasks (%) â”€â”€")
            for variant in variant_names:
                logging.info(f"      {variant:<20} : {cumulative_average_accuracies[variant]:.2f}%")
            
            logging.info("   â”€â”€ Final Task Accuracy (%) â”€â”€")
            for variant in variant_names:
                logging.info(f"      {variant:<20} : {data_wise_accuracies[variant]:.2f}%")
            
            # Optional: Identify best variants and log summary
            best_last = max(data_wise_accuracies.items(), key=lambda x: x[1])[0] if data_wise_accuracies else None
            best_avg = max(cumulative_average_accuracies.items(), key=lambda x: x[1])[0] if cumulative_average_accuracies else None

            if best_last and best_avg:
                if best_last == best_avg:
                    summary = f" Variant '{best_last}' is best in both final task and average performance."
                else:
                    summary = f" Best in Final Task: '{best_last}' | Best Average: '{best_avg}'"
            else:
                summary = " No valid variants found for comparison."

            logging.info("   â”€â”€ Summary â”€â”€")
            logging.info(f"      {summary}")
            
            return {
                "last_task_id": last_task_id,
                "last_task_accuracies": data_wise_accuracies,  # ä¿æŒå‘åå…¼å®¹æ€§
                "data_wise_accuracies": data_wise_accuracies,
                "average_accuracies": cumulative_average_accuracies,  # ä¿æŒå‘åå…¼å®¹æ€§
                "cumulative_average_accuracies": cumulative_average_accuracies,
                "cumulative_task_wise_accuracies": cumulative_task_wise_accuracies,
                "cumulative_class_wise_accuracies": cumulative_class_wise_accuracies}
    
    def get_aux_loader(self, args):
        aux_dataset_type = args.get('aux_dataset', 'flickr8k')
        num_samples =args['auxiliary_data_size']

        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])])

        if aux_dataset_type == 'imagenet':
            dataset = datasets.ImageFolder(args['auxiliary_data_path'] + '/ImageNet-2012/train', transform=transform)
        elif aux_dataset_type == 'cifar10':
            dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
        elif aux_dataset_type == 'svhn':
            dataset = datasets.SVHN(root='./data', split='train', download=True, transform=transform)
        elif aux_dataset_type == 'flickr8k':
            dataset = datasets.ImageFolder(args['auxiliary_data_path'] + '/flickr8k', transform=transform)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ aux_dataset_type: {aux_dataset_type}")

        indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)
        train_subset = Subset(dataset, indices)

        self.aux_loader = DataLoader(train_subset, batch_size=self.batch_size, shuffle=False, num_workers=2, pin_memory=True)
        self.aux_trainset = train_subset
        return self.aux_loader

    def count_trainable_parameters(self) -> Dict[str, int]:
        """ç»Ÿè®¡å„éƒ¨åˆ†çš„è®­ç»ƒå‚æ•°æ•°é‡"""
        param_counts = {}
        
        # è·å–æ¨¡å‹å‚æ•°ï¼Œå¤„ç†ä¸åŒæ¨¡å‹ç±»å‹
        if hasattr(self.network.vit, 'get_param_groups'):
            lora_params = self.network.vit.get_param_groups()
        else:
            # å¯¹äºå…¨å‚æ•°å¾®è°ƒæ¨¡å‹ï¼Œè·å–æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
            lora_params = [p for p in self.network.vit.parameters() if p.requires_grad]
        
        lora_count = sum(p.numel() for p in lora_params)
        param_counts["lora"] = lora_count
        
        # åˆ†ç±»å¤´å‚æ•°
        fc_count = sum(p.numel() for p in self.network.fc.parameters())
        param_counts["classifier"] = fc_count
        total_count = lora_count + fc_count
        param_counts["total"] = total_count
        
        return param_counts

    def count_total_parameters(self) -> int:
        """ç»Ÿè®¡æ¨¡å‹æ€»å‚æ•°æ•°é‡ï¼ˆåŒ…æ‹¬å†»ç»“å‚æ•°ï¼‰"""
        return sum(p.numel() for p in self.network.parameters())

    def print_parameter_statistics(self, task_id: int) -> None:
        """æ‰“å°å‚æ•°ç»Ÿè®¡ä¿¡æ¯"""
        trainable_params = self.count_trainable_parameters()
        total_params = self.count_total_parameters()
        
        logging.info(f"=== ä»»åŠ¡ {task_id} å‚æ•°ç»Ÿè®¡ ===")
        logging.info(f"æ€»æ¨¡å‹å‚æ•°: {total_params:,}")
        logging.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params['total']:,}")
        logging.info(f"  - LoRAå‚æ•°: {trainable_params['lora']:,}")
        logging.info(f"  - åˆ†ç±»å¤´å‚æ•°: {trainable_params['classifier']:,}")
        
        # è®¡ç®—å‚æ•°æ•ˆç‡
        efficiency = (trainable_params['total'] / total_params) * 100
        logging.info(f"å‚æ•°æ•ˆç‡: {efficiency:.2f}%")
