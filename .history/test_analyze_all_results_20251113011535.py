#!/usr/bin/env python3
"""
æµ‹è¯• analyze_all_results å‡½æ•°çš„æ­£ç¡®æ€§
"""

import sys
import os
import logging
import json

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from trainer import analyze_all_results

def test_analyze_all_results():
    """æµ‹è¯• analyze_all_results å‡½æ•°"""
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(filename)s] => %(message)s',
        handlers=[logging.StreamHandler(sys.stdout)]
    )
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„ all_results æ•°æ®
    mock_all_results = {
        "seed_1993": {
            "last_task_id": 2,
            "last_task_accuracies": {
                "lda": 82.1,
                "qda": 84.7
            },
            "average_accuracies": {
                "lda": 75.3,
                "qda": 78.17
            },
            "per_task_results": {
                0: {"lda": 75.5, "qda": 78.2},
                1: {"lda": 68.3, "qda": 71.6},
                2: {"lda": 82.1, "qda": 84.7}
            },
            "log_path": "/path/to/seed_1993/logs"
        },
        "seed_1996": {
            "last_task_id": 2,
            "last_task_accuracies": {
                "lda": 81.8,
                "qda": 84.2
            },
            "average_accuracies": {
                "lda": 74.9,
                "qda": 77.8
            },
            "per_task_results": {
                0: {"lda": 75.2, "qda": 77.9},
                1: {"lda": 67.8, "qda": 71.2},
                2: {"lda": 81.8, "qda": 84.2}
            },
            "log_path": "/path/to/seed_1996/logs"
        },
        "seed_1997": {
            "last_task_id": 2,
            "last_task_accuracies": {
                "lda": 82.5,
                "qda": 85.1
            },
            "average_accuracies": {
                "lda": 75.7,
                "qda": 78.5
            },
            "per_task_results": {
                0: {"lda": 75.8, "qda": 78.5},
                1: {"lda": 68.7, "qda": 71.9},
                2: {"lda": 82.5, "qda": 85.1}
            },
            "log_path": "/path/to/seed_1997/logs"
        }
    }
    
    # æ¨¡æ‹Ÿæ•°æ®é›†åç§°
    dataset_names = ["CIFAR-100", "CUB200", "Cars196"]
    
    # æµ‹è¯•å‡½æ•°
    print("=" * 80)
    print("ğŸ§ª å¼€å§‹æµ‹è¯• analyze_all_results å‡½æ•°")
    print("=" * 80)
    
    try:
        # æµ‹è¯•ä¿å­˜JSONåŠŸèƒ½
        output_path = "./test_statistics.json"
        statistics_results = analyze_all_results(mock_all_results, dataset_names, save_json=True, output_path=output_path)
        
        print("\nğŸ” æ£€æŸ¥è¿”å›çš„ç»Ÿè®¡ç»“æœç»“æ„:")
        print(f"  - åŒ…å«summary: {'summary' in statistics_results}")
        print(f"  - åŒ…å«variants: {'variants' in statistics_results}")
        print(f"  - åŒ…å«overall_summary: {'overall_summary' in statistics_results}")
        
        if 'summary' in statistics_results:
            summary = statistics_results['summary']
            print(f"  - ç§å­æ•°é‡: {summary.get('num_seeds', 'N/A')}")
            print(f"  - å˜ä½“æ•°é‡: {summary.get('num_variants', 'N/A')}")
            print(f"  - ä»»åŠ¡æ•°é‡: {summary.get('num_tasks', 'N/A')}")
        
        if 'variants' in statistics_results:
            variants = statistics_results['variants']
            for variant_name, variant_stats in variants.items():
                print(f"\n  ğŸ“Š å˜ä½“ {variant_name}:")
                if 'last_task_accuracy' in variant_stats and 'mean' in variant_stats['last_task_accuracy']:
                    lta = variant_stats['last_task_accuracy']
                    print(f"    - æœ€åä»»åŠ¡å‡†ç¡®ç‡: {lta['mean']}% Â± {lta['std']}%")
                
                if 'average_accuracy' in variant_stats and 'mean' in variant_stats['average_accuracy']:
                    aa = variant_stats['average_accuracy']
                    print(f"    - å¹³å‡å‡†ç¡®ç‡: {aa['mean']}% Â± {aa['std']}%")
                
                if 'per_task_accuracies' in variant_stats:
                    for task_id, task_stats in variant_stats['per_task_accuracies'].items():
                        if 'mean' in task_stats:
                            print(f"    - ä»»åŠ¡ {task_id} ({task_stats.get('dataset_name', 'Unknown')}): {task_stats['mean']}% Â± {task_stats['std']}%")
        
        # æ£€æŸ¥JSONæ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
        if os.path.exists(output_path):
            print(f"\nâœ… JSONæ–‡ä»¶å·²ç”Ÿæˆ: {output_path}")
            
            # è¯»å–å¹¶æ˜¾ç¤ºJSONç»“æ„
            with open(output_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            print("\nğŸ“„ JSONæ–‡ä»¶ç»“æ„é¢„è§ˆ:")
        }
    }
    analyze_all_results(incomplete_results)
    
    print("\nâœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    success = test_analyze_all_results()
    test_edge_cases()
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        sys.exit(0)
    else:
        print("\nğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼")
        sys.exit(1)