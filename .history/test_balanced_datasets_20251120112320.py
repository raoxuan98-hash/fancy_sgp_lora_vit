#!/usr/bin/env python3
"""
æµ‹è¯•å¹³è¡¡æ•°æ®é›†çš„è„šæœ¬
éªŒè¯æ•°æ®é›†é‡æ–°åˆ’åˆ†å’Œæ–°çš„æ•°æ®ç®¡ç†å™¨æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import os
import sys
import logging
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from dataset_resplitter import DatasetResplitter
from utils.balanced_cross_domain_data_manager import create_balanced_data_manager
from utils.cross_domain_data_manager import CrossDomainDataManagerCore

def test_dataset_resplitter():
    """æµ‹è¯•æ•°æ®é›†é‡æ–°åˆ’åˆ†å™¨"""
    print("=" * 60)
    print("æµ‹è¯•æ•°æ®é›†é‡æ–°åˆ’åˆ†å™¨")
    print("=" * 60)
    
    # åªæµ‹è¯•å‡ ä¸ªå°æ•°æ®é›†ä»¥èŠ‚çœæ—¶é—´
    test_datasets = ['dtd', 'mnist', 'cifar100_224']
    
    # åˆ›å»ºé‡æ–°åˆ’åˆ†å™¨
    resplitter = DatasetResplitter(
        max_samples_per_class=128,
        seed=42,
        output_dir="test_balanced_datasets"
    )
    
    # å¤„ç†æµ‹è¯•æ•°æ®é›†
    results = resplitter.resplit_all_datasets(test_datasets)
    
    # æ£€æŸ¥ç»“æœ
    for dataset_name, result in results.items():
        if 'error' in result:
            print(f"âŒ {dataset_name}: {result['error']}")
        else:
            print(f"âœ… {dataset_name}: å¤„ç†æˆåŠŸ")
            if 'classes_with_insufficient_samples' in result:
                insufficient = result['classes_with_insufficient_samples']
                if insufficient:
                    print(f"   âš ï¸  æœ‰ {len(insufficient)} ä¸ªç±»åˆ«æ ·æœ¬ä¸è¶³128")
                else:
                    print(f"   âœ… æ‰€æœ‰ç±»åˆ«éƒ½æœ‰è¶³å¤Ÿçš„æ ·æœ¬")
    
    return True

def test_balanced_data_manager():
    """æµ‹è¯•å¹³è¡¡æ•°æ®ç®¡ç†å™¨"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•å¹³è¡¡æ•°æ®ç®¡ç†å™¨")
    print("=" * 60)
    
    # æµ‹è¯•æ•°æ®é›†
    test_datasets = ['dtd', 'mnist', 'cifar100_224']
    
    try:
        # åˆ›å»ºå¹³è¡¡æ•°æ®ç®¡ç†å™¨
        balanced_manager = create_balanced_data_manager(
            dataset_names=test_datasets,
            balanced_datasets_root="test_balanced_datasets",
            use_balanced_datasets=True,
            log_level=logging.WARNING
        )
        
        print(f"âœ… å¹³è¡¡æ•°æ®ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
        print(f"   æ€»ä»»åŠ¡æ•°: {balanced_manager.nb_tasks}")
        print(f"   æ€»ç±»åˆ«æ•°: {balanced_manager.num_classes}")
        
        # æµ‹è¯•è·å–æ•°æ®é›†
        for task_id in range(balanced_manager.nb_tasks):
            dataset_info = balanced_manager.datasets[task_id]
            train_samples = len(dataset_info['train_data'])
            test_samples = len(dataset_info['test_data'])
            num_classes = dataset_info['num_classes']
            dataset_name = dataset_info['name']
            
            print(f"\nğŸ“Š æ•°æ®é›† {dataset_name} (ä»»åŠ¡ {task_id}):")
            print(f"   ç±»åˆ«æ•°: {num_classes}")
            print(f"   è®­ç»ƒæ ·æœ¬: {train_samples}")
            print(f"   æµ‹è¯•æ ·æœ¬: {test_samples}")
            print(f"   å¹³å‡æ¯ç±»è®­ç»ƒæ ·æœ¬: {train_samples/num_classes:.2f}")
            print(f"   å¹³å‡æ¯ç±»æµ‹è¯•æ ·æœ¬: {test_samples/num_classes:.2f}")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = balanced_manager.get_balanced_statistics()
        print(f"\nğŸ“ˆ å¹³è¡¡åç»Ÿè®¡ä¿¡æ¯:")
        for dataset_name, stat in stats.items():
            print(f"   {dataset_name}:")
            print(f"     è®­ç»ƒæ¯ç±»: min={stat['train_per_class']['min']}, "
                  f"max={stat['train_per_class']['max']}, "
                  f"mean={stat['train_per_class']['mean']:.2f}")
            print(f"     æµ‹è¯•æ¯ç±»: min={stat['test_per_class']['min']}, "
                  f"max={stat['test_per_class']['max']}, "
                  f"mean={stat['test_per_class']['mean']:.2f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¹³è¡¡æ•°æ®ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_comparison_with_original():
    """æµ‹è¯•ä¸åŸå§‹æ•°æ®é›†çš„æ¯”è¾ƒ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•ä¸åŸå§‹æ•°æ®é›†çš„æ¯”è¾ƒ")
    print("=" * 60)
    
    test_datasets = ['dtd', 'mnist', 'cifar100_224']
    
    try:
        # åˆ›å»ºå¹³è¡¡æ•°æ®ç®¡ç†å™¨
        balanced_manager = create_balanced_data_manager(
            dataset_names=test_datasets,
            balanced_datasets_root="test_balanced_datasets",
            use_balanced_datasets=True,
            log_level=logging.WARNING
        )
        
        # åˆ›å»ºåŸå§‹æ•°æ®ç®¡ç†å™¨
        original_manager = CrossDomainDataManagerCore(
            dataset_names=test_datasets,
            shuffle=False,
            seed=0,
            num_shots=0,
            num_samples_per_task_for_evaluation=0,
            log_level=logging.WARNING
        )
        
        # æ¯”è¾ƒç»Ÿè®¡ä¿¡æ¯
        comparison = balanced_manager.compare_with_original()
        
        print("ğŸ“Š åŸå§‹ vs å¹³è¡¡æ•°æ®é›†æ¯”è¾ƒ:")
        for dataset_name, comp in comparison.items():
            print(f"\nğŸ” {dataset_name}:")
            
            orig = comp['original']
            bal = comp['balanced']
            
            print(f"   è®­ç»ƒæ ·æœ¬: {orig['total_train_samples']} â†’ {bal['total_train_samples']}")
            print(f"   æµ‹è¯•æ ·æœ¬: {orig['total_test_samples']} â†’ {bal['total_test_samples']}")
            
            print(f"   è®­ç»ƒæ¯ç±»èŒƒå›´: {orig['train_per_class_stats']['min']}-{orig['train_per_class_stats']['max']} "
                  f"â†’ {bal['train_per_class_stats']['min']}-{bal['train_per_class_stats']['max']}")
            
            print(f"   æµ‹è¯•æ¯ç±»èŒƒå›´: {orig['test_per_class_stats']['min']}-{orig['test_per_class_stats']['max']} "
                  f"â†’ {bal['test_per_class_stats']['min']}-{bal['test_per_class_stats']['max']}")
            
            # è®¡ç®—æ”¹å–„ç¨‹åº¦
            orig_imbalance = orig['test_per_class_stats']['max'] / orig['test_per_class_stats']['min'] if orig['test_per_class_stats']['min'] > 0 else float('inf')
            bal_imbalance = bal['test_per_class_stats']['max'] / bal['test_per_class_stats']['min'] if bal['test_per_class_stats']['min'] > 0 else float('inf')
            
            print(f"   æµ‹è¯•é›†ä¸å¹³è¡¡æ¯”ç‡: {orig_imbalance:.2f}x â†’ {bal_imbalance:.2f}x")
            
            if bal_imbalance < orig_imbalance:
                print(f"   âœ… ä¸å¹³è¡¡æ€§æ”¹å–„äº† {(orig_imbalance/bal_imbalance):.2f}x")
            else:
                print(f"   âš ï¸  ä¸å¹³è¡¡æ€§æœªæ”¹å–„")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¯”è¾ƒæµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½")
    print("=" * 60)
    
    test_datasets = ['dtd', 'mnist']
    
    try:
        # åˆ›å»ºå¹³è¡¡æ•°æ®ç®¡ç†å™¨
        balanced_manager = create_balanced_data_manager(
            dataset_names=test_datasets,
            balanced_datasets_root="test_balanced_datasets",
            use_balanced_datasets=True,
            log_level=logging.WARNING
        )
        
        # æµ‹è¯•è·å–æ•°æ®é›†
        for task_id in range(balanced_manager.nb_tasks):
            print(f"\nğŸ” æµ‹è¯•ä»»åŠ¡ {task_id} ({balanced_manager.datasets[task_id]['name']}):")
            
            # æµ‹è¯•è®­ç»ƒé›†
            try:
                train_dataset = balanced_manager.get_subset(task_id, source="train", mode="train")
                print(f"   âœ… è®­ç»ƒé›†åŠ è½½æˆåŠŸ: {len(train_dataset)} ä¸ªæ ·æœ¬")
                
                # æµ‹è¯•è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬
                if len(train_dataset) > 0:
                    sample, label, class_name = train_dataset[0]
                    print(f"   ğŸ“· æ ·æœ¬å½¢çŠ¶: {sample.shape if hasattr(sample, 'shape') else type(sample)}")
                    print(f"   ğŸ·ï¸  æ ‡ç­¾: {label}, ç±»å: {class_name}")
                
            except Exception as e:
                print(f"   âŒ è®­ç»ƒé›†åŠ è½½å¤±è´¥: {str(e)}")
                return False
            
            # æµ‹è¯•æµ‹è¯•é›†
            try:
                test_dataset = balanced_manager.get_subset(task_id, source="test", mode="test")
                print(f"   âœ… æµ‹è¯•é›†åŠ è½½æˆåŠŸ: {len(test_dataset)} ä¸ªæ ·æœ¬")
                
                # æµ‹è¯•è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬
                if len(test_dataset) > 0:
                    sample, label, class_name = test_dataset[0]
                    print(f"   ğŸ“· æ ·æœ¬å½¢çŠ¶: {sample.shape if hasattr(sample, 'shape') else type(sample)}")
                    print(f"   ğŸ·ï¸  æ ‡ç­¾: {label}, ç±»å: {class_name}")
                
            except Exception as e:
                print(f"   âŒ æµ‹è¯•é›†åŠ è½½å¤±è´¥: {str(e)}")
                return False
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def check_metadata_files():
    """æ£€æŸ¥å…ƒæ•°æ®æ–‡ä»¶æ˜¯å¦æ­£ç¡®ç”Ÿæˆ"""
    print("\n" + "=" * 60)
    print("æ£€æŸ¥å…ƒæ•°æ®æ–‡ä»¶")
    print("=" * 60)
    
    metadata_dir = Path("test_balanced_datasets/metadata")
    
    if not metadata_dir.exists():
        print("âŒ å…ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨")
        return False
    
    required_files = [
        "original_distribution.json",
        "balanced_distribution.json", 
        "sampling_config.json",
        "dataset_statistics.json"
    ]
    
    all_exist = True
    for filename in required_files:
        file_path = metadata_dir / filename
        if file_path.exists():
            print(f"âœ… {filename} å­˜åœ¨")
            
            # æ£€æŸ¥æ–‡ä»¶å†…å®¹
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                print(f"   ğŸ“„ åŒ…å« {len(data)} ä¸ªæ¡ç›®")
            except Exception as e:
                print(f"   âš ï¸  è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        else:
            print(f"âŒ {filename} ä¸å­˜åœ¨")
            all_exist = False
    
    return all_exist

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•å¹³è¡¡æ•°æ®é›†ç³»ç»Ÿ")
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.basicConfig(level=logging.WARNING)
    
    tests = [
        ("æ•°æ®é›†é‡æ–°åˆ’åˆ†", test_dataset_resplitter),
        ("å¹³è¡¡æ•°æ®ç®¡ç†å™¨", test_balanced_data_manager),
        ("ä¸åŸå§‹æ•°æ®é›†æ¯”è¾ƒ", test_comparison_with_original),
        ("æ•°æ®åŠ è½½åŠŸèƒ½", test_data_loading),
        ("å…ƒæ•°æ®æ–‡ä»¶æ£€æŸ¥", check_metadata_files)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ æµ‹è¯• '{test_name}' å‡ºç°å¼‚å¸¸: {str(e)}")
            results.append((test_name, False))
    
    # è¾“å‡ºæµ‹è¯•ç»“æœæ‘˜è¦
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ‘˜è¦")
    print("=" * 60)
    
    passed = 0
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»è®¡: {passed}/{len(results)} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == len(results):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¹³è¡¡æ•°æ®é›†ç³»ç»Ÿå·¥ä½œæ­£å¸¸ã€‚")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)