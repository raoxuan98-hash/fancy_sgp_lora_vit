import torch
import torch.nn as nn
import timm
from utils.inc_net import get_vit
from models.full_finetune import FullFinetuneViT

def test_full_finetune():
    """æµ‹è¯•å…¨å‚æ•°å¾®è°ƒå®ç°"""
    print("=== æµ‹è¯•å…¨å‚æ•°å¾®è°ƒå®ç° ===")
    
    # åˆ›å»ºæµ‹è¯•å‚æ•°
    args = {
        'vit_type': 'vit-b-p16',
        'lora_type': 'full_ft',
        'lora_rank': 4,
        'include_norm': False,
        'freeze_patch_embed': True,
        'finetune_layers': None  # é»˜è®¤æ‰€æœ‰å±‚
    }
    
    # æµ‹è¯•æ¨¡å‹åˆ›å»º
    try:
        vit = timm.create_model("vit_base_patch16_224", pretrained=False, num_classes=0)
        vit.head = nn.Identity()
        del vit.norm
        vit.norm = nn.LayerNorm(768, elementwise_affine=False)
        
        # ä½¿ç”¨æˆ‘ä»¬çš„FullFinetuneViTç±»
        full_ft_model = FullFinetuneViT(vit, 
                                       include_norm=args['include_norm'],
                                       freeze_patch_embed=args['freeze_patch_embed'],
                                       finetune_layers=args['finetune_layers'])
        
        print("âœ“ FullFinetuneViTæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        x = torch.randn(2, 3, 224, 224)
        with torch.no_grad():
            output = full_ft_model(x)
        print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # æµ‹è¯•å‚æ•°ç»Ÿè®¡
        trainable_params = full_ft_model.count_trainable_parameters()
        total_params = full_ft_model.count_total_parameters()
        efficiency = (trainable_params / total_params) * 100
        
        print(f"âœ“ å‚æ•°ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"  å‚æ•°æ•ˆç‡: {efficiency:.2f}%")
        
        # æµ‹è¯•æ¥å£ä¸€è‡´æ€§
        param_groups = full_ft_model.get_param_groups()
        module_names = full_ft_model.get_module_names()
        use_projection = full_ft_model.use_projection
        
        print(f"âœ“ æ¥å£æµ‹è¯•:")
        print(f"  å‚æ•°ç»„æ•°é‡: {len(param_groups)}")
        print(f"  æ¨¡å—åç§°: {module_names[:3]}...")  # åªæ˜¾ç¤ºå‰3ä¸ª
        print(f"  ä½¿ç”¨æŠ•å½±: {use_projection}")
        
        # æµ‹è¯•é€šè¿‡utils.inc_netåˆ›å»º
        print("\n=== æµ‹è¯•é€šè¿‡utils/inc_netåˆ›å»º ===")
        vit_model = get_vit(args, pretrained=False)
        print("âœ“ é€šè¿‡get_vitåˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å‚æ•°è·å–
        if hasattr(vit_model, 'get_param_groups'):
            params = vit_model.get_param_groups()
            print(f"âœ“ å‚æ•°è·å–æˆåŠŸï¼Œæ•°é‡: {len(params)}")
        
        return True
        
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_full_finetune()
    if success:
        print("\nğŸ‰ å…¨å‚æ•°å¾®è°ƒå®ç°æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâŒ å…¨å‚æ•°å¾®è°ƒå®ç°æµ‹è¯•å¤±è´¥ï¼")