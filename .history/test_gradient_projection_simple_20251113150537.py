import torch
import torch.nn as nn
import timm
from lora import NullSpaceViT

def test_gradient_projection_simple():
    """ç®€å•æµ‹è¯•æ¢¯åº¦æŠ•å½±æœºåˆ¶"""
    print("=== æµ‹è¯•æ¢¯åº¦æŠ•å½±æœºåˆ¶ï¼ˆæ³¨æ„åŠ›+FFNæ¨¡å—ï¼‰ ===")
    
    try:
        # åˆ›å»ºViTæ¨¡å‹
        vit = timm.create_model("vit_base_patch16_224", pretrained=False, num_classes=0)
        vit.head = nn.Identity()
        del vit.norm
        vit.norm = nn.LayerNorm(768, elementwise_affine=False)
        
        # ä½¿ç”¨NullSpaceViTåŒ…è£…
        nullspace_model = NullSpaceViT(vit, use_projection=True)
        print("âœ“ NullSpaceViTæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æ£€æŸ¥å“ªäº›æ¨¡å—è¢«åŒ…å«åœ¨æ¢¯åº¦æŠ•å½±ä¸­
        print("\n=== æ£€æŸ¥æ¢¯åº¦æŠ•å½±æ¨¡å— ===")
        trainable_modules = []
        frozen_modules = []
        
        for name, param in nullspace_model.named_parameters():
            if param.requires_grad:
                trainable_modules.append(name)
            else:
                frozen_modules.append(name)
        
        print(f"å¯è®­ç»ƒæ¨¡å—æ•°é‡: {len(trainable_modules)}")
        print("å‰5ä¸ªå¯è®­ç»ƒæ¨¡å—:")
        for name in trainable_modules[:5]:
            print(f"  - {name}")
        
        print(f"\nå†»ç»“æ¨¡å—æ•°é‡: {len(frozen_modules)}")
        print("å‰5ä¸ªå†»ç»“æ¨¡å—:")
        for name in frozen_modules[:5]:
            print(f"  - {name}")
        
        # éªŒè¯åªæœ‰æ³¨æ„åŠ›æ¨¡å—å’ŒFFNæ¨¡å—æ˜¯å¯è®­ç»ƒçš„
        attention_ffn_modules = []
        other_trainable = []
        
        for name in trainable_modules:
            if any(keyword in name for keyword in ['attn', 'mlp', 'fc1', 'fc2', 'qkv', 'proj']):
                attention_ffn_modules.append(name)
            else:
                other_trainable.append(name)
        
        print(f"\næ³¨æ„åŠ›/FFNå¯è®­ç»ƒæ¨¡å—: {len(attention_ffn_modules)}")
        print("å‰5ä¸ª:")
        for name in attention_ffn_modules[:5]:
            print(f"  - {name}")
            
        if other_trainable:
            print(f"\nå…¶ä»–å¯è®­ç»ƒæ¨¡å—: {len(other_trainable)}")
            for name in other_trainable:
                print(f"  - {name}")
        else:
            print("\nâœ“ æ²¡æœ‰å…¶ä»–å¯è®­ç»ƒæ¨¡å—ï¼Œåªæœ‰æ³¨æ„åŠ›/FFNæ¨¡å—æ˜¯å¯è®­ç»ƒçš„")
        
        # æ£€æŸ¥æ¢¯åº¦æŠ•å½±é’©å­æ˜¯å¦æ­£ç¡®æ³¨å†Œ
        print("\n=== æ£€æŸ¥æ¢¯åº¦æŠ•å½±é’©å­ ===")
        hook_registered_params = list(nullspace_model._param_to_name.keys())
        print(f"æ³¨å†Œäº†æ¢¯åº¦æŠ•å½±é’©å­çš„å‚æ•°æ•°é‡: {len(hook_registered_params)}")
        
        # éªŒè¯è¿™äº›å‚æ•°éƒ½å±äºæ³¨æ„åŠ›æˆ–FFNæ¨¡å—
        hook_modules = []
        for param in hook_registered_params:
            name = nullspace_model._param_to_name[param]
            hook_modules.append(name)
        
        print("å‰5ä¸ªæ³¨å†Œäº†é’©å­çš„æ¨¡å—:")
        for name in hook_modules[:5]:
            print(f"  - {name}")
        
        # éªŒè¯æ‰€æœ‰é’©å­æ¨¡å—éƒ½æ˜¯æ³¨æ„åŠ›æˆ–FFNæ¨¡å—
        non_attention_ffn_hooks = []
        for name in hook_modules:
            if not any(keyword in name for keyword in ['attn', 'mlp', 'fc1', 'fc2', 'qkv', 'proj', 'final_norm']):
                non_attention_ffn_hooks.append(name)
        
        if non_attention_ffn_hooks:
            print(f"\nâœ— å‘ç°éæ³¨æ„åŠ›/FFNæ¨¡å—çš„é’©å­: {non_attention_ffn_hooks}")
            return False
        else:
            print("\nâœ“ æ‰€æœ‰é’©å­éƒ½æ³¨å†Œåœ¨æ³¨æ„åŠ›/FFNæ¨¡å—ä¸Š")
        
        # æµ‹è¯•æŠ•å½±çŸ©é˜µæ›´æ–°
        print("\n=== æµ‹è¯•æŠ•å½±çŸ©é˜µæ›´æ–° ===")
        module_names = nullspace_model.get_module_names()
        print(f"æ¨¡å—æ•°é‡: {len(module_names)}")
        print("å‰5ä¸ªæ¨¡å—:")
        for name in module_names[:5]:
            print(f"  - {name}")
        
        # åˆ›å»ºæ¨¡æ‹Ÿåæ–¹å·®çŸ©é˜µå¹¶æ›´æ–°
        mock_covariances = {}
        for name in module_names[:2]:  # åªæµ‹è¯•å‰2ä¸ª
            mock_cov = torch.eye(768)  # ç®€å•çš„å•ä½çŸ©é˜µ
            mock_covariances[name] = mock_cov
        
        nullspace_model.update_projection_matrices(mock_covariances, soft_projection=True, weight_temp=1.0)
        
        # æ£€æŸ¥æŠ•å½±çŸ©é˜µæ˜¯å¦æ­£ç¡®å­˜å‚¨
        for name in mock_covariances.keys():
            if name in nullspace_model.projection_matrices:
                proj = nullspace_model.projection_matrices[name]
                print(f"âœ“ æ¨¡å— {name} çš„æŠ•å½±çŸ©é˜µå½¢çŠ¶: {proj.shape}")
            else:
                print(f"âœ— æ¨¡å— {name} çš„æŠ•å½±çŸ©é˜µæœªæ‰¾åˆ°")
                return False
        
        return True
        
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_gradient_projection_simple()
    if success:
        print("\nğŸ‰ æ¢¯åº¦æŠ•å½±æœºåˆ¶æµ‹è¯•é€šè¿‡ï¼")
        print("âœ“ ç¡®è®¤ï¼šåªæœ‰æ³¨æ„åŠ›æ¨¡å—å’ŒFFNæ¨¡å—å®ç°äº†æ¢¯åº¦ä¿®æ­£")
    else:
        print("\nâŒ æ¢¯åº¦æŠ•å½±æœºåˆ¶æµ‹è¯•å¤±è´¥ï¼")