#!/usr/bin/env python3
"""
æµ‹è¯•å¢é‡æ‹†åˆ†ä»»åŠ¡å‘½åæ˜¾ç¤ºçš„ä¿®å¤æ•ˆæœ
"""

import sys
import os
import tempfile
import shutil
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.abspath('.'))

from models.subspace_lora import SubspaceLoRA
from utils.balanced_cross_domain_data_manager import BalancedCrossDomainDataManagerCore

def test_incremental_task_naming():
    """æµ‹è¯•å¢é‡æ‹†åˆ†åä»»åŠ¡åç§°æ˜¾ç¤ºæ˜¯å¦æ­£ç¡®"""
    
    print("ğŸ§ª æµ‹è¯•å¢é‡æ‹†åˆ†ä»»åŠ¡å‘½åæ˜¾ç¤ºä¿®å¤")
    print("=" * 60)
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºæµ‹è¯•
    with tempfile.TemporaryDirectory() as temp_dir:
        try:
            # åˆ›å»ºå¹³è¡¡æ•°æ®ç®¡ç†å™¨ï¼Œå¯ç”¨å¢é‡æ‹†åˆ†
            dataset_names = ['cifar100_224', 'imagenet-r', 'cars196_224']
            
            print(f"ğŸ“Š åŸå§‹æ•°æ®é›†åˆ—è¡¨: {dataset_names}")
            
            # åˆ›å»ºæ•°æ®ç®¡ç†å™¨
            data_manager = BalancedCrossDomainDataManagerCore(
                dataset_names=dataset_names,
                balanced_datasets_root="balanced_datasets",
                shuffle=False,
                seed=42,
                num_shots=0,
                use_balanced_datasets=False,  # ä½¿ç”¨åŸå§‹æ•°æ®é›†
                enable_incremental_split=True,
                num_incremental_splits=2,
                incremental_split_seed=42
            )
            
            print(f"ğŸ“ˆ å¢é‡æ‹†åˆ†åä»»åŠ¡æ•°é‡: {data_manager.nb_tasks}")
            print(f"ğŸ“ˆ ä»»åŠ¡æ•°æ®é›†ä¿¡æ¯:")
            for i, dataset in enumerate(data_manager.datasets):
                original_name = dataset.get('original_dataset_name', dataset['name'])
                print(f"  ä»»åŠ¡ {i}: {dataset['name']} -> åŸå§‹åç§°: {original_name}")
            
            # æ¨¡æ‹Ÿanalyze_task_resultså‡½æ•°ä¸­çš„æ•°æ®é›†åç§°æ˜ å°„é€»è¾‘
            def test_dataset_name_mapping(data_manager, task_id, dataset_names):
                """æµ‹è¯•æ•°æ®é›†åç§°æ˜ å°„é€»è¾‘"""
                if hasattr(data_manager, 'datasets') and task_id < len(data_manager.datasets):
                    dataset_info = data_manager.datasets[task_id]
                    if 'original_dataset_name' in dataset_info:
                        # å¢é‡æ‹†åˆ†æƒ…å†µï¼šä½¿ç”¨åŸå§‹æ•°æ®é›†åç§°
                        dataset_name = dataset_info['original_dataset_name']
                    elif 'name' in dataset_info:
                        # æ™®é€šæƒ…å†µï¼šä½¿ç”¨æ•°æ®é›†åç§°
                        dataset_name = dataset_info['name']
                    elif dataset_names and task_id < len(dataset_names):
                        # å›é€€åˆ°ä¼ å…¥çš„dataset_names
                        dataset_name = dataset_names[task_id]
                    else:
                        # æœ€åå›é€€
                        dataset_name = f"Task {task_id}"
                else:
                    # å›é€€æ–¹æ¡ˆ
                    dataset_name = dataset_names[task_id] if dataset_names and task_id < len(dataset_names) else f"Task {task_id}"
                
                # æ¸…ç†æ•°æ®é›†åç§°ï¼Œä½¿å…¶æ›´ç¾è§‚
                if dataset_name.endswith('_split_0') or dataset_name.endswith('_split_1'):
                    dataset_name = dataset_name.split('_split_')[0]
                elif dataset_name.endswith('_224'):
                    # ä¿æŒ_224åç¼€ä»¥åŒºåˆ†ä¸åŒåˆ†è¾¨ç‡
                    pass
                
                return dataset_name
            
            # æµ‹è¯•æ•°æ®é›†åç§°æ˜ å°„
            print(f"\nğŸ§ª æµ‹è¯•æ•°æ®é›†åç§°æ˜ å°„:")
            for task_id in range(data_manager.nb_tasks):
                mapped_name = test_dataset_name_mapping(data_manager, task_id, dataset_names)
                print(f"  ä»»åŠ¡ {task_id} -> {mapped_name}")
            
            # éªŒè¯æ˜ å°„ç»“æœ
            expected_mapping = {
                0: 'cifar100_224',
                1: 'cifar100_224', 
                2: 'imagenet-r',
                3: 'imagenet-r',
                4: 'cars196_224',
                5: 'cars196_224'
            }
            
            print(f"\nâœ… éªŒè¯æ˜ å°„ç»“æœ:")
            success_count = 0
            for task_id in range(data_manager.nb_tasks):
                mapped_name = test_dataset_name_mapping(data_manager, task_id, dataset_names)
                expected_name = expected_mapping.get(task_id, f"Task {task_id}")
                
                if mapped_name == expected_name:
                    print(f"  âœ… ä»»åŠ¡ {task_id}: {mapped_name} (æ­£ç¡®)")
                    success_count += 1
                else:
                    print(f"  âŒ ä»»åŠ¡ {task_id}: {mapped_name} (æœŸæœ›: {expected_name})")
            
            success_rate = success_count / data_manager.nb_tasks * 100
            print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {success_count}/{data_manager.nb_tasks} ({success_rate:.1f}%) æˆåŠŸ")
            
            if success_rate >= 90:
                print("ğŸ‰ å¢é‡æ‹†åˆ†ä»»åŠ¡å‘½åæ˜¾ç¤ºä¿®å¤æˆåŠŸï¼")
                return True
            else:
                print("âŒ å¢é‡æ‹†åˆ†ä»»åŠ¡å‘½åæ˜¾ç¤ºä¿®å¤å¤±è´¥ï¼")
                return False
                
        except Exception as e:
            print(f"âŒ æµ‹è¯•æ‰§è¡Œå‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return False

if __name__ == "__main__":
    success = test_incremental_task_naming()
    sys.exit(0 if success else 1)