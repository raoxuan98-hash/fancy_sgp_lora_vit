#!/usr/bin/env python3
"""
æµ‹è¯•å¢é‡æ‹†åˆ†æ ‡ç­¾æ˜ å°„ä¿®å¤æ˜¯å¦æœ‰æ•ˆ
"""

import logging
import numpy as np
import torch
from torch.utils.data import DataLoader

from utils.balanced_cross_domain_data_manager import create_balanced_data_manager

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(filename)s:%(lineno)d] %(levelname)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

def test_incremental_split_label_mapping():
    """æµ‹è¯•å¢é‡æ‹†åˆ†çš„æ ‡ç­¾æ˜ å°„æ˜¯å¦æ­£ç¡®"""
    print("=" * 80)
    print("æµ‹è¯•å¢é‡æ‹†åˆ†çš„æ ‡ç­¾æ˜ å°„")
    print("=" * 80)
    
    # åˆ›å»ºå¹³è¡¡æ•°æ®ç®¡ç†å™¨ï¼ˆå¯ç”¨å¢é‡æ‹†åˆ†ï¼‰
    datasets = ['cifar100_224', 'cub200_224']
    
    manager = create_balanced_data_manager(
        dataset_names=datasets,
        balanced_datasets_root="balanced_datasets",
        use_balanced_datasets=True,
        enable_incremental_split=True,
        num_incremental_splits=3,
        incremental_split_seed=42
    )
    
    print(f"\næ•°æ®ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ:")
    print(f"  - æ€»ä»»åŠ¡æ•°: {manager.nb_tasks}")
    print(f"  - æ€»ç±»åˆ«æ•°: {manager.num_classes}")
    print(f"  - å¢é‡æ‹†åˆ†å¯ç”¨: {manager.enable_incremental_split}")
    
    # æµ‹è¯•æ¯ä¸ªä»»åŠ¡çš„æ ‡ç­¾èŒƒå›´
    print(f"\næµ‹è¯•æ¯ä¸ªä»»åŠ¡çš„æ ‡ç­¾èŒƒå›´:")
    for task_id in range(min(6, manager.nb_tasks)):  # æµ‹è¯•å‰6ä¸ªä»»åŠ¡
        dataset_info = manager.datasets[task_id]
        global_offset = manager.global_label_offset[task_id]
        
        # è·å–è®­ç»ƒé›†
        train_set = manager.get_incremental_subset(
            task=task_id, source="train", cumulative=False, mode="test")
        train_loader = DataLoader(train_set, batch_size=32, shuffle=False)
        
        # è·å–æµ‹è¯•é›†
        test_set = manager.get_incremental_subset(
            task=task_id, source="test", cumulative=False, mode="test")
        test_loader = DataLoader(test_set, batch_size=32, shuffle=False)
        
        # æ”¶é›†æ‰€æœ‰æ ‡ç­¾
        train_labels = []
        for batch in train_loader:
            train_labels.extend(batch[1].numpy())
        
        test_labels = []
        for batch in test_loader:
            test_labels.extend(batch[1].numpy())
        
        train_labels = np.array(train_labels)
        test_labels = np.array(test_labels)
        
        print(f"\nä»»åŠ¡ {task_id} ({dataset_info['name']}):")
        print(f"  - æ•°æ®é›†ç±»åˆ«æ•°: {dataset_info['num_classes']}")
        print(f"  - å…¨å±€åç§»: {global_offset}")
        print(f"  - è®­ç»ƒé›†æ ‡ç­¾èŒƒå›´: {np.min(train_labels)} - {np.max(train_labels)}")
        print(f"  - æµ‹è¯•é›†æ ‡ç­¾èŒƒå›´: {np.min(test_labels)} - {np.max(test_labels)}")
        print(f"  - æœŸæœ›æ ‡ç­¾èŒƒå›´: {global_offset} - {global_offset + dataset_info['num_classes'] - 1}")
        
        # éªŒè¯æ ‡ç­¾èŒƒå›´æ˜¯å¦æ­£ç¡®
        expected_min = global_offset
        expected_max = global_offset + dataset_info['num_classes'] - 1
        
        train_min_ok = np.min(train_labels) == expected_min
        train_max_ok = np.max(train_labels) == expected_max
        test_min_ok = np.min(test_labels) == expected_min
        test_max_ok = np.max(test_labels) == expected_max
        
        print(f"  - è®­ç»ƒé›†æ ‡ç­¾èŒƒå›´æ­£ç¡®: {'âœ“' if train_min_ok and train_max_ok else 'âœ—'}")
        print(f"  - æµ‹è¯•é›†æ ‡ç­¾èŒƒå›´æ­£ç¡®: {'âœ“' if test_min_ok and test_max_ok else 'âœ—'}")
        
        # éªŒè¯æ ‡ç­¾æ˜¯å¦è¿ç»­
        train_unique = np.unique(train_labels)
        test_unique = np.unique(test_labels)
        expected_labels = np.arange(expected_min, expected_max + 1)
        
        train_continuous = np.array_equal(np.sort(train_unique), expected_labels)
        test_continuous = np.array_equal(np.sort(test_unique), expected_labels)
        
        print(f"  - è®­ç»ƒé›†æ ‡ç­¾è¿ç»­: {'âœ“' if train_continuous else 'âœ—'}")
        print(f"  - æµ‹è¯•é›†æ ‡ç­¾è¿ç»­: {'âœ“' if test_continuous else 'âœ—'}")
        
        if not (train_min_ok and train_max_ok and test_min_ok and test_max_ok and 
                train_continuous and test_continuous):
            print(f"  âŒ ä»»åŠ¡ {task_id} æµ‹è¯•å¤±è´¥!")
            return False
        else:
            print(f"  âœ… ä»»åŠ¡ {task_id} æµ‹è¯•é€šè¿‡!")
    
    print(f"\nğŸ‰ æ‰€æœ‰ä»»åŠ¡æµ‹è¯•é€šè¿‡!")
    return True

def test_cumulative_mode():
    """æµ‹è¯•ç´¯ç§¯æ¨¡å¼æ˜¯å¦æ­£ç¡®"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•ç´¯ç§¯æ¨¡å¼")
    print("=" * 80)
    
    # åˆ›å»ºå¹³è¡¡æ•°æ®ç®¡ç†å™¨ï¼ˆå¯ç”¨å¢é‡æ‹†åˆ†ï¼‰
    datasets = ['cifar100_224', 'cub200_224']
    
    manager = create_balanced_data_manager(
        dataset_names=datasets,
        balanced_datasets_root="balanced_datasets",
        use_balanced_datasets=True,
        enable_incremental_split=True,
        num_incremental_splits=3,
        incremental_split_seed=42
    )
    
    # æµ‹è¯•ç´¯ç§¯æ¨¡å¼
    for task_id in [1, 3, 5]:  # æµ‹è¯•å‡ ä¸ªä»»åŠ¡
        if task_id >= manager.nb_tasks:
            continue
            
        print(f"\næµ‹è¯•ä»»åŠ¡ {task_id} çš„ç´¯ç§¯æ¨¡å¼:")
        
        # è·å–ç´¯ç§¯æµ‹è¯•é›†
        cumulative_test_set = manager.get_incremental_subset(
            task=task_id, source="test", cumulative=True, mode="test")
        cumulative_test_loader = DataLoader(cumulative_test_set, batch_size=32, shuffle=False)
        
        # æ”¶é›†æ‰€æœ‰æ ‡ç­¾
        cumulative_labels = []
        for batch in cumulative_test_loader:
