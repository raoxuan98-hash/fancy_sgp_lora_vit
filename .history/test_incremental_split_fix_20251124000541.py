#!/usr/bin/env python3
"""
æµ‹è¯•å¢é‡æ‹†åˆ†æ ‡ç­¾æ˜ å°„ä¿®å¤æ˜¯å¦æœ‰æ•ˆ
"""

import logging
import numpy as np
import torch
from torch.utils.data import DataLoader

from utils.balanced_cross_domain_data_manager import create_balanced_data_manager

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(filename)s:%(lineno)d] %(levelname)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

def test_incremental_split_label_mapping():
    """æµ‹è¯•å¢é‡æ‹†åˆ†çš„æ ‡ç­¾æ˜ å°„æ˜¯å¦æ­£ç¡®"""
    print("=" * 80)
    print("æµ‹è¯•å¢é‡æ‹†åˆ†çš„æ ‡ç­¾æ˜ å°„")
    print("=" * 80)
    
    # åˆ›å»ºå¹³è¡¡æ•°æ®ç®¡ç†å™¨ï¼ˆå¯ç”¨å¢é‡æ‹†åˆ†ï¼‰
    datasets = ['cifar100_224', 'cub200_224']
    
    manager = create_balanced_data_manager(
        dataset_names=datasets,
        balanced_datasets_root="balanced_datasets",
        use_balanced_datasets=True,
        enable_incremental_split=True,
        num_incremental_splits=3,
        incremental_split_seed=42
    )
    
    print(f"\næ•°æ®ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ:")
    print(f"  - æ€»ä»»åŠ¡æ•°: {manager.nb_tasks}")
    print(f"  - æ€»ç±»åˆ«æ•°: {manager.num_classes}")
    print(f"  - å¢é‡æ‹†åˆ†å¯ç”¨: {manager.enable_incremental_split}")
    
    # æµ‹è¯•æ¯ä¸ªä»»åŠ¡çš„æ ‡ç­¾èŒƒå›´
    print(f"\næµ‹è¯•æ¯ä¸ªä»»åŠ¡çš„æ ‡ç­¾èŒƒå›´:")
    for task_id in range(min(6, manager.nb_tasks)):  # æµ‹è¯•å‰6ä¸ªä»»åŠ¡
        dataset_info = manager.datasets[task_id]
        global_offset = manager.global_label_offset[task_id]
        
        # è·å–è®­ç»ƒé›†
        train_set = manager.get_incremental_subset(
            task=task_id, source="train", cumulative=False, mode="test")
        train_loader = DataLoader(train_set, batch_size=32, shuffle=False)
        
        # è·å–æµ‹è¯•é›†
        test_set = manager.get_incremental_subset(
            task=task_id, source="test", cumulative=False, mode="test")
        test_loader = DataLoader(test_set, batch_size=32, shuffle=False)
        
        # æ”¶é›†æ‰€æœ‰æ ‡ç­¾
        train_labels = []
        for batch in train_loader:
            train_labels.extend(batch[1].numpy())
        
        test_labels = []
        for batch in test_loader:
            test_labels.extend(batch[1].numpy())
        
        train_labels = np.array(train_labels)
        test_labels = np.array(test_labels)
        
        print(f"\nä»»åŠ¡ {task_id} ({dataset_info['name']}):")
        print(f"  - æ•°æ®é›†ç±»åˆ«æ•°: {dataset_info['num_classes']}")
        print(f"  - å…¨å±€åç§»: {global_offset}")
        print(f"  - è®­ç»ƒé›†æ ‡ç­¾èŒƒå›´: {np.min(train_labels)} - {np.max(train_labels)}")
        print(f"  - æµ‹è¯•é›†æ ‡ç­¾èŒƒå›´: {np.min(test_labels)} - {np.max(test_labels)}")
        print(f"  - æœŸæœ›æ ‡ç­¾èŒƒå›´: {global_offset} - {global_offset + dataset_info['num_classes'] - 1}")
        
        # éªŒè¯æ ‡ç­¾èŒƒå›´æ˜¯å¦æ­£ç¡®
        expected_min = global_offset
        expected_max = global_offset + dataset_info['num_classes'] - 1
        
        train_min_ok = np.min(train_labels) == expected_min
        train_max_ok = np.max(train_labels) == expected_max
        test_min_ok = np.min(test_labels) == expected_min
        test_max_ok = np.max(test_labels) == expected_max
        
        print(f"  - è®­ç»ƒé›†æ ‡ç­¾èŒƒå›´æ­£ç¡®: {'âœ“' if train_min_ok and train_max_ok else 'âœ—'}")
        print(f"  - æµ‹è¯•é›†æ ‡ç­¾èŒƒå›´æ­£ç¡®: {'âœ“' if test_min_ok and test_max_ok else 'âœ—'}")
        
        # éªŒè¯æ ‡ç­¾æ˜¯å¦è¿ç»­
        train_unique = np.unique(train_labels)
        test_unique = np.unique(test_labels)
        expected_labels = np.arange(expected_min, expected_max + 1)
        
        train_continuous = np.array_equal(np.sort(train_unique), expected_labels)
        test_continuous = np.array_equal(np.sort(test_unique), expected_labels)
        
        print(f"  - è®­ç»ƒé›†æ ‡ç­¾è¿ç»­: {'âœ“' if train_continuous else 'âœ—'}")
        print(f"  - æµ‹è¯•é›†æ ‡ç­¾è¿ç»­: {'âœ“' if test_continuous else 'âœ—'}")
        
        if not (train_min_ok and train_max_ok and test_min_ok and test_max_ok and 
                train_continuous and test_continuous):
            print(f"  âŒ ä»»åŠ¡ {task_id} æµ‹è¯•å¤±è´¥!")
            return False
        else:
            print(f"  âœ… ä»»åŠ¡ {task_id} æµ‹è¯•é€šè¿‡!")
    
    print(f"\nğŸ‰ æ‰€æœ‰ä»»åŠ¡æµ‹è¯•é€šè¿‡!")
    return True

def test_cumulative_mode():
    """æµ‹è¯•ç´¯ç§¯æ¨¡å¼æ˜¯å¦æ­£ç¡®"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•ç´¯ç§¯æ¨¡å¼")
    print("=" * 80)
    
    # åˆ›å»ºå¹³è¡¡æ•°æ®ç®¡ç†å™¨ï¼ˆå¯ç”¨å¢é‡æ‹†åˆ†ï¼‰
    datasets = ['cifar100_224', 'cub200_224']
    
    manager = create_balanced_data_manager(
        dataset_names=datasets,
        balanced_datasets_root="balanced_datasets",
        use_balanced_datasets=True,
        enable_incremental_split=True,
        num_incremental_splits=3,
        incremental_split_seed=42
    )
    
    # æµ‹è¯•ç´¯ç§¯æ¨¡å¼
    for task_id in [1, 3, 5]:  # æµ‹è¯•å‡ ä¸ªä»»åŠ¡
        if task_id >= manager.nb_tasks:
            continue
            
        print(f"\næµ‹è¯•ä»»åŠ¡ {task_id} çš„ç´¯ç§¯æ¨¡å¼:")
        
        # è·å–ç´¯ç§¯æµ‹è¯•é›†
        cumulative_test_set = manager.get_incremental_subset(
            task=task_id, source="test", cumulative=True, mode="test")
        cumulative_test_loader = DataLoader(cumulative_test_set, batch_size=32, shuffle=False)
        
        # æ”¶é›†æ‰€æœ‰æ ‡ç­¾
        cumulative_labels = []
        for batch in cumulative_test_loader:
            cumulative_labels.extend(batch[1].numpy())
        
        cumulative_labels = np.array(cumulative_labels)
        
        # è®¡ç®—æœŸæœ›çš„æ ‡ç­¾èŒƒå›´
        total_classes_up_to_task = sum(manager.datasets[i]['num_classes'] for i in range(task_id + 1))
        expected_min = 0
        expected_max = total_classes_up_to_task - 1
        
        print(f"  - ç´¯ç§¯æ ‡ç­¾èŒƒå›´: {np.min(cumulative_labels)} - {np.max(cumulative_labels)}")
        print(f"  - æœŸæœ›æ ‡ç­¾èŒƒå›´: {expected_min} - {expected_max}")
        
        # éªŒè¯æ ‡ç­¾èŒƒå›´
        min_ok = np.min(cumulative_labels) == expected_min
        max_ok = np.max(cumulative_labels) == expected_max
        
        print(f"  - ç´¯ç§¯æ ‡ç­¾èŒƒå›´æ­£ç¡®: {'âœ“' if min_ok and max_ok else 'âœ—'}")
        
        # éªŒè¯æ ‡ç­¾æ˜¯å¦åŒ…å«æ‰€æœ‰æœŸæœ›çš„æ ‡ç­¾
        unique_labels = np.unique(cumulative_labels)
        expected_labels = np.arange(expected_min, expected_max + 1)
        
        # æ³¨æ„ï¼šç”±äºå¢é‡æ‹†åˆ†ï¼Œå¯èƒ½ä¸æ˜¯æ‰€æœ‰æ ‡ç­¾éƒ½å‡ºç°ï¼Œä½†èŒƒå›´åº”è¯¥æ˜¯æ­£ç¡®çš„
        range_ok = (np.min(unique_labels) >= expected_min and 
                   np.max(unique_labels) <= expected_max)
        
        print(f"  - ç´¯ç§¯æ ‡ç­¾åœ¨æœŸæœ›èŒƒå›´å†…: {'âœ“' if range_ok else 'âœ—'}")
        
        if not (min_ok and max_ok and range_ok):
            print(f"  âŒ ä»»åŠ¡ {task_id} ç´¯ç§¯æ¨¡å¼æµ‹è¯•å¤±è´¥!")
            return False
        else:
            print(f"  âœ… ä»»åŠ¡ {task_id} ç´¯ç§¯æ¨¡å¼æµ‹è¯•é€šè¿‡!")
    
    print(f"\nğŸ‰ æ‰€æœ‰ç´¯ç§¯æ¨¡å¼æµ‹è¯•é€šè¿‡!")
    return True

def test_training_simulation():
    """æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œæµ‹è¯•æ˜¯å¦ä¼šæŠ¥é”™"""
    print("\n" + "=" * 80)
    print("æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹")
    print("=" * 80)
    
    # åˆ›å»ºå¹³è¡¡æ•°æ®ç®¡ç†å™¨ï¼ˆå¯ç”¨å¢é‡æ‹†åˆ†ï¼‰
    datasets = ['cifar100_224', 'cub200_224']
    
    manager = create_balanced_data_manager(
        dataset_names=datasets,
        balanced_datasets_root="balanced_datasets",
        use_balanced_datasets=True,
        enable_incremental_split=True,
        num_incremental_splits=3,
        incremental_split_seed=42
    )
    
    # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    for task_id in range(min(3, manager.nb_tasks)):  # åªæµ‹è¯•å‰3ä¸ªä»»åŠ¡
        print(f"\næ¨¡æ‹Ÿä»»åŠ¡ {task_id} è®­ç»ƒ:")
        
        try:
            # è·å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            train_set = manager.get_incremental_subset(
                task=task_id, source="train", cumulative=False, mode="train")
            test_set = manager.get_incremental_subset(
                task=task_id, source="test", cumulative=True, mode="test")
            
            train_loader = DataLoader(train_set, batch_size=16, shuffle=True)
            test_loader = DataLoader(test_set, batch_size=32, shuffle=False)
            
            # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
            train_batch = next(iter(train_loader))
            test_batch = next(iter(test_loader))
            
            train_inputs, train_targets = train_batch[0], train_batch[1]
            test_inputs, test_targets = test_batch[0], test_batch[1]
            
            print(f"  - è®­ç»ƒæ‰¹æ¬¡: è¾“å…¥å½¢çŠ¶ {train_inputs.shape}, ç›®æ ‡å½¢çŠ¶ {train_targets.shape}")
            print(f"  - è®­ç»ƒæ ‡ç­¾èŒƒå›´: {train_targets.min().item()} - {train_targets.max().item()}")
            print(f"  - æµ‹è¯•æ‰¹æ¬¡: è¾“å…¥å½¢çŠ¶ {test_inputs.shape}, ç›®æ ‡å½¢çŠ¶ {test_targets.shape}")
            print(f"  - æµ‹è¯•æ ‡ç­¾èŒƒå›´: {test_targets.min().item()} - {test_targets.max().item()}")
            
            # æ¨¡æ‹Ÿåˆ†ç±»å™¨åˆ›å»ºï¼ˆç®€åŒ–ç‰ˆï¼‰
            task_size = manager.get_task_size(task_id)
            total_classes = sum(manager.datasets[i]['num_classes'] for i in range(task_id + 1))
            
            print(f"  - ä»»åŠ¡ç±»åˆ«æ•°: {task_size}")
            print(f"  - ç´¯ç§¯ç±»åˆ«æ•°: {total_classes}")
            
            # æ¨¡æ‹Ÿæ ‡ç­¾å¤„ç†
            known_classes = sum(manager.datasets[i]['num_classes'] for i in range(task_id))
            new_targets_rel = torch.where(
                train_targets - known_classes >= 0,
                train_targets - known_classes, -100)
            
            print(f"  - å·²çŸ¥ç±»åˆ«æ•°: {known_classes}")
            print(f"  - ç›¸å¯¹æ ‡ç­¾èŒƒå›´: {new_targets_rel.min().item()} - {new_targets_rel.max().item()}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡ç­¾è¶…å‡ºèŒƒå›´
            valid_targets = new_targets_rel[new_targets_rel >= 0]
            if len(valid_targets) > 0:
                max_valid_target = valid_targets.max().item()
                if max_valid_target >= task_size:
                    print(f"  âŒ æ ‡ç­¾ {max_valid_target} è¶…å‡ºä»»åŠ¡å¤§å° {task_size}!")
                    return False
                else:
                    print(f"  âœ… æ ‡ç­¾èŒƒå›´æ­£ç¡® (0-{task_size-1})")
            
