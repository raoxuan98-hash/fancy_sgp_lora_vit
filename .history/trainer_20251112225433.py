import os
import sys
import logging
import torch
import random
import numpy as np
from collections.abc import Mapping, Sequence
# from models.subspace_lora import SubspaceLoRA
from models.subspace_lora import SubspaceLoRA
from utils.data_manager import WithinDomainDataManager, CrossDomainDataManagerCore
from utils.toolkit import count_parameters
import re

def train(args):
    all_results = {}
    
    for run_id, seed in enumerate(args['seed_list']):
        args['seed'], args['run_id'] = seed, run_id
        logfile_head, logfile_name = build_log_dirs(args)
        args['log_path'] = logfile_name
        
        # Configure logging with unbuffered file handler for real-time updates
        log_file_path = os.path.join(logfile_name, 'record.log')
        
        # æ¸…é™¤ç°æœ‰çš„æ—¥å¿—å¤„ç†å™¨ï¼Œé¿å…å†²çª
        root_logger = logging.getLogger()
        for handler in root_logger.handlers[:]:
            root_logger.removeHandler(handler)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(filename=log_file_path, mode='a', encoding='utf-8')
        file_handler.stream.reconfigure(line_buffering=True)  # Enable line buffering
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        
        # è®¾ç½®æ ¼å¼
        formatter = logging.Formatter('%(asctime)s [%(filename)s] => %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
        root_logger.setLevel(logging.INFO)
        root_logger.addHandler(file_handler)
        root_logger.addHandler(console_handler)
        
        # æ‰“å°æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œæ–¹ä¾¿ç”¨æˆ·æŸ¥æ‰¾
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_file_path}")
        print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ 'tail -f {log_file_path}' å®æ—¶æŸ¥çœ‹æ—¥å¿—")
        print("-" * 80)
        
        args['log_path'] = logfile_name
        results = train_single_run(args)
        all_results[f"seed_{seed}"] = results
    

def train_single_run(args, return_model: bool = False):
    # Setting random seed and device for reproducibility
    set_random(args['seed'])
    print_args(args)
    
    # Initialize data manager and model

    if args['cross_domain']:
        data_manager = CrossDomainDataManagerCore(
            dataset_names=args['dataset'],
            shuffle=args['shuffle'],
            seed=args['seed'])
    else:
        data_manager = WithinDomainDataManager(
            dataset_name=args['dataset'],
            shuffle=args['shuffle'],
            seed=args['seed'],
            init_cls=args['init_cls'],
            increment=args['increment'],
            args=args)
    
    model = SubspaceLoRA(args)
    logging.info(f'All params: {count_parameters(model.network)}')
    logging.info(f'Trainable params: {count_parameters(model.network, True)}')
    final_results = model.loop(data_manager)
    
    # æ·»åŠ log_pathåˆ°ç»“æœä¸­ï¼Œä»¥ä¾¿aggregate_seed_resultså¯ä»¥æ‰¾åˆ°å®ƒ
    if 'log_path' in args:
        final_results['log_path'] = args['log_path']
    
    if return_model:
        return final_results, model
    return final_results

def set_device(device_type):
    """Properly set the device (either CPU or GPU) based on input"""
    if isinstance(device_type, (list, tuple)):
        return [torch.device(f'cuda:{d}' if d != -1 else 'cpu') for d in device_type]
    return torch.device('cuda' if device_type != -1 else 'cpu')

def set_random(seed):
    """Set random seeds to ensure reproducibility"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def print_args(args):
    """Log the arguments for this run"""
    for key, value in args.items():
        logging.info(f'{key}: {value}')


import os
from pathlib import Path
import json

def _filter_args_by_lora_type(args: dict) -> dict:
    """
    è¿‡æ»¤å‚æ•°å­—å…¸ï¼Œåªä¿ç•™ä¸å½“å‰LoRAç±»å‹ç›¸å…³çš„å‚æ•°
    è¿™æ ·å¯ä»¥é¿å…åœ¨params.jsonä¸­ä¿å­˜ä¸ç›¸å…³çš„å‚æ•°ï¼Œå¯¼è‡´æ—¥å¿—å‘½åæ··ä¹±
    """
    lora_type = args.get('lora_type', 'basic_lora')
    filtered_args = args.copy()
    
    # å®šä¹‰æ¯ç§LoRAç±»å‹ç›¸å…³çš„å‚æ•°
    sgp_lora_params = {'weight_temp', 'weight_kind', 'weight_p'}
    nsp_lora_params = {'nsp_eps', 'nsp_weight'}
    
    # ç§»é™¤ä¸å½“å‰LoRAç±»å‹ä¸ç›¸å…³çš„å‚æ•°
    if lora_type == 'sgp_lora':
        # ä¿ç•™SGPå‚æ•°ï¼Œç§»é™¤NSPå‚æ•°
        for param in nsp_lora_params:
            filtered_args.pop(param, None)
    elif lora_type == 'nsp_lora':
        # ä¿ç•™NSPå‚æ•°ï¼Œç§»é™¤SGPå‚æ•°
        for param in sgp_lora_params:
            filtered_args.pop(param, None)
    elif lora_type == 'basic_lora':
        # ç§»é™¤æ‰€æœ‰LoRAç‰¹å®šå‚æ•°
        for param in sgp_lora_params.union(nsp_lora_params):
            filtered_args.pop(param, None)
    elif lora_type == 'full':
        # ç§»é™¤æ‰€æœ‰LoRAç‰¹å®šå‚æ•°
        for param in sgp_lora_params.union(nsp_lora_params):
            filtered_args.pop(param, None)
    
    return filtered_args

def build_log_dirs(args: dict, root_dir="."):
    """æ ¹æ® args æ„å»ºå¤šçº§æ—¥å¿—ç›®å½•ï¼Œç¡®ä¿ä¸åŒ LoRA ç±»å‹çš„å‚æ•°æ­£ç¡®åˆ†ç¦»"""

    def sanitize_filename(s: str) -> str:
        """ç§»é™¤æˆ–æ›¿æ¢æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
        # Windows éæ³•å­—ç¬¦: \ / : * ? " < > |
        s = re.sub(r'[\\/:*?"<>|]', '_', str(s))
        # å¯é€‰ï¼šå‹ç¼©è¿ç»­ä¸‹åˆ’çº¿
        s = re.sub(r'_+', '_', s)
        return s.strip('_')

    def short(s: str, maxlen=40):
        """æˆªæ–­è¿‡é•¿å­—ç¬¦ä¸²ï¼Œä¸åŠ  hashï¼Œä»…ä¿ç•™å¯è¯»æ€§"""
        s = sanitize_filename(str(s))
        if len(s) <= maxlen:
            return s
        return s[:maxlen].rstrip('_')  # é¿å…æˆªæ–­åœ¨ä¸‹åˆ’çº¿å¤„

    def _get_lora_specific_params(lora_type: str, args: dict) -> list:
        """è·å–ç‰¹å®š LoRA ç±»å‹çš„å‚æ•°ï¼Œé¿å…äº¤å‰æ±¡æŸ“"""
        params = []
        
        if lora_type == 'sgp_lora':
            # SGP LoRA ç‰¹æœ‰å‚æ•°
            if 'weight_temp' in args:
                params.append(f"t-{short(args['weight_temp'])}")
            if 'weight_kind' in args:
                params.append(f"k-{short(args['weight_kind'])}")
            # å§‹ç»ˆåŒ…å« weight_p å‚æ•°ï¼Œå³ä½¿æ˜¯é»˜è®¤å€¼ï¼Œä»¥ç¡®ä¿ä¸åŒå‚æ•°ç»„åˆçš„å®éªŒç»“æœè¢«æ­£ç¡®åŒºåˆ†
            if 'weight_p' in args:
                params.append(f"p-{short(args['weight_p'])}")
                
        elif lora_type == 'nsp_lora':
            # NSP LoRA ç‰¹æœ‰å‚æ•°
            if 'nsp_eps' in args:
                params.append(f"eps-{short(args['nsp_eps'])}")
            if 'nsp_weight' in args:
                params.append(f"w-{short(args['nsp_weight'])}")
                
        elif lora_type == 'basic_lora':
            # Basic LoRA é€šå¸¸æ²¡æœ‰é¢å¤–å‚æ•°ï¼Œä½†å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
            pass
            
        elif lora_type == 'full':
            # Full fine-tuning å¯èƒ½æœ‰çš„å‚æ•°
            pass
            
        return params

    def _get_kd_params(args: dict) -> list:
        """è·å–çŸ¥è¯†è’¸é¦ç›¸å…³å‚æ•°ï¼Œç»Ÿä¸€å‘½åè§„åˆ™"""
        kd_params = []
        
        if args.get('gamma_kd', 0.0) > 0.0:
            kd_params.append(f"kd-{short(args['gamma_kd'])}")
            if 'kd_type' in args:
                kd_params.append(f"type-{short(args['kd_type'])}")
            if 'distillation_transform' in args:
                kd_params.append(f"dt-{short(args['distillation_transform'])}")
            if args.get('use_aux_for_kd', False):
                kd_params.append("aux")
            # æ·»åŠ update_teacher_each_taskå‚æ•°ï¼Œç®€å†™ä¸ºutt
            if 'update_teacher_each_task' in args:
                kd_params.append(f"utt-{short(args['update_teacher_each_task'])}")
                
        return kd_params

    def _validate_parameters(args: dict) -> None:
        """éªŒè¯å‚æ•°ç»„åˆçš„åˆç†æ€§"""
        lora_type = args.get('lora_type', 'basic_lora')
        
        # æ£€æŸ¥ LoRA ç‰¹å®šå‚æ•°æ˜¯å¦è¢«è¯¯ç”¨
        if lora_type != 'sgp_lora':
            sgp_params = ['weight_temp', 'weight_kind', 'weight_p']
            for param in sgp_params:
                if param in args and args[param] is not None:
                    logging.warning(f"âš ï¸ Parameter '{param}' is being used with lora_type='{lora_type}', but it's specific to sgp_lora")
        
        if lora_type != 'nsp_lora':
            nsp_params = ['nsp_eps', 'nsp_weight']
            for param in nsp_params:
                if param in args and args[param] is not None:
                    logging.warning(f"âš ï¸ Parameter '{param}' is being used with lora_type='{lora_type}', but it's specific to nsp_lora")

    # å‚æ•°éªŒè¯
    _validate_parameters(args)

    # é¡¶å±‚ï¼šæ¨¡å‹å’Œç”¨æˆ·ä¿¡æ¯
    base_dir = os.path.join(
        root_dir,
        f"{short(args['model_name'])}_logs_{short(args['user'])}",
        f"{short(args['dataset'])}_{short(args['vit_type'])}"
    )

    # äºŒçº§ï¼šä»»åŠ¡è®¾ç½®
    task_dir = os.path.join(
        base_dir,
        f"init-{short(args['init_cls'])}_inc-{short(args['increment'])}",
        f"lrank-{short(args.get('lora_rank', 'NA'))}_ltype-{short(args.get('lora_type', 'NA'))}"
    )

    # ä¸‰çº§ï¼šLoRA ç‰¹å®šå‚æ•°ï¼ˆåªåŒ…å«ç›¸å…³çš„ï¼‰
    lora_params = _get_lora_specific_params(args.get('lora_type', 'basic_lora'), args)
    
    # å››çº§ï¼šçŸ¥è¯†è’¸é¦å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
    kd_params = _get_kd_params(args)
    
    # åˆå¹¶ LoRA å’Œ KD å‚æ•°
    method_params = lora_params + kd_params
    
    # æ„å»ºæ–¹æ³•å‚æ•°ç›®å½•
    if method_params:
        method_subdir = "_".join(method_params)
        method_dir = os.path.join(task_dir, short(method_subdir, maxlen=80))
    else:
        method_dir = task_dir

    # äº”çº§ï¼šä¼˜åŒ–å™¨å’Œè®­ç»ƒå‚æ•°ï¼ˆä¸åŒ…å«ç§å­ï¼Œç§å­å°†åœ¨å­ç›®å½•ä¸­å¤„ç†ï¼‰
    opt_params = [
        f"opt-{args['optimizer']}",
        f"lr-{short(args['lrate'])}",
        f"b-{args['batch_size']}",
        f"i-{args['iterations']}"
    ]
    opt_str = "_".join(opt_params)
    opt_dir = os.path.join(method_dir, short(opt_str, maxlen=80))

    # === é€çº§åˆ›å»ºç›®å½• ===
    abs_log_dir = os.path.abspath(opt_dir)
    current = Path(abs_log_dir).root
    for part in Path(abs_log_dir).parts[1:]:
        current = Path(current) / part
        current.mkdir(exist_ok=True)

    # ä¿å­˜è¿‡æ»¤åçš„å‚æ•°åˆ° JSONï¼Œé¿å…å‚æ•°äº¤å‰æ±¡æŸ“
    filtered_args = _filter_args_by_lora_type(args)
    params_json = Path(abs_log_dir) / "params.json"
    if not params_json.exists():
        with open(params_json, "w", encoding="utf-8") as f:
            json.dump(filtered_args, f, ensure_ascii=False, indent=2)

    # ä¸ºæ¯ä¸ªç§å­åˆ›å»ºå­ç›®å½•
    seed_dir = os.path.join(abs_log_dir, f"seed_{args['seed']}")
    os.makedirs(seed_dir, exist_ok=True)

    # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ä½¿ç”¨ logging.infoï¼Œå› ä¸ºæ—¥å¿—è¿˜æ²¡æœ‰é…ç½®
    # ç›®å½•ä¿¡æ¯ä¼šåœ¨æ—¥å¿—é…ç½®å®Œæˆåé€šè¿‡ print_args å‡½æ•°è®°å½•

    return os.path.dirname(abs_log_dir), str(seed_dir)