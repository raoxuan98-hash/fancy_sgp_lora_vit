import os
import sys
import logging
import torch
import random
import numpy as np
from collections.abc import Mapping, Sequence
# from models.subspace_lora import SubspaceLoRA
from models.subspace_lora import SubspaceLoRA
from utils.data_manager import WithinDomainDataManager, CrossDomainDataManagerCore
from utils.balanced_cross_domain_data_manager import BalancedCrossDomainDataManagerCore
from utils.toolkit import count_parameters
import re

def train(args):
    all_results = {}
    
    for run_id, seed in enumerate(args['seed_list']):
        args['seed'], args['run_id'] = seed, run_id
        logfile_head, logfile_name = build_log_dirs(args)
        args['log_path'] = logfile_name
        
        # Configure logging with unbuffered file handler for real-time updates
        log_file_path = os.path.join(logfile_name, 'record.log')
        
        # æ¸…é™¤ç°æœ‰çš„æ—¥å¿—å¤„ç†å™¨ï¼Œé¿å…å†²çª
        root_logger = logging.getLogger()
        for handler in root_logger.handlers[:]:
            root_logger.removeHandler(handler)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(filename=log_file_path, mode='a', encoding='utf-8')
        file_handler.stream.reconfigure(line_buffering=True)  # Enable line buffering
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        
        # è®¾ç½®æ ¼å¼
        formatter = logging.Formatter('%(asctime)s [%(filename)s] => %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
        root_logger.setLevel(logging.INFO)
        root_logger.addHandler(file_handler)
        root_logger.addHandler(console_handler)
        
        # æ‰“å°æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œæ–¹ä¾¿ç”¨æˆ·æŸ¥æ‰¾
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_file_path}")
        print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ 'tail -f {log_file_path}' å®æ—¶æŸ¥çœ‹æ—¥å¿—")
        print("-" * 80)
        
        args['log_path'] = logfile_name
        results = train_single_run(args)
        all_results[f"seed_{seed}"] = results
    
    # åœ¨æ‰€æœ‰ç§å­è¿è¡Œå®Œæˆåï¼Œè¿›è¡Œç»Ÿè®¡åˆ†æ
    if len(all_results) > 1:  # åªæœ‰å¤šäºä¸€ä¸ªç§å­æ—¶æ‰è¿›è¡Œç»Ÿè®¡åˆ†æ
        dataset_names = args.get('cross_domain_datasets', None)
        analyze_all_results(all_results, dataset_names, save_json=True)
    

def train_single_run(args, return_model: bool = False):
    # Setting random seed and device for reproducibility
    set_random(args['seed'])
    print_args(args)
    
    # Initialize data manager and model

    if args['cross_domain']:
        # ä½¿ç”¨å¹³è¡¡åçš„cross-domainæ•°æ®é›†
        data_manager = BalancedCrossDomainDataManagerCore(
            dataset_names=args['cross_domain_datasets'],
            balanced_datasets_root="balanced_datasets",
            shuffle=args['shuffle'],
            seed=args['seed'],
            num_shots=args.get('num_shots', 0),
            num_samples_per_task_for_evaluation=args.get('num_samples_per_task_for_evaluation', 0),
            use_balanced_datasets=True)
    else:
        data_manager = WithinDomainDataManager(
            dataset_name=args['dataset'],
            shuffle=args['shuffle'],
            seed=args['seed'],
            init_cls=args['init_cls'],
            increment=args['increment'],
            args=args)
    
    model = SubspaceLoRA(args)
    logging.info(f'All params: {count_parameters(model.network)}')
    logging.info(f'Trainable params: {count_parameters(model.network, True)}')
    final_results = model.loop(data_manager)
    
    # æ·»åŠ log_pathåˆ°ç»“æœä¸­ï¼Œä»¥ä¾¿aggregate_seed_resultså¯ä»¥æ‰¾åˆ°å®ƒ
    if 'log_path' in args:
        final_results['log_path'] = args['log_path']
    
    if return_model:
        return final_results, model
    return final_results

def set_device(device_type):
    """Properly set the device (either CPU or GPU) based on input"""
    if isinstance(device_type, (list, tuple)):
        return [torch.device(f'cuda:{d}' if d != -1 else 'cpu') for d in device_type]
    return torch.device('cuda' if device_type != -1 else 'cpu')

def set_random(seed):
    """Set random seeds to ensure reproducibility"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def print_args(args):
    """Log the arguments for this run"""
    for key, value in args.items():
        logging.info(f'{key}: {value}')


import os
from pathlib import Path
import json

def _filter_args_by_lora_type(args: dict) -> dict:
    """
    è¿‡æ»¤å‚æ•°å­—å…¸ï¼Œåªä¿ç•™ä¸å½“å‰LoRAç±»å‹ç›¸å…³çš„å‚æ•°
    è¿™æ ·å¯ä»¥é¿å…åœ¨params.jsonä¸­ä¿å­˜ä¸ç›¸å…³çš„å‚æ•°ï¼Œå¯¼è‡´æ—¥å¿—å‘½åæ··ä¹±
    """
    lora_type = args.get('lora_type', 'basic_lora')
    filtered_args = args.copy()
    
    # å®šä¹‰æ¯ç§LoRAç±»å‹ç›¸å…³çš„å‚æ•°
    sgp_lora_params = {'weight_temp', 'weight_kind', 'weight_p'}
    nsp_lora_params = {'nsp_eps', 'nsp_weight'}
    
    # ç§»é™¤ä¸å½“å‰LoRAç±»å‹ä¸ç›¸å…³çš„å‚æ•°
    if lora_type == 'sgp_lora':
        # ä¿ç•™SGPå‚æ•°ï¼Œç§»é™¤NSPå‚æ•°
        for param in nsp_lora_params:
            filtered_args.pop(param, None)
    elif lora_type == 'nsp_lora':
        # ä¿ç•™NSPå‚æ•°ï¼Œç§»é™¤SGPå‚æ•°
        for param in sgp_lora_params:
            filtered_args.pop(param, None)
    elif lora_type == 'basic_lora':
        # ç§»é™¤æ‰€æœ‰LoRAç‰¹å®šå‚æ•°
        for param in sgp_lora_params.union(nsp_lora_params):
            filtered_args.pop(param, None)
    elif lora_type == 'full':
        # ç§»é™¤æ‰€æœ‰LoRAç‰¹å®šå‚æ•°
        for param in sgp_lora_params.union(nsp_lora_params):
            filtered_args.pop(param, None)
    
    return filtered_args

def build_log_dirs(args: dict, root_dir="."):
    """
    æ ¹æ® args æ„å»ºå¤šçº§æ—¥å¿—ç›®å½•ï¼Œç¡®ä¿ä¸åŒ LoRA ç±»å‹çš„å‚æ•°æ­£ç¡®åˆ†ç¦»
    
    ç›®å½•ç»“æ„æ”¹è¿›ï¼š
    - é¡¶å±‚ç›®å½•ç°åœ¨åŒ…å«å®éªŒç±»å‹æ ‡è¯†ï¼ˆcross_domain æˆ– within_domainï¼‰
    - cross_domainå®éªŒï¼šsldc_logs_{user}_cross_domain/{datasets}_{vit_type}/...
    - within_domainå®éªŒï¼šsldc_logs_{user}_within_domain/{dataset}_{vit_type}/...
    
    è¿™æ ·å¯ä»¥æ˜ç¡®åŒºåˆ†cross-domainå’Œwithin-domainçš„å®éªŒç»“æœï¼Œé¿å…æ··æ·†
    """

    def sanitize_filename(s: str) -> str:
        """ç§»é™¤æˆ–æ›¿æ¢æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
        # Windows éæ³•å­—ç¬¦: \ / : * ? " < > |
        s = re.sub(r'[\\/:*?"<>|]', '_', str(s))
        # å¯é€‰ï¼šå‹ç¼©è¿ç»­ä¸‹åˆ’çº¿
        s = re.sub(r'_+', '_', s)
        return s.strip('_')

    def short(s: str, maxlen=40):
        """æˆªæ–­è¿‡é•¿å­—ç¬¦ä¸²ï¼Œä¸åŠ  hashï¼Œä»…ä¿ç•™å¯è¯»æ€§"""
        s = sanitize_filename(str(s))
        if len(s) <= maxlen:
            return s
        return s[:maxlen].rstrip('_')  # é¿å…æˆªæ–­åœ¨ä¸‹åˆ’çº¿å¤„

    def _get_lora_specific_params(lora_type: str, args: dict) -> list:
        """è·å–ç‰¹å®š LoRA ç±»å‹çš„å‚æ•°ï¼Œé¿å…äº¤å‰æ±¡æŸ“"""
        params = []
        
        if lora_type == 'sgp_lora':
            # SGP LoRA ç‰¹æœ‰å‚æ•°
            if 'weight_temp' in args:
                params.append(f"t-{short(args['weight_temp'])}")
            if 'weight_kind' in args:
                params.append(f"k-{short(args['weight_kind'])}")
            # å§‹ç»ˆåŒ…å« weight_p å‚æ•°ï¼Œå³ä½¿æ˜¯é»˜è®¤å€¼ï¼Œä»¥ç¡®ä¿ä¸åŒå‚æ•°ç»„åˆçš„å®éªŒç»“æœè¢«æ­£ç¡®åŒºåˆ†
            if 'weight_p' in args:
                params.append(f"p-{short(args['weight_p'])}")
                
        elif lora_type == 'nsp_lora':
            # NSP LoRA ç‰¹æœ‰å‚æ•°
            if 'nsp_eps' in args:
                params.append(f"eps-{short(args['nsp_eps'])}")
            if 'nsp_weight' in args:
                params.append(f"w-{short(args['nsp_weight'])}")
                
        elif lora_type == 'basic_lora':
            # Basic LoRA é€šå¸¸æ²¡æœ‰é¢å¤–å‚æ•°ï¼Œä½†å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
            pass
            
        elif lora_type == 'full':
            # Full fine-tuning å¯èƒ½æœ‰çš„å‚æ•°
            pass
            
        return params

    def _get_kd_params(args: dict) -> list:
        """è·å–çŸ¥è¯†è’¸é¦ç›¸å…³å‚æ•°ï¼Œç»Ÿä¸€å‘½åè§„åˆ™"""
        kd_params = []
        
        if args.get('gamma_kd', 0.0) > 0.0:
            kd_params.append(f"kd-{short(args['gamma_kd'])}")
            if 'kd_type' in args:
                kd_params.append(f"type-{short(args['kd_type'])}")
            if 'distillation_transform' in args:
                kd_params.append(f"dt-{short(args['distillation_transform'])}")
            if args.get('use_aux_for_kd', False):
                kd_params.append("aux")
            # æ·»åŠ update_teacher_each_taskå‚æ•°ï¼Œç®€å†™ä¸ºutt
            if 'update_teacher_each_task' in args:
                kd_params.append(f"utt-{short(args['update_teacher_each_task'])}")
                
        return kd_params

    def _validate_parameters(args: dict) -> None:
        """éªŒè¯å‚æ•°ç»„åˆçš„åˆç†æ€§"""
        lora_type = args.get('lora_type', 'basic_lora')
        
        # æ£€æŸ¥ LoRA ç‰¹å®šå‚æ•°æ˜¯å¦è¢«è¯¯ç”¨
        if lora_type != 'sgp_lora':
            sgp_params = ['weight_temp', 'weight_kind', 'weight_p']
            for param in sgp_params:
                if param in args and args[param] is not None:
                    logging.warning(f"âš ï¸ Parameter '{param}' is being used with lora_type='{lora_type}', but it's specific to sgp_lora")
        
        if lora_type != 'nsp_lora':
            nsp_params = ['nsp_eps', 'nsp_weight']
            for param in nsp_params:
                if param in args and args[param] is not None:
                    logging.warning(f"âš ï¸ Parameter '{param}' is being used with lora_type='{lora_type}', but it's specific to nsp_lora")

    # å‚æ•°éªŒè¯
    _validate_parameters(args)

    # ç¡®å®šå®éªŒç±»å‹ï¼šcross-domain æˆ– within-domain
    is_cross_domain = args.get('cross_domain', False)
    
    # é¡¶å±‚ï¼šæ¨¡å‹ã€ç”¨æˆ·ä¿¡æ¯å’Œå®éªŒç±»å‹ï¼ˆæ˜ç¡®åŒºåˆ†cross-domainå’Œwithin-domainï¼‰
    experiment_type = "cross_domain" if is_cross_domain else "within_domain"
    base_dir = os.path.join(
        root_dir,
        f"{short(args['model_name'])}_logs_{short(args['user'])}_{experiment_type}"
    )

    # æ ¹æ®å®éªŒç±»å‹æ„å»ºä¸åŒçš„äºŒçº§ç›®å½•ç»“æ„
    if is_cross_domain:
        # è·¨åŸŸå®éªŒï¼šä½¿ç”¨orderæ ‡è¯†è€Œä¸æ˜¯å…·ä½“çš„æ•°æ®é›†åˆ—è¡¨
        if 'cross_domain_datasets' in args:
            # ä½¿ç”¨order1ä½œä¸ºå½“å‰æ•°æ®é›†é¡ºåºçš„æ ‡è¯†ï¼Œå°†æ¥æœ‰å…¶ä»–é¡ºåºæ—¶å¯å‘½åä¸ºorder2, order3ç­‰
            task_dir = os.path.join(
                base_dir,
                f"order1_{short(args['vit_type'])}",
                f"shots-{short(args.get('num_shots', 0))}",
                f"lrank-{short(args.get('lora_rank', 'NA'))}_ltype-{short(args.get('lora_type', 'NA'))}"
            )
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šè·¨åŸŸæ•°æ®é›†ï¼Œä½¿ç”¨é»˜è®¤æ ‡è¯†
            task_dir = os.path.join(
                base_dir,
                f"unknown_{short(args['vit_type'])}",
                f"shots-{short(args.get('num_shots', 0))}",
                f"lrank-{short(args.get('lora_rank', 'NA'))}_ltype-{short(args.get('lora_type', 'NA'))}"
            )
    else:
        # åŸŸå†…å®éªŒï¼šä½¿ç”¨ä¼ ç»Ÿçš„init_clså’Œincrementå‚æ•°
        task_dir = os.path.join(
            base_dir,
            f"{short(args['dataset'])}_{short(args['vit_type'])}",
            f"init-{short(args['init_cls'])}_inc-{short(args['increment'])}",
            f"lrank-{short(args.get('lora_rank', 'NA'))}_ltype-{short(args.get('lora_type', 'NA'))}"
        )

    # ä¸‰çº§ï¼šLoRA ç‰¹å®šå‚æ•°ï¼ˆåªåŒ…å«ç›¸å…³çš„ï¼‰
    lora_params = _get_lora_specific_params(args.get('lora_type', 'basic_lora'), args)
    
    # å››çº§ï¼šçŸ¥è¯†è’¸é¦å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
    kd_params = _get_kd_params(args)
    
    # åˆå¹¶ LoRA å’Œ KD å‚æ•°
    method_params = lora_params + kd_params
    
    # æ„å»ºæ–¹æ³•å‚æ•°ç›®å½•
    if method_params:
        method_subdir = "_".join(method_params)
        method_dir = os.path.join(task_dir, short(method_subdir, maxlen=80))
    else:
        method_dir = task_dir

    # äº”çº§ï¼šä¼˜åŒ–å™¨å’Œè®­ç»ƒå‚æ•°ï¼ˆä¸åŒ…å«ç§å­ï¼Œç§å­å°†åœ¨å­ç›®å½•ä¸­å¤„ç†ï¼‰
    opt_params = [
        f"opt-{args['optimizer']}",
        f"lr-{short(args['lrate'])}",
        f"b-{args['batch_size']}",
        f"i-{args['iterations']}"
    ]
    opt_str = "_".join(opt_params)
    opt_dir = os.path.join(method_dir, short(opt_str, maxlen=80))

    # === é€çº§åˆ›å»ºç›®å½• ===
    abs_log_dir = os.path.abspath(opt_dir)
    current = Path(abs_log_dir).root
    for part in Path(abs_log_dir).parts[1:]:
        current = Path(current) / part
        current.mkdir(exist_ok=True)

    # ä¿å­˜è¿‡æ»¤åçš„å‚æ•°åˆ° JSONï¼Œé¿å…å‚æ•°äº¤å‰æ±¡æŸ“
    filtered_args = _filter_args_by_lora_type(args)
    params_json = Path(abs_log_dir) / "params.json"
    if not params_json.exists():
        with open(params_json, "w", encoding="utf-8") as f:
            json.dump(filtered_args, f, ensure_ascii=False, indent=2)

    # ä¸ºæ¯ä¸ªç§å­åˆ›å»ºå­ç›®å½•
    seed_dir = os.path.join(abs_log_dir, f"seed_{args['seed']}")
    os.makedirs(seed_dir, exist_ok=True)

    # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ä½¿ç”¨ logging.infoï¼Œå› ä¸ºæ—¥å¿—è¿˜æ²¡æœ‰é…ç½®
    # ç›®å½•ä¿¡æ¯ä¼šåœ¨æ—¥å¿—é…ç½®å®Œæˆåé€šè¿‡ print_args å‡½æ•°è®°å½•

    return os.path.dirname(abs_log_dir), str(seed_dir)

def analyze_all_results(all_results: dict, dataset_names: list = [], save_json: bool = True, output_path: str = "") -> dict:
    """
    åˆ†æall_resultsä¸­å¤šä¸ªéšæœºç§å­çš„ç»“æœï¼Œè®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®å¹¶è®°å½•åˆ°æ—¥å¿—
    
    Args:
        all_results: åŒ…å«å¤šä¸ªéšæœºç§å­ç»“æœçš„å­—å…¸
        dataset_names: æ•°æ®é›†åç§°åˆ—è¡¨ï¼Œç”¨äºæ—¥å¿—è¾“å‡º
        save_json: æ˜¯å¦å°†ç»Ÿè®¡ç»“æœä¿å­˜ä¸ºJSONæ–‡ä»¶
        output_path: JSONæ–‡ä»¶ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
    
    Returns:
        dict: åŒ…å«ç»Ÿè®¡ç»“æœçš„å­—å…¸
    """
    import numpy as np
    import json
    from pathlib import Path
    
    if not all_results:
        logging.warning("ğŸ“Š all_resultsä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œç»Ÿè®¡åˆ†æ")
        return {}
    
    # è·å–æ‰€æœ‰ç§å­å’Œå˜ä½“åç§°
    seed_keys = list(all_results.keys())
    if len(seed_keys) == 0:
        logging.warning("ğŸ“Š æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç§å­ç»“æœ")
        return {}
    
    # ä»ç¬¬ä¸€ä¸ªç§å­ç»“æœä¸­è·å–å˜ä½“åç§°å’Œä»»åŠ¡ä¿¡æ¯
    first_seed_result = all_results[seed_keys[0]]
    variant_names = set()
    
    # ä»last_task_accuraciesè·å–å˜ä½“åç§°
    if 'last_task_accuracies' in first_seed_result:
        variant_names.update(first_seed_result['last_task_accuracies'].keys())
    
    # ä»average_accuraciesè·å–å˜ä½“åç§°
    if 'average_accuracies' in first_seed_result:
        variant_names.update(first_seed_result['average_accuracies'].keys())
    
    # ä»per_task_resultsè·å–å˜ä½“åç§°
    if 'per_task_results' in first_seed_result:
        for task_result in first_seed_result['per_task_results'].values():
            variant_names.update(task_result.keys())
    
    variant_names = sorted(list(variant_names))
    
    if not variant_names:
        logging.warning("ğŸ“Š æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å˜ä½“åç§°")
        return {}
    
    # è·å–ä»»åŠ¡IDåˆ—è¡¨
    task_ids = []
    if 'per_task_results' in first_seed_result:
        task_ids = sorted(first_seed_result['per_task_results'].keys())
    
    num_seeds = len(seed_keys)
    logging.info(f"ğŸ“Š å¼€å§‹åˆ†æ {num_seeds} ä¸ªéšæœºç§å­çš„å®éªŒç»“æœ")
    logging.info(f"ğŸ“Š å‘ç° {len(variant_names)} ä¸ªå˜ä½“: {', '.join(variant_names)}")
    if task_ids:
        logging.info(f"ğŸ“Š å‘ç° {len(task_ids)} ä¸ªä»»åŠ¡: {', '.join(map(str, task_ids))}")
    
    # åˆå§‹åŒ–ç»Ÿè®¡ç»“æœå­—å…¸
    statistics_results = {
        "summary": {
            "num_seeds": num_seeds,
            "num_variants": len(variant_names),
            "num_tasks": len(task_ids),
            "variant_names": variant_names,
            "task_ids": task_ids,
            "dataset_names": dataset_names
        },
        "variants": {}
    }
    
    # è®°å½•ç»Ÿè®¡ç»“æœ
    logging.info("=" * 80)
    logging.info("ğŸ“ˆ å¤šç§å­ç»Ÿè®¡åˆ†æç»“æœ")
    logging.info("=" * 80)
    
    for variant in variant_names:
        logging.info(f"\nğŸ” å˜ä½“: {variant}")
        logging.info("-" * 60)
        
        # åˆå§‹åŒ–å˜ä½“ç»Ÿè®¡ç»“æœ
        variant_stats = {
            "last_task_accuracy": {},
            "average_accuracy": {},
            "per_task_accuracies": {},
            "class_wise_accuracy": {}
        }
        
        # æ”¶é›†æœ€åä»»åŠ¡å‡†ç¡®ç‡æ•°æ®
        last_task_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if 'last_task_accuracies' in seed_result and variant in seed_result['last_task_accuracies']:
                last_task_accs.append(seed_result['last_task_accuracies'][variant])
        
        if last_task_accs:
            mean_last = np.mean(last_task_accs)
            std_last = np.std(last_task_accs)
            variant_stats["last_task_accuracy"] = {
                "mean": float(round(mean_last, 2)),
                "std": float(round(std_last, 2)),
                "raw_values": [float(round(acc, 2)) for acc in last_task_accs]
            }
            logging.info(f"  æœ€åä»»åŠ¡å‡†ç¡®ç‡: {mean_last:.2f}% Â± {std_last:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in last_task_accs])}")
        else:
            variant_stats["last_task_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  æœ€åä»»åŠ¡å‡†ç¡®ç‡: æ— æ•°æ®")
        
        # æ”¶é›†å¹³å‡å‡†ç¡®ç‡æ•°æ®
        avg_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if 'average_accuracies' in seed_result and variant in seed_result['average_accuracies']:
                avg_accs.append(seed_result['average_accuracies'][variant])
        
        if avg_accs:
            mean_avg = np.mean(avg_accs)
            std_avg = np.std(avg_accs)
            variant_stats["average_accuracy"] = {
                "mean": float(round(mean_avg, 2)),
                "std": float(round(std_avg, 2)),
                "raw_values": [float(round(acc, 2)) for acc in avg_accs]
            }
            logging.info(f"  å¹³å‡å‡†ç¡®ç‡: {mean_avg:.2f}% Â± {std_avg:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in avg_accs])}")
        else:
            variant_stats["average_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  å¹³å‡å‡†ç¡®ç‡: æ— æ•°æ®")
        
        # æ”¶é›†class-wiseå¹³å‡å‡†ç¡®ç‡æ•°æ®ï¼ˆä»…cross-domainåœºæ™¯ï¼‰
        class_wise_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if ('class_wise_accuracies' in seed_result and
                variant in seed_result['class_wise_accuracies']):
                class_wise_accs.append(seed_result['class_wise_accuracies'][variant])
        
        if class_wise_accs:
            mean_class_wise = np.mean(class_wise_accs)
            std_class_wise = np.std(class_wise_accs)
            variant_stats["class_wise_accuracy"] = {
                "mean": float(round(mean_class_wise, 2)),
                "std": float(round(std_class_wise, 2)),
                "raw_values": [float(round(acc, 2)) for acc in class_wise_accs]
            }
            logging.info(f"  Class-wiseå¹³å‡å‡†ç¡®ç‡: {mean_class_wise:.2f}% Â± {std_class_wise:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in class_wise_accs])}")
        else:
            variant_stats["class_wise_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  Class-wiseå¹³å‡å‡†ç¡®ç‡: æ— æ•°æ®")
        
        # æ”¶é›†å¹³å‡class-wiseå‡†ç¡®ç‡æ•°æ®ï¼ˆæ–°å¢åŠŸèƒ½ï¼šæ‰€æœ‰ä»»åŠ¡çš„class-wiseå¹³å‡å‡†ç¡®åº¦çš„å¹³å‡å€¼ï¼‰
        average_class_wise_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if ('average_class_wise_accuracies' in seed_result and
                variant in seed_result['average_class_wise_accuracies']):
                average_class_wise_accs.append(seed_result['average_class_wise_accuracies'][variant])
        
        if average_class_wise_accs:
            mean_avg_class_wise = np.mean(average_class_wise_accs)
            std_avg_class_wise = np.std(average_class_wise_accs)
            variant_stats["average_class_wise_accuracy"] = {
                "mean": float(round(mean_avg_class_wise, 2)),
                "std": float(round(std_avg_class_wise, 2)),
                "raw_values": [float(round(acc, 2)) for acc in average_class_wise_accs]
            }
            logging.info(f"  å¹³å‡Class-wiseå‡†ç¡®ç‡(æ‰€æœ‰ä»»åŠ¡): {mean_avg_class_wise:.2f}% Â± {std_avg_class_wise:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in average_class_wise_accs])}")
        else:
            variant_stats["average_class_wise_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  å¹³å‡Class-wiseå‡†ç¡®ç‡(æ‰€æœ‰ä»»åŠ¡): æ— æ•°æ®")
        
        # è®°å½•æ¯ä¸ªä»»åŠ¡çš„class-wiseå‡†ç¡®åº¦ï¼ˆç‹¬ç«‹æ˜¾ç¤ºï¼Œä¸ä¾èµ–äºaverage_class_wise_accsï¼‰
        if task_ids:
            logging.info(f"  å„ä»»åŠ¡Class-wiseå‡†ç¡®ç‡:")
            # ä»ç¬¬ä¸€ä¸ªç§å­è·å–æ¯ä¸ªä»»åŠ¡çš„class-wiseå‡†ç¡®åº¦
            first_seed_result = all_results[seed_keys[0]]
            if 'per_task_class_wise_accuracies' in first_seed_result and variant in first_seed_result['per_task_class_wise_accuracies']:
                per_task_accs = first_seed_result['per_task_class_wise_accuracies'][variant]
                for i, task_id in enumerate(task_ids):
                    if i < len(per_task_accs):
                        dataset_name = dataset_names[task_id - 1] if dataset_names and task_id - 1 < len(dataset_names) else f"Task {task_id}"
                        logging.info(f"    {dataset_name}: {per_task_accs[i]:.2f}%")
            else:
                logging.info(f"    æ— per_task_class_wise_accuraciesæ•°æ®")
        
        # æ”¶é›†æ¯ä¸ªä»»åŠ¡çš„å‡†ç¡®ç‡æ•°æ®
        if task_ids:
            logging.info(f"  å„ä»»åŠ¡å‡†ç¡®ç‡:")
            for task_id in task_ids:
                task_accs = []
                for seed_key in seed_keys:
                    seed_result = all_results[seed_key]
                    if ('per_task_results' in seed_result and
                        task_id in seed_result['per_task_results'] and
                        variant in seed_result['per_task_results'][task_id]):
                        # ç¡®ä¿åªè·å–æ•°å€¼ç±»å‹çš„æ•°æ®
                        task_value = seed_result['per_task_results'][task_id][variant]
                        if isinstance(task_value, (int, float)):
                            task_accs.append(task_value)
                
                if task_accs:
                    mean_task = np.mean(task_accs)
                    std_task = np.std(task_accs)
                    dataset_name = dataset_names[task_id - 1] if dataset_names and task_id - 1 < len(dataset_names) else f"Task {task_id}"
                    variant_stats["per_task_accuracies"][str(task_id)] = {
                        "dataset_name": dataset_name,
                        "mean": float(round(mean_task, 2)),
                        "std": float(round(std_task, 2)),
                        "raw_values": [float(round(acc, 2)) for acc in task_accs]
                    }
                    logging.info(f"    {dataset_name}: {mean_task:.2f}% Â± {std_task:.2f}%")
                    logging.info(f"      è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in task_accs])}")
                else:
                    dataset_name = dataset_names[task_id - 1] if dataset_names and task_id - 1 < len(dataset_names) else f"Task {task_id}"
                    variant_stats["per_task_accuracies"][str(task_id)] = {
                        "dataset_name": dataset_name,
                        "error": "æ— æ•°æ®"
                    }
                    logging.info(f"    {dataset_name}: æ— æ•°æ®")
    
        statistics_results["variants"][variant] = variant_stats
    
    # è®°å½•æ•´ä½“ç»Ÿè®¡æ‘˜è¦
    logging.info("\n" + "=" * 80)
    logging.info("ğŸ“‹ æ•´ä½“æ€§èƒ½æ‘˜è¦")
    logging.info("=" * 80)
    
    # æ·»åŠ æ•´ä½“æ‘˜è¦åˆ°ç»Ÿè®¡ç»“æœ
    statistics_results["overall_summary"] = {}
    
    for variant in variant_names:
        # æ”¶é›†å¹³å‡å‡†ç¡®ç‡ç”¨äºæ•´ä½“æ¯”è¾ƒ
        avg_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if 'average_accuracies' in seed_result and variant in seed_result['average_accuracies']:
                avg_accs.append(seed_result['average_accuracies'][variant])
        
        if avg_accs:
            mean_avg = np.mean(avg_accs)
            std_avg = np.std(avg_accs)
            statistics_results["overall_summary"][variant] = {
                "mean": float(round(mean_avg, 2)),
                "std": float(round(std_avg, 2)),
                "num_seeds": len(avg_accs)
            }
            logging.info(f"  {variant:<20}: {mean_avg:.2f}% Â± {std_avg:.2f}% (åŸºäº {len(avg_accs)} ä¸ªç§å­)")
    
    logging.info("=" * 80)
    logging.info(f"ğŸ“Š ç»Ÿè®¡åˆ†æå®Œæˆï¼Œå…±åˆ†æ {num_seeds} ä¸ªç§å­ï¼Œ{len(variant_names)} ä¸ªå˜ä½“")
    logging.info("=" * 80)
    
    # ä¿å­˜JSONæ–‡ä»¶
    if save_json:
        if not output_path:
            # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºè·¯å¾„
            if 'log_path' in first_seed_result:
                log_dir = Path(first_seed_result['log_path']).parent
            else:
                log_dir = Path("./statistics_results")
            log_dir.mkdir(exist_ok=True)
            output_path = str(log_dir / "multi_seed_statistics.json")
        
        output_path_obj = Path(output_path)
        output_path_obj.parent.mkdir(parents=True, exist_ok=True)
        
        with open(str(output_path_obj), 'w', encoding='utf-8') as f:
            json.dump(statistics_results, f, ensure_ascii=False, indent=2)
        
        logging.info(f"ğŸ“ ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {output_path_obj}")
    
    return statistics_results