import os
import sys
import logging
import torch
import random
import numpy as np
from collections.abc import Mapping, Sequence
# from models.subspace_lora import SubspaceLoRA
from models.subspace_lora import SubspaceLoRA
from utils.data_manager import WithinDomainDataManager, CrossDomainDataManagerCore
from utils.balanced_cross_domain_data_manager import BalancedCrossDomainDataManagerCore
from utils.toolkit import count_parameters
import re

def train(args):
    all_results = {}
    
    for run_id, seed in enumerate(args['seed_list']):
        args['seed'], args['run_id'] = seed, run_id
        logfile_head, logfile_name = build_log_dirs(args)
        args['log_path'] = logfile_name
        
        # Configure logging with unbuffered file handler for real-time updates
        log_file_path = os.path.join(logfile_name, 'record.log')
        
        # æ¸…é™¤ç°æœ‰çš„æ—¥å¿—å¤„ç†å™¨ï¼Œé¿å…å†²çª
        root_logger = logging.getLogger()
        for handler in root_logger.handlers[:]:
            root_logger.removeHandler(handler)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(filename=log_file_path, mode='a', encoding='utf-8')
        file_handler.stream.reconfigure(line_buffering=True)  # Enable line buffering
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        
        # è®¾ç½®æ ¼å¼
        formatter = logging.Formatter('%(asctime)s [%(filename)s] => %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
        root_logger.setLevel(logging.INFO)
        root_logger.addHandler(file_handler)
        root_logger.addHandler(console_handler)
        
        # æ‰“å°æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œæ–¹ä¾¿ç”¨æˆ·æŸ¥æ‰¾
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_file_path}")
        print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ 'tail -f {log_file_path}' å®æ—¶æŸ¥çœ‹æ—¥å¿—")
        print("-" * 80)
        
        args['log_path'] = logfile_name
        results = train_single_run(args)
        all_results[f"seed_{seed}"] = results
    
    # åœ¨æ‰€æœ‰ç§å­è¿è¡Œå®Œæˆåï¼Œè¿›è¡Œç»Ÿè®¡åˆ†æ
    if len(all_results) > 1:  # åªæœ‰å¤šäºä¸€ä¸ªç§å­æ—¶æ‰è¿›è¡Œç»Ÿè®¡åˆ†æ
        dataset_names = args.get('cross_domain_datasets', None)
        analyze_all_results(all_results, dataset_names, save_json=True)
    

def train_single_run(args, return_model: bool = False):
    # Setting random seed and device for reproducibility
    set_random(args['seed'])
    print_args(args)
    
    # Initialize data manager and model

    if args['cross_domain']:
        # ä½¿ç”¨å¹³è¡¡åçš„cross-domainæ•°æ®é›†
        data_manager = BalancedCrossDomainDataManagerCore(
            dataset_names=args['cross_domain_datasets'],
            balanced_datasets_root="balanced_datasets",
            shuffle=args['shuffle'],
            seed=args['seed'],
            num_shots=args.get('num_shots', 0),
            use_balanced_datasets=True)
    else:
        data_manager = WithinDomainDataManager(
            dataset_name=args['dataset'],
            shuffle=args['shuffle'],
            seed=args['seed'],
            init_cls=args['init_cls'],
            increment=args['increment'],
            args=args)
    
    model = SubspaceLoRA(args)
    logging.info(f'All params: {count_parameters(model.network)}')
    logging.info(f'Trainable params: {count_parameters(model.network, True)}')
    final_results = model.loop(data_manager)
    
    # æ·»åŠ log_pathåˆ°ç»“æœä¸­ï¼Œä»¥ä¾¿aggregate_seed_resultså¯ä»¥æ‰¾åˆ°å®ƒ
    if 'log_path' in args:
        final_results['log_path'] = args['log_path']
    
    if return_model:
        return final_results, model
    return final_results

def set_device(device_type):
    """Properly set the device (either CPU or GPU) based on input"""
    if isinstance(device_type, (list, tuple)):
        return [torch.device(f'cuda:{d}' if d != -1 else 'cpu') for d in device_type]
    return torch.device('cuda' if device_type != -1 else 'cpu')

def set_random(seed):
    """Set random seeds to ensure reproducibility"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def print_args(args):
    """Log the arguments for this run"""
    for key, value in args.items():
        logging.info(f'{key}: {value}')


import os
from pathlib import Path
import json

def _filter_args_by_lora_type(args: dict) -> dict:
    """
    è¿‡æ»¤å‚æ•°å­—å…¸ï¼Œåªä¿ç•™ä¸å½“å‰LoRAç±»å‹ç›¸å…³çš„å‚æ•°
    è¿™æ ·å¯ä»¥é¿å…åœ¨params.jsonä¸­ä¿å­˜ä¸ç›¸å…³çš„å‚æ•°ï¼Œå¯¼è‡´æ—¥å¿—å‘½åæ··ä¹±
    """
    lora_type = args.get('lora_type', 'basic_lora')
    filtered_args = args.copy()
    
    # å®šä¹‰æ¯ç§LoRAç±»å‹ç›¸å…³çš„å‚æ•°
    sgp_lora_params = {'weight_temp', 'weight_kind', 'weight_p'}
    nsp_lora_params = {'nsp_eps', 'nsp_weight'}
    # Hopfieldå‚æ•°é€‚ç”¨äºæ‰€æœ‰LoRAç±»å‹
    hopfield_params = {'hopfield_temp', 'hopfield_topk'}
    
    # ç§»é™¤ä¸å½“å‰LoRAç±»å‹ä¸ç›¸å…³çš„å‚æ•°
    if lora_type == 'sgp_lora':
        # ä¿ç•™SGPå‚æ•°å’ŒHopfieldå‚æ•°ï¼Œç§»é™¤NSPå‚æ•°
        for param in nsp_lora_params:
            filtered_args.pop(param, None)
    elif lora_type == 'nsp_lora':
        # ä¿ç•™NSPå‚æ•°å’ŒHopfieldå‚æ•°ï¼Œç§»é™¤SGPå‚æ•°
        for param in sgp_lora_params:
            filtered_args.pop(param, None)
    elif lora_type == 'basic_lora':
        # åªä¿ç•™Hopfieldå‚æ•°ï¼Œç§»é™¤å…¶ä»–LoRAç‰¹å®šå‚æ•°
        for param in sgp_lora_params.union(nsp_lora_params):
            filtered_args.pop(param, None)
    elif lora_type == 'full':
        # åªä¿ç•™Hopfieldå‚æ•°ï¼Œç§»é™¤å…¶ä»–LoRAç‰¹å®šå‚æ•°
        for param in sgp_lora_params.union(nsp_lora_params):
            filtered_args.pop(param, None)
    
    return filtered_args

def build_log_dirs(args: dict, root_dir="."):
    """
    æ ¹æ® args æ„å»ºå¤šçº§æ—¥å¿—ç›®å½•ï¼Œç¡®ä¿ä¸åŒ LoRA ç±»å‹çš„å‚æ•°æ­£ç¡®åˆ†ç¦»
    
    ç›®å½•ç»“æ„æ”¹è¿›ï¼š
    - é¡¶å±‚ç›®å½•ç°åœ¨åŒ…å«å®éªŒç±»å‹æ ‡è¯†ï¼ˆcross_domain æˆ– within_domainï¼‰
    - cross_domainå®éªŒï¼šsldc_logs_{user}_cross_domain/{datasets}_{vit_type}/...
    - within_domainå®éªŒï¼šsldc_logs_{user}_within_domain/{dataset}_{vit_type}/...
    
    è¿™æ ·å¯ä»¥æ˜ç¡®åŒºåˆ†cross-domainå’Œwithin-domainçš„å®éªŒç»“æœï¼Œé¿å…æ··æ·†
    """

    def sanitize_filename(s: str) -> str:
        """ç§»é™¤æˆ–æ›¿æ¢æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
        # Windows éæ³•å­—ç¬¦: \ / : * ? " < > |
        s = re.sub(r'[\\/:*?"<>|]', '_', str(s))
        # å¯é€‰ï¼šå‹ç¼©è¿ç»­ä¸‹åˆ’çº¿
        s = re.sub(r'_+', '_', s)
        return s.strip('_')

    def short(s: str, maxlen=40):
        """æˆªæ–­è¿‡é•¿å­—ç¬¦ä¸²ï¼Œä¸åŠ  hashï¼Œä»…ä¿ç•™å¯è¯»æ€§"""
        s = sanitize_filename(str(s))
        if len(s) <= maxlen:
            return s
        return s[:maxlen].rstrip('_')  # é¿å…æˆªæ–­åœ¨ä¸‹åˆ’çº¿å¤„

    def _get_lora_specific_params(lora_type: str, args: dict) -> list:
        """è·å–ç‰¹å®š LoRA ç±»å‹çš„å‚æ•°ï¼Œé¿å…äº¤å‰æ±¡æŸ“"""
        params = []
        
        if lora_type == 'sgp_lora':
            # SGP LoRA ç‰¹æœ‰å‚æ•°
            if 'weight_temp' in args:
                params.append(f"t-{short(args['weight_temp'])}")
            if 'weight_kind' in args:
                params.append(f"k-{short(args['weight_kind'])}")
            # å§‹ç»ˆåŒ…å« weight_p å‚æ•°ï¼Œå³ä½¿æ˜¯é»˜è®¤å€¼ï¼Œä»¥ç¡®ä¿ä¸åŒå‚æ•°ç»„åˆçš„å®éªŒç»“æœè¢«æ­£ç¡®åŒºåˆ†
            if 'weight_p' in args:
                params.append(f"p-{short(args['weight_p'])}")
                
        elif lora_type == 'nsp_lora':
            # NSP LoRA ç‰¹æœ‰å‚æ•°
            if 'nsp_eps' in args:
                params.append(f"eps-{short(args['nsp_eps'])}")
            if 'nsp_weight' in args:
                params.append(f"w-{short(args['nsp_weight'])}")
                
        elif lora_type == 'basic_lora':
            # Basic LoRA é€šå¸¸æ²¡æœ‰é¢å¤–å‚æ•°ï¼Œä½†å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
            pass
            
        elif lora_type == 'full':
            # Full fine-tuning å¯èƒ½æœ‰çš„å‚æ•°
            pass
            
        # æ·»åŠ Hopfieldå‚æ•°ï¼Œé€‚ç”¨äºæ‰€æœ‰LoRAç±»å‹
        if 'hopfield_temp' in args:
            params.append(f"htemp-{short(args['hopfield_temp'])}")
        if 'hopfield_topk' in args:
            params.append(f"hk-{short(args['hopfield_topk'])}")
            
        return params

    def _get_kd_params(args: dict) -> list:
        """è·å–çŸ¥è¯†è’¸é¦ç›¸å…³å‚æ•°ï¼Œç»Ÿä¸€å‘½åè§„åˆ™"""
        kd_params = []
        
        if args.get('gamma_kd', 0.0) > 0.0:
            kd_params.append(f"kd-{short(args['gamma_kd'])}")
            if 'kd_type' in args:
                kd_params.append(f"type-{short(args['kd_type'])}")
            if 'distillation_transform' in args:
                kd_params.append(f"dt-{short(args['distillation_transform'])}")
            if args.get('use_aux_for_kd', False):
                kd_params.append("aux")
            # æ·»åŠ update_teacher_each_taskå‚æ•°ï¼Œç®€å†™ä¸ºutt
            if 'update_teacher_each_task' in args:
                kd_params.append(f"utt-{short(args['update_teacher_each_task'])}")
                
        return kd_params

    def _get_auxiliary_params(args: dict) -> list:
        """è·å–è¾…åŠ©æ•°æ®ç›¸å…³å‚æ•°ï¼ŒåŒ…æ‹¬feature_combination_type"""
        aux_params = []
        
        # æ·»åŠ feature_combination_typeå‚æ•°åˆ°æ—¥å¿—å‘½å
        if 'feature_combination_type' in args:
            aux_params.append(f"fc-{short(args['feature_combination_type'])}")
            
        return aux_params

    def _validate_parameters(args: dict) -> None:
        """éªŒè¯å‚æ•°ç»„åˆçš„åˆç†æ€§"""
        lora_type = args.get('lora_type', 'basic_lora')
        
        # æ£€æŸ¥ LoRA ç‰¹å®šå‚æ•°æ˜¯å¦è¢«è¯¯ç”¨
        if lora_type != 'sgp_lora':
            sgp_params = ['weight_temp', 'weight_kind', 'weight_p']
            for param in sgp_params:
                if param in args and args[param] is not None:
                    logging.warning(f"âš ï¸ Parameter '{param}' is being used with lora_type='{lora_type}', but it's specific to sgp_lora")
        
        if lora_type != 'nsp_lora':
            nsp_params = ['nsp_eps', 'nsp_weight']
            for param in nsp_params:
                if param in args and args[param] is not None:
                    logging.warning(f"âš ï¸ Parameter '{param}' is being used with lora_type='{lora_type}', but it's specific to nsp_lora")

    # å‚æ•°éªŒè¯
    _validate_parameters(args)

    # ç¡®å®šå®éªŒç±»å‹ï¼šcross-domain æˆ– within-domain
    is_cross_domain = args.get('cross_domain', False)
    
    # é¡¶å±‚ï¼šæ¨¡å‹ã€ç”¨æˆ·ä¿¡æ¯å’Œå®éªŒç±»å‹ï¼ˆæ˜ç¡®åŒºåˆ†cross-domainå’Œwithin-domainï¼‰
    experiment_type = "cross_domain" if is_cross_domain else "within_domain"
    base_dir = os.path.join(
        root_dir,
        f"{short(args['model_name'])}_logs_{short(args['user'])}_{experiment_type}"
    )

    # æ ¹æ®å®éªŒç±»å‹æ„å»ºä¸åŒçš„äºŒçº§ç›®å½•ç»“æ„
    if is_cross_domain:
        # è·¨åŸŸå®éªŒï¼šä½¿ç”¨orderæ ‡è¯†è€Œä¸æ˜¯å…·ä½“çš„æ•°æ®é›†åˆ—è¡¨
        if 'cross_domain_datasets' in args:
            # ä½¿ç”¨order1ä½œä¸ºå½“å‰æ•°æ®é›†é¡ºåºçš„æ ‡è¯†ï¼Œå°†æ¥æœ‰å…¶ä»–é¡ºåºæ—¶å¯å‘½åä¸ºorder2, order3ç­‰
            task_dir = os.path.join(
                base_dir,
                f"order1_{short(args['vit_type'])}",
                f"shots-{short(args.get('num_shots', 0))}",
                f"lrank-{short(args.get('lora_rank', 'NA'))}_ltype-{short(args.get('lora_type', 'NA'))}"
            )
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šè·¨åŸŸæ•°æ®é›†ï¼Œä½¿ç”¨é»˜è®¤æ ‡è¯†
            task_dir = os.path.join(
                base_dir,
                f"unknown_{short(args['vit_type'])}",
                f"shots-{short(args.get('num_shots', 0))}",
                f"lrank-{short(args.get('lora_rank', 'NA'))}_ltype-{short(args.get('lora_type', 'NA'))}"
            )
    else:
        # åŸŸå†…å®éªŒï¼šä½¿ç”¨ä¼ ç»Ÿçš„init_clså’Œincrementå‚æ•°
        task_dir = os.path.join(
            base_dir,
            f"{short(args['dataset'])}_{short(args['vit_type'])}",
            f"init-{short(args['init_cls'])}_inc-{short(args['increment'])}",
            f"lrank-{short(args.get('lora_rank', 'NA'))}_ltype-{short(args.get('lora_type', 'NA'))}"
        )

    # ä¸‰çº§ï¼šLoRA ç‰¹å®šå‚æ•°ï¼ˆåªåŒ…å«ç›¸å…³çš„ï¼‰
    lora_params = _get_lora_specific_params(args.get('lora_type', 'basic_lora'), args)
    
    # å››çº§ï¼šçŸ¥è¯†è’¸é¦å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
    kd_params = _get_kd_params(args)
    
    # äº”çº§ï¼šè¾…åŠ©æ•°æ®å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
    aux_params = _get_auxiliary_params(args)
    
    # åˆå¹¶ LoRAã€KD å’Œè¾…åŠ©æ•°æ®å‚æ•°
    method_params = lora_params + kd_params + aux_params
    
    # æ„å»ºæ–¹æ³•å‚æ•°ç›®å½•
    if method_params:
        method_subdir = "_".join(method_params)
        method_dir = os.path.join(task_dir, short(method_subdir, maxlen=80))
    else:
        method_dir = task_dir

    # å…­çº§ï¼šä¼˜åŒ–å™¨å’Œè®­ç»ƒå‚æ•°ï¼ˆä¸åŒ…å«ç§å­ï¼Œç§å­å°†åœ¨å­ç›®å½•ä¸­å¤„ç†ï¼‰
    opt_params = [
        f"opt-{args['optimizer']}",
        f"lr-{short(args['lrate'])}",
        f"b-{args['batch_size']}",
        f"i-{args['iterations']}"
    ]
    opt_str = "_".join(opt_params)
    opt_dir = os.path.join(method_dir, short(opt_str, maxlen=80))

    # === é€çº§åˆ›å»ºç›®å½• ===
    abs_log_dir = os.path.abspath(opt_dir)
    current = Path(abs_log_dir).root
    for part in Path(abs_log_dir).parts[1:]:
        current = Path(current) / part
        current.mkdir(exist_ok=True)

    # ä¿å­˜è¿‡æ»¤åçš„å‚æ•°åˆ° JSONï¼Œé¿å…å‚æ•°äº¤å‰æ±¡æŸ“
    filtered_args = _filter_args_by_lora_type(args)
    params_json = Path(abs_log_dir) / "params.json"
    if not params_json.exists():
        with open(params_json, "w", encoding="utf-8") as f:
            json.dump(filtered_args, f, ensure_ascii=False, indent=2)

    # ä¸ºæ¯ä¸ªç§å­åˆ›å»ºå­ç›®å½•
    seed_dir = os.path.join(abs_log_dir, f"seed_{args['seed']}")
    os.makedirs(seed_dir, exist_ok=True)

    # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ä½¿ç”¨ logging.infoï¼Œå› ä¸ºæ—¥å¿—è¿˜æ²¡æœ‰é…ç½®
    # ç›®å½•ä¿¡æ¯ä¼šåœ¨æ—¥å¿—é…ç½®å®Œæˆåé€šè¿‡ print_args å‡½æ•°è®°å½•

    return os.path.dirname(abs_log_dir), str(seed_dir)

def analyze_all_results(all_results: dict, dataset_names: list = [], save_json: bool = True, output_path: str = "") -> dict:
    """
    åˆ†æall_resultsä¸­å¤šä¸ªéšæœºç§å­çš„ç»“æœï¼Œè®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®å¹¶è®°å½•åˆ°æ—¥å¿—
    é€‚é…æ–°çš„final_resultsæ ¼å¼
    
    Args:
        all_results: åŒ…å«å¤šä¸ªéšæœºç§å­ç»“æœçš„å­—å…¸
        dataset_names: æ•°æ®é›†åç§°åˆ—è¡¨ï¼Œç”¨äºæ—¥å¿—è¾“å‡º
        save_json: æ˜¯å¦å°†ç»Ÿè®¡ç»“æœä¿å­˜ä¸ºJSONæ–‡ä»¶
        output_path: JSONæ–‡ä»¶ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
    
    Returns:
        dict: åŒ…å«ç»Ÿè®¡ç»“æœçš„å­—å…¸
    """
    import numpy as np
    import json
    from pathlib import Path
    
    if not all_results:
        logging.warning("ğŸ“Š all_resultsä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œç»Ÿè®¡åˆ†æ")
        return {}
    
    # è·å–æ‰€æœ‰ç§å­å’Œå˜ä½“åç§°
    seed_keys = list(all_results.keys())
    if len(seed_keys) == 0:
        logging.warning("ğŸ“Š æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç§å­ç»“æœ")
        return {}
    
    # ä»ç¬¬ä¸€ä¸ªç§å­ç»“æœä¸­è·å–å˜ä½“åç§°
    first_seed_result = all_results[seed_keys[0]]
    variant_names = set()
    
    # ä»last_task_accuraciesè·å–å˜ä½“åç§°
    if 'last_task_accuracies' in first_seed_result:
        variant_names.update(first_seed_result['last_task_accuracies'].keys())
    
    # ä»average_accuraciesè·å–å˜ä½“åç§°
    if 'average_accuracies' in first_seed_result:
        variant_names.update(first_seed_result['average_accuracies'].keys())
    
    # ä»cumulative_task_wise_accuraciesè·å–å˜ä½“åç§°
    if 'cumulative_task_wise_accuracies' in first_seed_result:
        variant_names.update(first_seed_result['cumulative_task_wise_accuracies'].keys())
    
    # ä»cumulative_class_wise_accuraciesè·å–å˜ä½“åç§°
    if 'cumulative_class_wise_accuracies' in first_seed_result:
        variant_names.update(first_seed_result['cumulative_class_wise_accuracies'].keys())
    
    # ä»task_wise_accuraciesè·å–å˜ä½“åç§°
    if 'task_wise_accuracies' in first_seed_result:
        variant_names.update(first_seed_result['task_wise_accuracies'].keys())
    
    # ä»class_wise_accuraciesè·å–å˜ä½“åç§°
    if 'class_wise_accuracies' in first_seed_result:
        variant_names.update(first_seed_result['class_wise_accuracies'].keys())
    
    variant_names = sorted(list(variant_names))
    
    if not variant_names:
        logging.warning("ğŸ“Š æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å˜ä½“åç§°")
        return {}
    
    # è·å–ä»»åŠ¡IDåˆ—è¡¨ï¼ˆä»last_task_idæ¨æ–­ï¼‰
    task_ids = []
    if 'last_task_id' in first_seed_result:
        last_task_id = first_seed_result['last_task_id']
        task_ids = list(range(1, last_task_id + 1))  # å‡è®¾ä»»åŠ¡IDä»1å¼€å§‹
    
    num_seeds = len(seed_keys)
    logging.info(f"ğŸ“Š å¼€å§‹åˆ†æ {num_seeds} ä¸ªéšæœºç§å­çš„å®éªŒç»“æœ")
    logging.info(f"ğŸ“Š å‘ç° {len(variant_names)} ä¸ªå˜ä½“: {', '.join(variant_names)}")
    if task_ids:
        logging.info(f"ğŸ“Š å‘ç° {len(task_ids)} ä¸ªä»»åŠ¡: {', '.join(map(str, task_ids))}")
    
    # åˆå§‹åŒ–ç»Ÿè®¡ç»“æœå­—å…¸
    statistics_results = {
        "summary": {
            "num_seeds": num_seeds,
            "num_variants": len(variant_names),
            "num_tasks": len(task_ids),
            "variant_names": variant_names,
            "task_ids": task_ids,
            "dataset_names": dataset_names
        },
        "variants": {}
    }
    
    # è®°å½•ç»Ÿè®¡ç»“æœ
    logging.info("=" * 80)
    logging.info("ğŸ“ˆ å¤šç§å­ç»Ÿè®¡åˆ†æç»“æœ")
    logging.info("=" * 80)
    
    for variant in variant_names:
        logging.info(f"\nğŸ” å˜ä½“: {variant}")
        logging.info("-" * 60)
        
        # åˆå§‹åŒ–å˜ä½“ç»Ÿè®¡ç»“æœ
        variant_stats = {
            "data_wise_accuracy": {},
            "cumulative_average_accuracy": {},
            "cumulative_task_wise_accuracy": {},
            "cumulative_class_wise_accuracy": {},
            "task_wise_accuracy": {},
            "class_wise_accuracy": {}
        }
        
        # 1. æ”¶é›†æ•°æ®é›†çº§åˆ«å‡†ç¡®ç‡æ•°æ®ï¼ˆdata-wiseï¼‰
        data_wise_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if 'last_task_accuracies' in seed_result and variant in seed_result['last_task_accuracies']:
                data_wise_accs.append(seed_result['last_task_accuracies'][variant])
        
        if data_wise_accs:
            mean_data_wise = np.mean(data_wise_accs)
            std_data_wise = np.std(data_wise_accs)
            variant_stats["data_wise_accuracy"] = {
                "mean": float(round(mean_data_wise, 2)),
                "std": float(round(std_data_wise, 2)),
                "raw_values": [float(round(acc, 2)) for acc in data_wise_accs]
            }
            logging.info(f"  æ•°æ®é›†çº§åˆ«å‡†ç¡®ç‡: {mean_data_wise:.2f}% Â± {std_data_wise:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in data_wise_accs])}")
        else:
            variant_stats["data_wise_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  æ•°æ®é›†çº§åˆ«å‡†ç¡®ç‡: æ— æ•°æ®")
        
        # 2. æ”¶é›†ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ•°æ®
        cumulative_avg_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if 'average_accuracies' in seed_result and variant in seed_result['average_accuracies']:
                cumulative_avg_accs.append(seed_result['average_accuracies'][variant])
        
        if cumulative_avg_accs:
            mean_cumulative_avg = np.mean(cumulative_avg_accs)
            std_cumulative_avg = np.std(cumulative_avg_accs)
            variant_stats["cumulative_average_accuracy"] = {
                "mean": float(round(mean_cumulative_avg, 2)),
                "std": float(round(std_cumulative_avg, 2)),
                "raw_values": [float(round(acc, 2)) for acc in cumulative_avg_accs]
            }
            logging.info(f"  ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡: {mean_cumulative_avg:.2f}% Â± {std_cumulative_avg:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in cumulative_avg_accs])}")
        else:
            variant_stats["cumulative_average_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡: æ— æ•°æ®")
        
        # 2.5. æ”¶é›†ä»»åŠ¡çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ•°æ®
        cumulative_task_wise_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if 'cumulative_task_wise_accuracies' in seed_result and variant in seed_result['cumulative_task_wise_accuracies']:
                cumulative_task_wise_accs.append(seed_result['cumulative_task_wise_accuracies'][variant])
        
        if cumulative_task_wise_accs:
            mean_cumulative_task_wise = np.mean(cumulative_task_wise_accs)
            std_cumulative_task_wise = np.std(cumulative_task_wise_accs)
            variant_stats["cumulative_task_wise_accuracy"] = {
                "mean": float(round(mean_cumulative_task_wise, 2)),
                "std": float(round(std_cumulative_task_wise, 2)),
                "raw_values": [float(round(acc, 2)) for acc in cumulative_task_wise_accs]
            }
            logging.info(f"  ä»»åŠ¡çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡: {mean_cumulative_task_wise:.2f}% Â± {std_cumulative_task_wise:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in cumulative_task_wise_accs])}")
        else:
            variant_stats["cumulative_task_wise_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  ä»»åŠ¡çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡: æ— æ•°æ®")
        
        # 2.6. æ”¶é›†ç±»åˆ«çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ•°æ®
        cumulative_class_wise_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if 'cumulative_class_wise_accuracies' in seed_result and variant in seed_result['cumulative_class_wise_accuracies']:
                cumulative_class_wise_accs.append(seed_result['cumulative_class_wise_accuracies'][variant])
        
        if cumulative_class_wise_accs:
            mean_cumulative_class_wise = np.mean(cumulative_class_wise_accs)
            std_cumulative_class_wise = np.std(cumulative_class_wise_accs)
            variant_stats["cumulative_class_wise_accuracy"] = {
                "mean": float(round(mean_cumulative_class_wise, 2)),
                "std": float(round(std_cumulative_class_wise, 2)),
                "raw_values": [float(round(acc, 2)) for acc in cumulative_class_wise_accs]
            }
            logging.info(f"  ç±»åˆ«çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡: {mean_cumulative_class_wise:.2f}% Â± {std_cumulative_class_wise:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in cumulative_class_wise_accs])}")
        else:
            variant_stats["cumulative_class_wise_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  ç±»åˆ«çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡: æ— æ•°æ®")
        
        # 3. æ”¶é›†ä»»åŠ¡çº§åˆ«å¹³å‡å‡†ç¡®ç‡æ•°æ®
        task_wise_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if 'task_wise_accuracies' in seed_result and variant in seed_result['task_wise_accuracies']:
                task_wise_accs.append(seed_result['task_wise_accuracies'][variant])
        
        if task_wise_accs:
            mean_task_wise = np.mean(task_wise_accs)
            std_task_wise = np.std(task_wise_accs)
            variant_stats["task_wise_accuracy"] = {
                "mean": float(round(mean_task_wise, 2)),
                "std": float(round(std_task_wise, 2)),
                "raw_values": [float(round(acc, 2)) for acc in task_wise_accs]
            }
            logging.info(f"  ä»»åŠ¡çº§åˆ«å¹³å‡å‡†ç¡®ç‡: {mean_task_wise:.2f}% Â± {std_task_wise:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in task_wise_accs])}")
        else:
            variant_stats["task_wise_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  ä»»åŠ¡çº§åˆ«å¹³å‡å‡†ç¡®ç‡: æ— æ•°æ®")
        
        # 4. æ”¶é›†ç±»åˆ«çº§åˆ«å¹³å‡å‡†ç¡®ç‡æ•°æ®
        class_wise_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if 'class_wise_accuracies' in seed_result and variant in seed_result['class_wise_accuracies']:
                class_wise_accs.append(seed_result['class_wise_accuracies'][variant])
        
        if class_wise_accs:
            mean_class_wise = np.mean(class_wise_accs)
            std_class_wise = np.std(class_wise_accs)
            variant_stats["class_wise_accuracy"] = {
                "mean": float(round(mean_class_wise, 2)),
                "std": float(round(std_class_wise, 2)),
                "raw_values": [float(round(acc, 2)) for acc in class_wise_accs]
            }
            logging.info(f"  ç±»åˆ«çº§åˆ«å¹³å‡å‡†ç¡®ç‡: {mean_class_wise:.2f}% Â± {std_class_wise:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in class_wise_accs])}")
        else:
            variant_stats["class_wise_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  ç±»åˆ«çº§åˆ«å¹³å‡å‡†ç¡®ç‡: æ— æ•°æ®")
        
        # 5. å¦‚æœæœ‰per-taskè¯¦ç»†ä¿¡æ¯ï¼Œä¹Ÿæ˜¾ç¤ºï¼ˆä»task_wise_accuraciesä¸­æå–ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ£€æŸ¥ç¬¬ä¸€ä¸ªç§å­çš„æ•°æ®ç»“æ„
        first_seed_result = all_results[seed_keys[0]]
        if ('task_wise_accuracies' in first_seed_result and 
            variant in first_seed_result['task_wise_accuracies'] and
            hasattr(first_seed_result['task_wise_accuracies'][variant], 'keys')):
            
            logging.info(f"  å„ä»»åŠ¡è¯¦ç»†å‡†ç¡®ç‡:")
            task_wise_data = first_seed_result['task_wise_accuracies'][variant]
            if isinstance(task_wise_data, dict):
                for task_id, acc in task_wise_data.items():
                    dataset_name = dataset_names[int(task_id)] if dataset_names and int(task_id) < len(dataset_names) else f"Task {task_id}"
                    logging.info(f"    {dataset_name}: {acc:.2f}%")
        
        statistics_results["variants"][variant] = variant_stats
    
    # æ€§èƒ½æ’åæ€»ç»“
    logging.info("\n" + "=" * 80)
    logging.info("ğŸ† æ€§èƒ½æ’åæ€»ç»“")
    logging.info("=" * 80)
    
    # æŒ‰æ•°æ®é›†çº§åˆ«å‡†ç¡®ç‡æ’å
    ranked_by_data_wise = []
    for variant in variant_names:
        if variant_stats := statistics_results["variants"][variant]:
            if "data_wise_accuracy" in variant_stats and "mean" in variant_stats["data_wise_accuracy"]:
                ranked_by_data_wise.append((variant, variant_stats["data_wise_accuracy"]["mean"]))
    
    if ranked_by_data_wise:
        ranked_by_data_wise.sort(key=lambda x: x[1], reverse=True)
        logging.info("ğŸ“ˆ æŒ‰æ•°æ®é›†çº§åˆ«å‡†ç¡®ç‡æ’å:")
        for i, (variant, acc) in enumerate(ranked_by_data_wise, 1):
            logging.info(f"  {i:2d}. {variant:<30}: {acc:.2f}%")
    
    # æŒ‰ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ’å
    ranked_by_cumulative_avg = []
    for variant in variant_names:
        if variant_stats := statistics_results["variants"][variant]:
            if "cumulative_average_accuracy" in variant_stats and "mean" in variant_stats["cumulative_average_accuracy"]:
                ranked_by_cumulative_avg.append((variant, variant_stats["cumulative_average_accuracy"]["mean"]))
    
    if ranked_by_cumulative_avg:
        ranked_by_cumulative_avg.sort(key=lambda x: x[1], reverse=True)
        logging.info("\nğŸ“Š æŒ‰ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ’å:")
        for i, (variant, acc) in enumerate(ranked_by_cumulative_avg, 1):
            logging.info(f"  {i:2d}. {variant:<30}: {acc:.2f}%")
    
    # æŒ‰ä»»åŠ¡çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ’å
    ranked_by_cumulative_task_wise = []
    for variant in variant_names:
        if variant_stats := statistics_results["variants"][variant]:
            if "cumulative_task_wise_accuracy" in variant_stats and "mean" in variant_stats["cumulative_task_wise_accuracy"]:
                ranked_by_cumulative_task_wise.append((variant, variant_stats["cumulative_task_wise_accuracy"]["mean"]))
    
    if ranked_by_cumulative_task_wise:
        ranked_by_cumulative_task_wise.sort(key=lambda x: x[1], reverse=True)
        logging.info("\nğŸ“Š æŒ‰ä»»åŠ¡çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ’å:")
        for i, (variant, acc) in enumerate(ranked_by_cumulative_task_wise, 1):
            logging.info(f"  {i:2d}. {variant:<30}: {acc:.2f}%")
    
    # æŒ‰ç±»åˆ«çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ’å
    ranked_by_cumulative_class_wise = []
    for variant in variant_names:
        if variant_stats := statistics_results["variants"][variant]:
            if "cumulative_class_wise_accuracy" in variant_stats and "mean" in variant_stats["cumulative_class_wise_accuracy"]:
                ranked_by_cumulative_class_wise.append((variant, variant_stats["cumulative_class_wise_accuracy"]["mean"]))
    
    if ranked_by_cumulative_class_wise:
        ranked_by_cumulative_class_wise.sort(key=lambda x: x[1], reverse=True)
        logging.info("\nğŸ“Š æŒ‰ç±»åˆ«çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ’å:")
        for i, (variant, acc) in enumerate(ranked_by_cumulative_class_wise, 1):
            logging.info(f"  {i:2d}. {variant:<30}: {acc:.2f}%")
    
    # ä¿å­˜JSONæ–‡ä»¶
    if save_json:
        try:
            if not output_path:
                # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºè·¯å¾„
                first_seed_log_path = all_results[seed_keys[0]].get('log_path', '')
                if first_seed_log_path:
                    parent_dir = Path(first_seed_log_path).parent
                    output_path = output_path = str(parent_dir / "multi_seed_statistics.json")
                else:
                    output_path = "multi_seed_statistics.json"
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(statistics_results, f, ensure_ascii=False, indent=2)
            logging.info(f"\nğŸ’¾ ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            logging.warning(f"âŒ ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
    
    return statistics_results
