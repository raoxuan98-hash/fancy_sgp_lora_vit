import os
import sys
import logging
import torch
import random
import numpy as np
from collections.abc import Mapping, Sequence
from models.subspace_lora import SubspaceLoRA
from utils.data_manager import WithinDomainDataManager, CrossDomainDataManagerCore
from utils.balanced_cross_domain_data_manager import BalancedCrossDomainDataManagerCore
from utils.toolkit import count_parameters
import re

def train(args):
    all_results = {}
    
    for run_id, seed in enumerate(args['seed_list']):
        args['seed'], args['run_id'] = seed, run_id
        logfile_head, logfile_name = build_log_dirs(args)
        args['log_path'] = logfile_name
        
        # Configure logging with unbuffered file handler for real-time updates
        log_file_path = os.path.join(logfile_name, 'record.log')
        
        # æ¸…é™¤ç°æœ‰çš„æ—¥å¿—å¤„ç†å™¨ï¼Œé¿å…å†²çª
        root_logger = logging.getLogger()
        for handler in root_logger.handlers[:]:
            root_logger.removeHandler(handler)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(filename=log_file_path, mode='a', encoding='utf-8')
        file_handler.stream.reconfigure(line_buffering=True)  # Enable line buffering
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        
        # è®¾ç½®æ ¼å¼
        formatter = logging.Formatter('%(asctime)s [%(filename)s] => %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
        root_logger.setLevel(logging.INFO)
        root_logger.addHandler(file_handler)
        root_logger.addHandler(console_handler)
        
        # æ‰“å°æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œæ–¹ä¾¿ç”¨æˆ·æŸ¥æ‰¾
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_file_path}")
        print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ 'tail -f {log_file_path}' å®æ—¶æŸ¥çœ‹æ—¥å¿—")
        print("-" * 80)
        
        args['log_path'] = logfile_name
        results = train_single_run(args)
        all_results[f"seed_{seed}"] = results
    
    # åœ¨æ‰€æœ‰ç§å­è¿è¡Œå®Œæˆåï¼Œè¿›è¡Œç»Ÿè®¡åˆ†æ
    if len(all_results) > 1:  # åªæœ‰å¤šäºä¸€ä¸ªç§å­æ—¶æ‰è¿›è¡Œç»Ÿè®¡åˆ†æ
        dataset_names = args.get('cross_domain_datasets', None)
        analyze_all_results(all_results, dataset_names, save_json=True)
    

def train_single_run(args, return_model: bool = False):
    # Setting random seed and device for reproducibility
    set_random(args['seed'])
    print_args(args)
    
    # Initialize data manager and model

    if args['cross_domain']:
        # ä½¿ç”¨å¹³è¡¡åçš„cross-domainæ•°æ®é›†
        data_manager = BalancedCrossDomainDataManagerCore(
            dataset_names=args['cross_domain_datasets'],
            balanced_datasets_root="balanced_datasets",
            seed=args['seed'],
            num_shots=args.get('num_shots', 0),
            use_balanced_datasets=True,
            enable_incremental_split=args.get('enable_incremental_split', False),
            num_incremental_splits=args.get('num_incremental_splits', 5),
            incremental_split_seed=args.get('incremental_split_seed', 42))
    else:
        data_manager = WithinDomainDataManager(
            dataset_name=args['dataset'],
            seed=args['seed'],
            init_cls=args['init_cls'],
            increment=args['increment'],
            args=args)
    
    model = SubspaceLoRA(args)
    logging.info(f'All params: {count_parameters(model.network)}')
    logging.info(f'Trainable params: {count_parameters(model.network, True)}')
    final_results = model.loop(data_manager)
    
    # æ·»åŠ log_pathåˆ°ç»“æœä¸­ï¼Œä»¥ä¾¿aggregate_seed_resultså¯ä»¥æ‰¾åˆ°å®ƒ
    if 'log_path' in args:
        final_results['log_path'] = args['log_path']
    
    if return_model:
        return final_results, model
    return final_results

def set_device(device_type):
    """Properly set the device (either CPU or GPU) based on input"""
    if isinstance(device_type, (list, tuple)):
        return [torch.device(f'cuda:{d}' if d != -1 else 'cpu') for d in device_type]
    return torch.device('cuda' if device_type != -1 else 'cpu')

def set_random(seed):
    """Set random seeds to ensure reproducibility"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def print_args(args):
    """Log the arguments for this run"""
    for key, value in args.items():
        logging.info(f'{key}: {value}')


import os
from pathlib import Path
import json

def _filter_args_by_lora_type(args: dict) -> dict:
    """
    è¿‡æ»¤å‚æ•°å­—å…¸ï¼Œåªä¿ç•™ä¸å½“å‰LoRAç±»å‹ç›¸å…³çš„å‚æ•°
    è¿™æ ·å¯ä»¥é¿å…åœ¨params.jsonä¸­ä¿å­˜ä¸ç›¸å…³çš„å‚æ•°ï¼Œå¯¼è‡´æ—¥å¿—å‘½åæ··ä¹±
    """
    lora_type = args.get('lora_type', 'basic_lora')
    filtered_args = args.copy()
    
    # å®šä¹‰æ¯ç§LoRAç±»å‹ç›¸å…³çš„å‚æ•°
    sgp_lora_params = {'weight_temp', 'weight_kind', 'weight_p'}
    nsp_lora_params = {'nsp_eps', 'nsp_weight'}
    # Hopfieldå‚æ•°é€‚ç”¨äºæ‰€æœ‰LoRAç±»å‹
    hopfield_params = {'hopfield_temp', 'hopfield_topk'}
    
    # ç§»é™¤ä¸å½“å‰LoRAç±»å‹ä¸ç›¸å…³çš„å‚æ•°
    if lora_type == 'sgp_lora':
        # ä¿ç•™SGPå‚æ•°å’ŒHopfieldå‚æ•°ï¼Œç§»é™¤NSPå‚æ•°
        for param in nsp_lora_params:
            filtered_args.pop(param, None)
    elif lora_type == 'nsp_lora':
        # ä¿ç•™NSPå‚æ•°å’ŒHopfieldå‚æ•°ï¼Œç§»é™¤SGPå‚æ•°
        for param in sgp_lora_params:
            filtered_args.pop(param, None)
    elif lora_type == 'basic_lora':
        # åªä¿ç•™Hopfieldå‚æ•°ï¼Œç§»é™¤å…¶ä»–LoRAç‰¹å®šå‚æ•°
        for param in sgp_lora_params.union(nsp_lora_params):
            filtered_args.pop(param, None)
    elif lora_type in ['full', 'full_nsp', 'joint_lora', 'joint_full']:
        # å¯¹äºfullç±»å‹ï¼Œåªä¿ç•™Hopfieldå‚æ•°ï¼Œç§»é™¤å…¶ä»–LoRAç‰¹å®šå‚æ•°
        for param in sgp_lora_params.union(nsp_lora_params):
            filtered_args.pop(param, None)
    
    return filtered_args

def build_log_dirs(args: dict, root_dir="."):
    """
    æ ¹æ® args æ„å»ºå¤šçº§æ—¥å¿—ç›®å½•ï¼Œä¼˜åŒ–è·¯å¾„é•¿åº¦å’Œå¯è¯»æ€§
    
    ä¼˜åŒ–åçš„ç›®å½•ç»“æ„ï¼š
    - é¡¶å±‚ç›®å½•ï¼šæ¨¡å‹+ç”¨æˆ·+å®éªŒç±»å‹ï¼ˆæ›´ç®€æ´ï¼‰
    - äºŒçº§ç›®å½•ï¼šæ•°æ®é›†/ä»»åŠ¡é…ç½®ï¼ˆä½¿ç”¨ç¼©å†™ï¼‰
    - ä¸‰çº§ç›®å½•ï¼šæ–¹æ³•å‚æ•°ï¼ˆåˆå¹¶å¤šä¸ªå‚æ•°ç»„ï¼‰
    - å››çº§ç›®å½•ï¼šè®­ç»ƒå‚æ•°ï¼ˆç®€åŒ–è¡¨ç¤ºï¼‰
    - äº”çº§ç›®å½•ï¼šç§å­ï¼ˆä¿æŒä¸å˜ï¼‰
    
    ä¸»è¦ä¼˜åŒ–ç‚¹ï¼š
    1. ç¼©çŸ­ç›®å½•åç§°é•¿åº¦
    2. åˆå¹¶ç›¸å…³å‚æ•°ç»„
    3. ä½¿ç”¨æ›´æœ‰æ„ä¹‰çš„ç¼©å†™
    4. é¿å…é‡å¤ä¿¡æ¯
    """

    def sanitize_filename(s: str) -> str:
        """ç§»é™¤æˆ–æ›¿æ¢æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
        # Windows éæ³•å­—ç¬¦: \ / : * ? " < > |
        s = re.sub(r'[\\/:*?"<>|]', '_', str(s))
        # å‹ç¼©è¿ç»­ä¸‹åˆ’çº¿
        s = re.sub(r'_+', '_', s)
        return s.strip('_')

    def short(s: str, maxlen=30) -> str:
        """æˆªæ–­è¿‡é•¿å­—ç¬¦ä¸²ï¼Œä¿æŒå¯è¯»æ€§"""
        s = sanitize_filename(str(s))
        if len(s) <= maxlen:
            return s
        # å°è¯•ä¿æŒå…³é”®éƒ¨åˆ†
        return s[:maxlen].rstrip('_')

    def _get_vit_short_name(vit_type: str) -> str:
        """å°†å®Œæ•´çš„ViTç±»å‹åè½¬æ¢ä¸ºçŸ­åç§°"""
        vit_type = vit_type.lower().strip()
        
        # æ ¹æ®æ‚¨æä¾›çš„choicesè¿›è¡Œæ˜ å°„
        mapping = {
            'vit-b-p16': 'B16',           # ViT-Base/16
            'vit-b-p16-dino': 'B16_D',    # ViT-Base/16 + DINOé¢„è®­ç»ƒ
            'vit-b-p16-mae': 'B16_M',     # ViT-Base/16 + MAEé¢„è®­ç»ƒ
            'vit-b-p16-clip': 'B16_C',    # ViT-Base/16 + CLIPé¢„è®­ç»ƒ
            'vit-b-p16-mocov3': 'B16_M3', # ViT-Base/16 + MoCoV3é¢„è®­ç»ƒ
        }
        
        # å¦‚æœæ‰¾åˆ°æ˜ å°„ï¼Œè¿”å›çŸ­åç§°ï¼›å¦åˆ™è¿”å›åŸå§‹å€¼çš„å‰6ä¸ªå­—ç¬¦
        return mapping.get(vit_type, short(vit_type, 8))

    def _get_lora_specific_params(lora_type: str, args: dict) -> list:
        """è·å–ç‰¹å®š LoRA ç±»å‹çš„å‚æ•°ï¼Œä½¿ç”¨æ›´ç®€æ´çš„è¡¨ç¤º"""
        params = []
        
        # LoRAåŸºç¡€å‚æ•°ï¼ˆæ‰€æœ‰ç±»å‹éƒ½æœ‰ï¼‰
        if 'lora_rank' in args:
            params.append(f"r{short(args['lora_rank'], 3)}")
        
        # LoRAç±»å‹
        if lora_type == 'sgp_lora':
            params.append("SGP")
            # SGP LoRA ç‰¹æœ‰å‚æ•°
            if 'weight_temp' in args:
                params.append(f"t{short(args['weight_temp'], 4)}")
            if 'weight_kind' in args:
                params.append(f"k{short(args['weight_kind'], 4)}")
            if 'weight_p' in args:
                params.append(f"p{short(args['weight_p'], 4)}")
                
        elif lora_type == 'nsp_lora':
            params.append("NSP")
            # NSP LoRA ç‰¹æœ‰å‚æ•°
            if 'nsp_eps' in args:
                params.append(f"Îµ{short(args['nsp_eps'], 4)}")
            if 'nsp_weight' in args:
                params.append(f"w{short(args['nsp_weight'], 4)}")
                
        elif lora_type == 'basic_lora':
            params.append("Basic")
            
        elif lora_type == 'full':
            params.append("Full")
            
        elif lora_type == 'full_nsp':
            params.append("Full_NSP")
            # Full_NSPç‰¹æœ‰å‚æ•°
            if 'nsp_eps' in args:
                params.append(f"Îµ{short(args['nsp_eps'], 4)}")
            if 'nsp_weight' in args:
                params.append(f"w{short(args['nsp_weight'], 4)}")
                
        elif lora_type == 'joint_lora':
            params.append("Joint_LORA")
            # joint_loraå¯ä»¥åŒ…å«LoRAç›¸å…³å‚æ•°
            if 'lora_rank' in args:
                params.append(f"r{short(args['lora_rank'], 3)}")
                
        elif lora_type == 'joint_full':
            params.append("Joint_Full")
        
        # Hopfieldå‚æ•°ï¼ˆé€‚ç”¨äºæ‰€æœ‰LoRAç±»å‹ï¼‰
        if 'hopfield_temp' in args and args['hopfield_temp'] != 0.07:
            params.append(f"ht{short(args['hopfield_temp'], 4)}")
        if 'hopfield_topk' in args and args['hopfield_topk'] != 5:
            params.append(f"hk{short(args['hopfield_topk'], 3)}")
        
        # æƒé‡æ’å€¼å‚æ•°
        if args.get('enable_weight_interpolation', False):
            alpha = args.get('interpolation_alpha', 1.0)
            if alpha < 1.0:  # åªæœ‰å½“æ’å€¼å®é™…å¯ç”¨æ—¶æ‰æ˜¾ç¤º
                params.append(f"interp{short(alpha, 4)}")

        return params

    def _get_kd_params(args: dict) -> list:
        """è·å–çŸ¥è¯†è’¸é¦ç›¸å…³å‚æ•°ï¼Œç®€åŒ–è¡¨ç¤º"""
        kd_params = []
        
        if args.get('gamma_kd', 0.0) > 0.0:
            kd_params.append(f"kd{short(args['gamma_kd'], 4)}")
            if 'kd_type' in args:
                # åªå–ç¬¬ä¸€ä¸ªå­—æ¯ä½œä¸ºç®€å†™
                kd_type = str(args['kd_type'])[0].upper()
                kd_params.append(f"T{kd_type}")
            if 'distillation_transform' in args:
                dt = str(args['distillation_transform'])
                if dt == 'none':
                    kd_params.append("DTN")
                elif dt == 'log':
                    kd_params.append("DTL")
                else:
                    kd_params.append(f"DT{dt[:3]}")
            if args.get('use_aux_for_kd', False):
                kd_params.append("AUX")
            if 'update_teacher_each_task' in args:
                kd_params.append(f"UT{short(args['update_teacher_each_task'], 3)}")
                
        return kd_params

    def _get_auxiliary_params(args: dict) -> list:
        """è·å–è¾…åŠ©æ•°æ®ç›¸å…³å‚æ•°ï¼Œç®€åŒ–è¡¨ç¤º"""
        aux_params = []
        
        # feature_combination_typeå‚æ•°
        if 'feature_combination_type' in args:
            fc_type = args['feature_combination_type']
            if fc_type == 'combined':
                aux_params.append("C")
            elif fc_type == 'aux_only':
                aux_params.append("A")
            elif fc_type == 'current_only':
                aux_params.append("O")
        
        # auxiliary_data_sizeå‚æ•°
        aux_size = args.get('auxiliary_data_size', 2048)
        if aux_size != 2048:  # åªåœ¨éé»˜è®¤æƒ…å†µä¸‹æ˜¾ç¤º
            if aux_size < 1000:
                aux_params.append(f"AS{aux_size}")
            else:
                aux_params.append(f"AS{aux_size//1000}K")
            
        return aux_params

    def _get_incremental_split_params(args: dict) -> list:
        """è·å–å¢é‡æ‹†åˆ†ç›¸å…³å‚æ•°ï¼Œç®€åŒ–è¡¨ç¤º"""
        inc_params = []
        
        if args.get('enable_incremental_split', False):
            inc_params.append("I")
            num_splits = args.get('num_incremental_splits', 2)
            if num_splits != 2:
                inc_params.append(f"S{num_splits}")
        else:
            inc_params.append("NI")  # Not Incremental
            
        return inc_params

    def _validate_parameters(args: dict) -> None:
        """éªŒè¯å‚æ•°ç»„åˆçš„åˆç†æ€§"""
        lora_type = args.get('lora_type', 'basic_lora')
        
        # æ£€æŸ¥ LoRA ç‰¹å®šå‚æ•°æ˜¯å¦è¢«è¯¯ç”¨
        if lora_type not in ['sgp_lora', 'joint_lora']:
            sgp_params = ['weight_temp', 'weight_kind', 'weight_p']
            for param in sgp_params:
                if param in args and args[param] is not None:
                    logging.warning(f"âš ï¸ Parameter '{param}' is being used with lora_type='{lora_type}', but it's specific to sgp_lora")
        
        if lora_type not in ['nsp_lora', 'full_nsp', 'joint_lora', 'joint_full']:
            nsp_params = ['nsp_eps', 'nsp_weight']
            for param in nsp_params:
                if param in args and args[param] is not None:
                    logging.warning(f"âš ï¸ Parameter '{param}' is being used with lora_type='{lora_type}', but it's specific to nsp_lora")

    # å‚æ•°éªŒè¯
    _validate_parameters(args)

    # ç¡®å®šå®éªŒç±»å‹ï¼šcross-domain æˆ– within-domain
    is_cross_domain = args.get('cross_domain', False)
    
    # 1. é¡¶å±‚ç›®å½•ï¼šæ¨¡å‹+ç”¨æˆ·+å®éªŒç±»å‹ï¼ˆä½¿ç”¨ç®€çŸ­æ ‡è¯†ï¼‰
    experiment_type = "CD" if is_cross_domain else "WD"  # CD: Cross-Domain, WD: Within-Domain
    model_short = args.get('model_name', 'SLDC').split('_')[0][:4]  # å–æ¨¡å‹å‰4ä¸ªå­—ç¬¦
    base_dir = os.path.join(
        root_dir,
        f"{model_short}_{experiment_type}_{short(args.get('user', 'user'))}"
    )

    # 2. æ•°æ®é›†/ä»»åŠ¡é…ç½®å±‚ï¼ˆæ ¹æ®å®éªŒç±»å‹ï¼‰
    if is_cross_domain:
        # è·¨åŸŸå®éªŒï¼šä½¿ç”¨ç®€åŒ–çš„æ ‡è¯†ç¬¦
        if 'cross_domain_datasets' in args:
            datasets = args['cross_domain_datasets']
            vit_short = _get_vit_short_name(args.get('vit_type', 'vit-b-p16'))
            if isinstance(datasets, list):
                if len(datasets) <= 3:
                    # å¦‚æœæ•°æ®é›†æ•°é‡å°‘ï¼Œæ˜¾ç¤ºç¼©å†™
                    ds_names = "_".join([short(ds[:6], 6) for ds in datasets])
                else:
                    # æ•°æ®é›†æ•°é‡å¤šï¼Œåªæ˜¾ç¤ºæ•°é‡å’Œç¬¬ä¸€ä¸ª
                    ds_names = f"{len(datasets)}ds_{short(datasets[0][:6], 6)}"
            else:
                ds_names = short(str(datasets)[:15], 15)
            
            task_dir = os.path.join(
                base_dir,
                f"DS_{ds_names}",
                f"V{vit_short}",  # ä½¿ç”¨çŸ­åç§°
                f"SH{args.get('num_shots', 0)}"
            )
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šè·¨åŸŸæ•°æ®é›†ï¼Œä½¿ç”¨é»˜è®¤æ ‡è¯†
            task_dir = os.path.join(
                base_dir,
                f"V{short(args.get('vit_type', 'b16'))}",
                f"SH{args.get('num_shots', 0)}"
            )
    else:
        # åŸŸå†…å®éªŒï¼šä½¿ç”¨ä¼ ç»Ÿçš„init_clså’Œincrementå‚æ•°
        vit_short = _get_vit_short_name(args.get('vit_type', 'vit-b-p16'))
        task_dir = os.path.join(
            base_dir,
            f"DS_{short(args.get('dataset', 'cifar100'))}",
            f"V{vit_short}",  # ä½¿ç”¨çŸ­åç§°
            f"I{args.get('init_cls', 50)}_C{args.get('increment', 10)}"
        )

    # 3. æ–¹æ³•å‚æ•°å±‚ï¼ˆåˆå¹¶æ‰€æœ‰æ–¹æ³•ç›¸å…³å‚æ•°ï¼‰
    lora_params = _get_lora_specific_params(args.get('lora_type', 'basic_lora'), args)
    kd_params = _get_kd_params(args)
    aux_params = _get_auxiliary_params(args)
    inc_split_params = _get_incremental_split_params(args)
    
    # åˆå¹¶æ‰€æœ‰æ–¹æ³•å‚æ•°
    all_method_params = lora_params + kd_params + aux_params + inc_split_params
    method_str = "_".join(all_method_params) if all_method_params else "default"
    
    method_dir = os.path.join(task_dir, short(method_str, maxlen=40))

    # 4. è®­ç»ƒå‚æ•°å±‚ï¼ˆç®€åŒ–ï¼‰
    opt_params = [
        f"O{short(args.get('optimizer', 'sgd')[:3])}",
        f"LR{short(args.get('lrate', 0.1))}",
        f"B{args.get('batch_size', 64)}",
        f"IT{args.get('iterations')}"
    ]
    opt_str = "_".join(opt_params)
    opt_dir = os.path.join(method_dir, short(opt_str, maxlen=30))

    # === é€çº§åˆ›å»ºç›®å½• ===
    abs_log_dir = os.path.abspath(opt_dir)
    current = Path(abs_log_dir).root
    for part in Path(abs_log_dir).parts[1:]:
        current = Path(current) / part
        try:
            current.mkdir(exist_ok=True)
        except OSError as e:
            # å¦‚æœç›®å½•åå¤ªé•¿ï¼Œå°è¯•ç¼©çŸ­
            if len(str(current)) > 200:
                # ç¼©çŸ­æœ€åä¸€éƒ¨åˆ†
                parent = current.parent
                short_part = short(current.name, maxlen=20)
                current = parent / short_part
                current.mkdir(exist_ok=True)
            else:
                raise e

    # ä¿å­˜è¿‡æ»¤åçš„å‚æ•°åˆ° JSONï¼Œé¿å…å‚æ•°äº¤å‰æ±¡æŸ“
    filtered_args = _filter_args_by_lora_type(args)
    params_json = Path(abs_log_dir) / "params.json"
    if not params_json.exists():
        with open(params_json, "w", encoding="utf-8") as f:
            json.dump(filtered_args, f, ensure_ascii=False, indent=2)

    # 5. ç§å­å±‚ï¼ˆä¿æŒä¸å˜ï¼‰
    seed_dir = os.path.join(abs_log_dir, f"seed_{args['seed']}")
    os.makedirs(seed_dir, exist_ok=True)

    # è®°å½•æ—¥å¿—ç›®å½•ä¿¡æ¯
    logging.info(f"ğŸ“ æ—¥å¿—ç›®å½•ç»“æ„:")
    logging.info(f"  é¡¶å±‚: {base_dir}")
    logging.info(f"  ä»»åŠ¡é…ç½®: {os.path.basename(task_dir)}")
    logging.info(f"  æ–¹æ³•å‚æ•°: {method_str}")
    logging.info(f"  è®­ç»ƒå‚æ•°: {opt_str}")
    logging.info(f"  ç§å­: seed_{args['seed']}")
    logging.info(f"  å®Œæ•´è·¯å¾„: {seed_dir}")

    return os.path.dirname(abs_log_dir), str(seed_dir)

def analyze_all_results(all_results: dict, dataset_names: list = [], save_json: bool = True, output_path: str = "") -> dict:
    """
    åˆ†æall_resultsä¸­å¤šä¸ªéšæœºç§å­çš„ç»“æœï¼Œè®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®å¹¶è®°å½•åˆ°æ—¥å¿—
    é€‚é…æ–°çš„final_resultsæ ¼å¼
    
    Args:
        all_results: åŒ…å«å¤šä¸ªéšæœºç§å­ç»“æœçš„å­—å…¸
        dataset_names: æ•°æ®é›†åç§°åˆ—è¡¨ï¼Œç”¨äºæ—¥å¿—è¾“å‡º
        save_json: æ˜¯å¦å°†ç»Ÿè®¡ç»“æœä¿å­˜ä¸ºJSONæ–‡ä»¶
        output_path: JSONæ–‡ä»¶ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
    
    Returns:
        dict: åŒ…å«ç»Ÿè®¡ç»“æœçš„å­—å…¸
    """
    import numpy as np
    import json
    from pathlib import Path
    
    if not all_results:
        logging.warning("ğŸ“Š all_resultsä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œç»Ÿè®¡åˆ†æ")
        return {}
    
    # è·å–æ‰€æœ‰ç§å­å’Œå˜ä½“åç§°
    seed_keys = list(all_results.keys())
    if len(seed_keys) == 0:
        logging.warning("ğŸ“Š æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç§å­ç»“æœ")
        return {}
    
    # ä»ç¬¬ä¸€ä¸ªç§å­ç»“æœä¸­è·å–å˜ä½“åç§°
    first_seed_result = all_results[seed_keys[0]]
    variant_names = set()
    
    # ä»last_task_accuraciesè·å–å˜ä½“åç§°
    if 'last_task_accuracies' in first_seed_result:
        variant_names.update(first_seed_result['last_task_accuracies'].keys())
    
    # ä»average_accuraciesè·å–å˜ä½“åç§°
    if 'average_accuracies' in first_seed_result:
        variant_names.update(first_seed_result['average_accuracies'].keys())
    
    # ä»cumulative_task_wise_accuraciesè·å–å˜ä½“åç§°
    if 'cumulative_task_wise_accuracies' in first_seed_result:
        variant_names.update(first_seed_result['cumulative_task_wise_accuracies'].keys())
    
    # ä»cumulative_class_wise_accuraciesè·å–å˜ä½“åç§°
    if 'cumulative_class_wise_accuracies' in first_seed_result:
        variant_names.update(first_seed_result['cumulative_class_wise_accuracies'].keys())
    
    # ä»task_wise_accuraciesè·å–å˜ä½“åç§°
    if 'task_wise_accuracies' in first_seed_result:
        variant_names.update(first_seed_result['task_wise_accuracies'].keys())
    
    # ä»class_wise_accuraciesè·å–å˜ä½“åç§°
    if 'class_wise_accuracies' in first_seed_result:
        variant_names.update(first_seed_result['class_wise_accuracies'].keys())
    
    variant_names = sorted(list(variant_names))
    
    if not variant_names:
        logging.warning("ğŸ“Š æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å˜ä½“åç§°")
        return {}
    
    # è·å–ä»»åŠ¡IDåˆ—è¡¨ï¼ˆä»last_task_idæ¨æ–­ï¼‰
    task_ids = []
    if 'last_task_id' in first_seed_result:
        last_task_id = first_seed_result['last_task_id']
        task_ids = list(range(1, last_task_id + 1))  # å‡è®¾ä»»åŠ¡IDä»1å¼€å§‹
    
    num_seeds = len(seed_keys)
    logging.info(f"ğŸ“Š å¼€å§‹åˆ†æ {num_seeds} ä¸ªéšæœºç§å­çš„å®éªŒç»“æœ")
    logging.info(f"ğŸ“Š å‘ç° {len(variant_names)} ä¸ªå˜ä½“: {', '.join(variant_names)}")
    if task_ids:
        logging.info(f"ğŸ“Š å‘ç° {len(task_ids)} ä¸ªä»»åŠ¡: {', '.join(map(str, task_ids))}")
    
    # åˆå§‹åŒ–ç»Ÿè®¡ç»“æœå­—å…¸
    statistics_results = {
        "summary": {
            "num_seeds": num_seeds,
            "num_variants": len(variant_names),
            "num_tasks": len(task_ids),
            "variant_names": variant_names,
            "task_ids": task_ids,
            "dataset_names": dataset_names
        },
        "variants": {}
    }
    
    # è®°å½•ç»Ÿè®¡ç»“æœ
    logging.info("=" * 80)
    logging.info("ğŸ“ˆ å¤šç§å­ç»Ÿè®¡åˆ†æç»“æœ")
    logging.info("=" * 80)
    
    for variant in variant_names:
        logging.info(f"\nğŸ” å˜ä½“: {variant}")
        logging.info("-" * 60)
        
        # åˆå§‹åŒ–å˜ä½“ç»Ÿè®¡ç»“æœ
        variant_stats = {
            "data_wise_accuracy": {},
            "cumulative_average_accuracy": {},
            "cumulative_task_wise_accuracy": {},
            "cumulative_class_wise_accuracy": {},
            "task_wise_accuracy": {},
            "class_wise_accuracy": {}
        }
        
        # 1. æ”¶é›†æ•°æ®é›†çº§åˆ«å‡†ç¡®ç‡æ•°æ®ï¼ˆdata-wiseï¼‰
        data_wise_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if 'last_task_accuracies' in seed_result and variant in seed_result['last_task_accuracies']:
                data_wise_accs.append(seed_result['last_task_accuracies'][variant])
        
        if data_wise_accs:
            mean_data_wise = np.mean(data_wise_accs)
            std_data_wise = np.std(data_wise_accs)
            variant_stats["data_wise_accuracy"] = {
                "mean": float(round(mean_data_wise, 2)),
                "std": float(round(std_data_wise, 2)),
                "raw_values": [float(round(acc, 2)) for acc in data_wise_accs]
            }
            logging.info(f"  æ•°æ®é›†çº§åˆ«å‡†ç¡®ç‡: {mean_data_wise:.2f}% Â± {std_data_wise:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in data_wise_accs])}")
        else:
            variant_stats["data_wise_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  æ•°æ®é›†çº§åˆ«å‡†ç¡®ç‡: æ— æ•°æ®")
        
        # 2. æ”¶é›†ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ•°æ®
        cumulative_avg_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if 'average_accuracies' in seed_result and variant in seed_result['average_accuracies']:
                cumulative_avg_accs.append(seed_result['average_accuracies'][variant])
        
        if cumulative_avg_accs:
            mean_cumulative_avg = np.mean(cumulative_avg_accs)
            std_cumulative_avg = np.std(cumulative_avg_accs)
            variant_stats["cumulative_average_accuracy"] = {
                "mean": float(round(mean_cumulative_avg, 2)),
                "std": float(round(std_cumulative_avg, 2)),
                "raw_values": [float(round(acc, 2)) for acc in cumulative_avg_accs]
            }
            logging.info(f"  ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡: {mean_cumulative_avg:.2f}% Â± {std_cumulative_avg:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in cumulative_avg_accs])}")
        else:
            variant_stats["cumulative_average_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡: æ— æ•°æ®")
        
        # 2.5. æ”¶é›†ä»»åŠ¡çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ•°æ®
        cumulative_task_wise_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if 'cumulative_task_wise_accuracies' in seed_result and variant in seed_result['cumulative_task_wise_accuracies']:
                cumulative_task_wise_accs.append(seed_result['cumulative_task_wise_accuracies'][variant])
        
        if cumulative_task_wise_accs:
            mean_cumulative_task_wise = np.mean(cumulative_task_wise_accs)
            std_cumulative_task_wise = np.std(cumulative_task_wise_accs)
            variant_stats["cumulative_task_wise_accuracy"] = {
                "mean": float(round(mean_cumulative_task_wise, 2)),
                "std": float(round(std_cumulative_task_wise, 2)),
                "raw_values": [float(round(acc, 2)) for acc in cumulative_task_wise_accs]
            }
            logging.info(f"  ä»»åŠ¡çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡: {mean_cumulative_task_wise:.2f}% Â± {std_cumulative_task_wise:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in cumulative_task_wise_accs])}")
        else:
            variant_stats["cumulative_task_wise_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  ä»»åŠ¡çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡: æ— æ•°æ®")
        
        # 2.6. æ”¶é›†ç±»åˆ«çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ•°æ®
        cumulative_class_wise_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if 'cumulative_class_wise_accuracies' in seed_result and variant in seed_result['cumulative_class_wise_accuracies']:
                cumulative_class_wise_accs.append(seed_result['cumulative_class_wise_accuracies'][variant])
        
        if cumulative_class_wise_accs:
            mean_cumulative_class_wise = np.mean(cumulative_class_wise_accs)
            std_cumulative_class_wise = np.std(cumulative_class_wise_accs)
            variant_stats["cumulative_class_wise_accuracy"] = {
                "mean": float(round(mean_cumulative_class_wise, 2)),
                "std": float(round(std_cumulative_class_wise, 2)),
                "raw_values": [float(round(acc, 2)) for acc in cumulative_class_wise_accs]
            }
            logging.info(f"  ç±»åˆ«çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡: {mean_cumulative_class_wise:.2f}% Â± {std_cumulative_class_wise:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in cumulative_class_wise_accs])}")
        else:
            variant_stats["cumulative_class_wise_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  ç±»åˆ«çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡: æ— æ•°æ®")
        
        # 3. æ”¶é›†ä»»åŠ¡çº§åˆ«å¹³å‡å‡†ç¡®ç‡æ•°æ®
        task_wise_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if 'task_wise_accuracies' in seed_result and variant in seed_result['task_wise_accuracies']:
                task_wise_accs.append(seed_result['task_wise_accuracies'][variant])
        
        if task_wise_accs:
            mean_task_wise = np.mean(task_wise_accs)
            std_task_wise = np.std(task_wise_accs)
            variant_stats["task_wise_accuracy"] = {
                "mean": float(round(mean_task_wise, 2)),
                "std": float(round(std_task_wise, 2)),
                "raw_values": [float(round(acc, 2)) for acc in task_wise_accs]
            }
            logging.info(f"  ä»»åŠ¡çº§åˆ«å¹³å‡å‡†ç¡®ç‡: {mean_task_wise:.2f}% Â± {std_task_wise:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in task_wise_accs])}")
        else:
            variant_stats["task_wise_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  ä»»åŠ¡çº§åˆ«å¹³å‡å‡†ç¡®ç‡: æ— æ•°æ®")
        
        # 4. æ”¶é›†ç±»åˆ«çº§åˆ«å¹³å‡å‡†ç¡®ç‡æ•°æ®
        class_wise_accs = []
        for seed_key in seed_keys:
            seed_result = all_results[seed_key]
            if 'class_wise_accuracies' in seed_result and variant in seed_result['class_wise_accuracies']:
                class_wise_accs.append(seed_result['class_wise_accuracies'][variant])
        
        if class_wise_accs:
            mean_class_wise = np.mean(class_wise_accs)
            std_class_wise = np.std(class_wise_accs)
            variant_stats["class_wise_accuracy"] = {
                "mean": float(round(mean_class_wise, 2)),
                "std": float(round(std_class_wise, 2)),
                "raw_values": [float(round(acc, 2)) for acc in class_wise_accs]
            }
            logging.info(f"  ç±»åˆ«çº§åˆ«å¹³å‡å‡†ç¡®ç‡: {mean_class_wise:.2f}% Â± {std_class_wise:.2f}%")
            logging.info(f"    è¯¦ç»†æ•°æ®: {', '.join([f'{acc:.2f}%' for acc in class_wise_accs])}")
        else:
            variant_stats["class_wise_accuracy"] = {"error": "æ— æ•°æ®"}
            logging.info(f"  ç±»åˆ«çº§åˆ«å¹³å‡å‡†ç¡®ç‡: æ— æ•°æ®")
        
        # 5. å¦‚æœæœ‰per-taskè¯¦ç»†ä¿¡æ¯ï¼Œä¹Ÿæ˜¾ç¤ºï¼ˆä»task_wise_accuraciesä¸­æå–ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ£€æŸ¥ç¬¬ä¸€ä¸ªç§å­çš„æ•°æ®ç»“æ„
        first_seed_result = all_results[seed_keys[0]]
        if ('task_wise_accuracies' in first_seed_result and 
            variant in first_seed_result['task_wise_accuracies'] and
            hasattr(first_seed_result['task_wise_accuracies'][variant], 'keys')):
            
            logging.info(f"  å„ä»»åŠ¡è¯¦ç»†å‡†ç¡®ç‡:")
            task_wise_data = first_seed_result['task_wise_accuracies'][variant]
            if isinstance(task_wise_data, dict):
                for task_id, acc in task_wise_data.items():
                    # ä¿®å¤ä»»åŠ¡åç§°æ˜¾ç¤ºé€»è¾‘ï¼Œé€‚é…å¢é‡æ‹†åˆ†
                    if dataset_names and int(task_id) < len(dataset_names):
                        dataset_name = dataset_names[int(task_id)]
                        # å¯¹äºå¢é‡æ‹†åˆ†åœºæ™¯ï¼Œè¿›ä¸€æ­¥æ¸…ç†æ•°æ®é›†åç§°
                        if dataset_name.endswith('_split_0') or dataset_name.endswith('_split_1'):
                            dataset_name = dataset_name.split('_split_')[0]
                    else:
                        dataset_name = f"Task {task_id}"
                    logging.info(f"    {dataset_name}: {acc:.2f}%")
        
        statistics_results["variants"][variant] = variant_stats
    
    # æ€§èƒ½æ’åæ€»ç»“
    logging.info("\n" + "=" * 80)
    logging.info("ğŸ† æ€§èƒ½æ’åæ€»ç»“")
    logging.info("=" * 80)
    
    # æŒ‰æ•°æ®é›†çº§åˆ«å‡†ç¡®ç‡æ’å
    ranked_by_data_wise = []
    for variant in variant_names:
        if variant_stats := statistics_results["variants"][variant]:
            if "data_wise_accuracy" in variant_stats and "mean" in variant_stats["data_wise_accuracy"]:
                ranked_by_data_wise.append((variant, variant_stats["data_wise_accuracy"]["mean"]))
    
    if ranked_by_data_wise:
        ranked_by_data_wise.sort(key=lambda x: x[1], reverse=True)
        logging.info("ğŸ“ˆ æŒ‰æ•°æ®é›†çº§åˆ«å‡†ç¡®ç‡æ’å:")
        for i, (variant, acc) in enumerate(ranked_by_data_wise, 1):
            logging.info(f"  {i:2d}. {variant:<30}: {acc:.2f}%")
    
    # æŒ‰ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ’å
    ranked_by_cumulative_avg = []
    for variant in variant_names:
        if variant_stats := statistics_results["variants"][variant]:
            if "cumulative_average_accuracy" in variant_stats and "mean" in variant_stats["cumulative_average_accuracy"]:
                ranked_by_cumulative_avg.append((variant, variant_stats["cumulative_average_accuracy"]["mean"]))
    
    if ranked_by_cumulative_avg:
        ranked_by_cumulative_avg.sort(key=lambda x: x[1], reverse=True)
        logging.info("\nğŸ“Š æŒ‰ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ’å:")
        for i, (variant, acc) in enumerate(ranked_by_cumulative_avg, 1):
            logging.info(f"  {i:2d}. {variant:<30}: {acc:.2f}%")
    
    # æŒ‰ä»»åŠ¡çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ’å
    ranked_by_cumulative_task_wise = []
    for variant in variant_names:
        if variant_stats := statistics_results["variants"][variant]:
            if "cumulative_task_wise_accuracy" in variant_stats and "mean" in variant_stats["cumulative_task_wise_accuracy"]:
                ranked_by_cumulative_task_wise.append((variant, variant_stats["cumulative_task_wise_accuracy"]["mean"]))
    
    if ranked_by_cumulative_task_wise:
        ranked_by_cumulative_task_wise.sort(key=lambda x: x[1], reverse=True)
        logging.info("\nğŸ“Š æŒ‰ä»»åŠ¡çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ’å:")
        for i, (variant, acc) in enumerate(ranked_by_cumulative_task_wise, 1):
            logging.info(f"  {i:2d}. {variant:<30}: {acc:.2f}%")
    
    # æŒ‰ç±»åˆ«çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ’å
    ranked_by_cumulative_class_wise = []
    for variant in variant_names:
        if variant_stats := statistics_results["variants"][variant]:
            if "cumulative_class_wise_accuracy" in variant_stats and "mean" in variant_stats["cumulative_class_wise_accuracy"]:
                ranked_by_cumulative_class_wise.append((variant, variant_stats["cumulative_class_wise_accuracy"]["mean"]))
    
    if ranked_by_cumulative_class_wise:
        ranked_by_cumulative_class_wise.sort(key=lambda x: x[1], reverse=True)
        logging.info("\nğŸ“Š æŒ‰ç±»åˆ«çº§åˆ«ç´¯ç§¯å¹³å‡å‡†ç¡®ç‡æ’å:")
        for i, (variant, acc) in enumerate(ranked_by_cumulative_class_wise, 1):
            logging.info(f"  {i:2d}. {variant:<30}: {acc:.2f}%")
    
    # ä¿å­˜JSONæ–‡ä»¶
    if save_json:
        try:
            if not output_path:
                # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºè·¯å¾„
                first_seed_log_path = all_results[seed_keys[0]].get('log_path', '')
                if first_seed_log_path:
                    parent_dir = Path(first_seed_log_path).parent
                    output_path = output_path = str(parent_dir / "multi_seed_statistics.json")
                else:
                    output_path = "multi_seed_statistics.json"
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(statistics_results, f, ensure_ascii=False, indent=2)
            logging.info(f"\nğŸ’¾ ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            logging.warning(f"âŒ ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
    
    return statistics_results