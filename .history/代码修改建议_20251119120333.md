# SubspaceLoRA 显存优化 - 具体代码修改建议

## 🎯 高优先级修改 (立即实施)

### 1. 修复 distribution_compensator.py Line 262 - 大规模缓存问题

**原始问题**：
```python
# Line 262
self.cached_Z = torch.randn(50000, self.feature_dim)
```

**修改方案**：
```python
def __init__(self, ..., adaptive_cache_size=True):
    # ... 其他初始化代码
    self.adaptive_cache_size = adaptive_cache_size
    self.max_cache_size = 2000  # 从50000降到2000
    self.cached_Z = None
    self.current_cache_size = 0
    self.access_count = 0

def _get_adaptive_cache_size(self, feature_dim):
    """根据特征维度和可用内存动态调整缓存大小"""
    if not self.adaptive_cache_size:
        return self.max_cache_size
    
    # 估算每个特征向量占用的内存 (float32 = 4 bytes)
    vector_memory_gb = feature_dim * 4 / (1024**3)
    
    # 根据可用内存动态调整，目标占用不超过1GB
    max_vectors_by_memory = int(1.0 / vector_memory_gb) if vector_memory_gb > 0 else 1000
    
    # 取最小值
    return min(self.max_cache_size, max_vectors_by_memory, 5000)

def _initialize_cached_Z(self, feature_dim):
    """按需初始化缓存，避免立即占用大量显存"""
    if self.cached_Z is None:
        cache_size = self._get_adaptive_cache_size(feature_dim)
        self.cached_Z = torch.randn(cache_size, feature_dim)
        self.current_cache_size = cache_size
        logging.info(f"[CACHE] Initialized adaptive cache with {cache_size} vectors")
```

### 2. 优化特征提取过程 (Line 74-76) - 内存积累问题

**原始问题**：
```python
feats_before, feats_after, labels = [], [], []
for batch in data_loader:
    inputs = batch[0]
    targets = batch[1]
    inputs = inputs.to(self.device)
    feats_before.append(model_before(inputs).cpu())  # 累积大量张量
    feats_after.append(model_after(inputs).cpu())    # 累积大量张量
    labels.append(targets)
```

**修改方案**：
```python
@torch.no_grad()
def extract_features_before_after_optimized(
    self, 
    model_before: nn.Module, 
    model_after: nn.Module, 
    data_loader: DataLoader,
    memory_limit_gb: float = 4.0  # 添加内存限制
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """优化的特征提取，减少内存峰值"""
    model_before.eval()
    model_after.eval()
    model_before.to(self.device)
    model_after.to(self.device)

    # 预分配列表，避免频繁扩展
    feats_before, feats_after, labels = [], [], []
    processed_batches = 0
    
    for batch_idx, batch in enumerate(data_loader):
        inputs = batch[0]
        targets = batch[1]
        inputs = inputs.to(self.device, non_blocking=True)
        
        try:
            # 前向传播
            feat_before = model_before(inputs)
            feat_after = model_after(inputs)
            
            # 立即转移到CPU并释放GPU内存
            feats_before.append(feat_before.cpu())
            feats_after.append(feat_after.cpu())
            labels.append(targets)
            
            processed_batches += 1
            
            # 定期清理GPU缓存
            if batch_idx % 5 == 0:  # 每5个批次清理一次
                torch.cuda.empty_cache()
            
            # 检查内存使用，如果超过限制则停止累积
            if torch.cuda.is_available():
                current_memory = torch.cuda.memory_allocated() / (1024**3)
                if current_memory > memory_limit_gb:
                    logging.warning(f"[MEMORY] Breaking extraction at batch {batch_idx} due to memory limit: {current_memory:.2f}GB")
                    break
                    
        finally:
            # 确保GPU张量被释放
            if 'feat_before' in locals():
                del feat_before
            if 'feat_after' in locals():
                del feat_after
            if 'inputs' in locals():
                del inputs
    
    # 合并张量
    result_before = torch.cat(feats_before) if feats_before else torch.tensor([])
    result_after = torch.cat(feats_after) if feats_after else torch.tensor([])
    result_labels = torch.cat(labels) if labels else torch.tensor([])
    
    logging.info(f"[FEATURES] Extracted {processed_batches} batches, {result_before.size(0)} total samples")
    
    return result_before, result_after, result_labels
```

### 3. 优化注意力补偿器内存使用

**原始问题**：
```python
# Line 62: 分块处理时产生大量中间张量
cached_eps_chunk = torch.randn(chunk_size_actual, d, device=self.device)
```

**修改方案**：
```python
@torch.no_grad()
def compensate_optimized(self, stats_dict, base_temperature=0.05, top_k=2000, n_samples=2000, chunk_size=32):
    """优化的补偿方法，减少显存峰值"""
    assert self.is_trained, "SDC 尚未训练"
    out = {}
    fb = self.features_before
    drift = self.drift_vectors
    d = fb.size(1)
    
    # 关键：大幅减少chunk_size
    chunk_size = min(chunk_size, 32)  # 从128降到32
    
    for cid, stat in stats_dict.items():
        mu = stat.mean.to(self.device)
        cov = stat.cov.to(self.device)
        
        # 均值补偿（保持不变）
        fb_norm = F.normalize(fb, dim=1)
        mu_norm = F.normalize(mu.unsqueeze(0), dim=1)
        similarities = torch.matmul(fb_norm, mu_norm.t()).squeeze(1)
        
        temperature = base_temperature
        attention_weights = F.softmax(similarities / temperature, dim=0)
        
        if top_k > 0 and top_k < len(attention_weights):
            top_k_current = min(top_k, len(attention_weights))
            top_indices = torch.topk(attention_weights, top_k_current).indices
            mask = torch.zeros_like(attention_weights)
            mask[top_indices] = attention_weights[top_indices]
            mask = mask / mask.sum()
            attention_weights = mask
        
        drift_c = torch.sum(attention_weights.unsqueeze(1) * drift, dim=0)
        
        if self.compensate_cov:
            compensated_samples = []
            
            # 分块处理，每块后立即清理内存
            for i in range(0, n_samples, chunk_size):
                chunk_end = min(i + chunk_size, n_samples)
                chunk_size_actual = chunk_end - i
                
                # 关键优化：使用较小内存的随机数生成
                with torch.no_grad():  # 确保不记录梯度
                    cached_eps_chunk = torch.randn(chunk_size_actual, d, device=self.device)
                    samples_chunk = stat.sample(cached_eps=cached_eps_chunk).to(self.device)
                
                # 分块计算相似度
                samples_norm_chunk = F.normalize(samples_chunk, dim=1)
                fb_norm_expanded = F.normalize(fb, dim=1)
                
                sample_similarities_chunk = torch.matmul(samples_norm_chunk, fb_norm_expanded.t())
                sample_attention_weights_chunk = F.softmax(sample_similarities_chunk / temperature, dim=1)
                
                if top_k > 0 and top_k < sample_attention_weights_chunk.size(1):
                    top_vals, top_indices = torch.topk(sample_attention_weights_chunk, 
                                                    min(top_k, sample_attention_weights_chunk.size(1)), 
                                                    dim=1)
                    mask = torch.zeros_like(sample_attention_weights_chunk)
                    mask.scatter_(1, top_indices, top_vals)
                    mask = mask / mask.sum(dim=1, keepdim=True)
                    sample_attention_weights_chunk = mask
                
                # 应用漂移
                compensated_chunk = samples_chunk + torch.sum(
                    sample_attention_weights_chunk.unsqueeze(2) * drift.unsqueeze(0), dim=1
                )
                
                # 立即转移到CPU
                compensated_samples.append(compensated_chunk.cpu())
                
                # 及时清理GPU内存
                del samples_chunk, compensated_chunk, sample_attention_weights_chunk
                if i % (chunk_size * 3) == 0:  # 每3个chunk清理一次
                    torch.cuda.empty_cache()
            
            # 合并所有分块
            compensated_samples = torch.cat(compensated_samples, dim=0)
            
            mu_new = compensated_samples.mean(dim=0).cpu()
            cov_new = 0.9 * torch.cov(compensated_samples.T).cpu() + 0.1 * cov.cpu()
            
        else:
            mu_new = (mu + drift_c).cpu()
            cov_new = cov.cpu()

        out[cid] = GaussianStatistics(mu_new, cov_new, stat.reg)
        
        # 清理临时变量
        del mu, cov, compensated_samples, mu_new, cov_new
        
    # 清理GPU缓存
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return out
```

## 🔧 中优先级修改

### 4. 优化弱非线性补偿器协方差计算

**修改方案**：
```python
@torch.no_grad()
def compensate_efficient(self, stats_dict, n_samples=2000):  # 从5000降到2000
    assert self.is_trained, "WeakNonlinearCompensator 尚未训练"
    device = self.device
    out = {}
    
    for cid, s in stats_dict.items():
        # 减少采样数量
        n_samples_effective = min(n_samples, s.sample_size() if hasattr(s, 'sample_size') else 1000)
        
        # 优化采样：分批处理
        batch_size = 500  # 添加批处理
        compensated_batches = []
        
        for i in range(0, n_samples_effective, batch_size):
            end_idx = min(i + batch_size, n_samples_effective)
            samples = s.sample(end_idx - i).to(device)
            transformed = self.net(samples)
            compensated_batches.append(transformed.cpu())  # 立即转移到CPU
            del samples, transformed
        
        # 合并批次
        all_transformed = torch.cat(compensated_batches, dim=0)
        mu_new = all_transformed.mean(0).cpu()
        
        # 优化的协方差计算：移至CPU
        if all_transformed.size(0) > 1:
            cov_new = torch.cov(all_transformed.T).cpu()
        else:
            cov_new = torch.eye(all_transformed.size(1)) * 1e-4
            
        out[cid] = GaussianStatistics(mu_new, cov_new, s.reg)
        
        # 清理内存
        del all_transformed, mu_new, cov_new
        torch.cuda.empty_cache()
        
    return out
```

### 5. 在DistributionCompensator主流程中添加内存管理

**修改方案**：
```python
def build_all_variants_optimized(self, ...):
    """优化的变体构建流程"""
    # ... 参数验证 ...
    
    try:
        # 记录开始时的内存使用
        if torch.cuda.is_available():
            start_memory = torch.cuda.memory_allocated() / (1024**3)
            logging.info(f"[MEMORY] Starting build with {start_memory:.2f}GB allocated")
        
        # 一次性提取所有需要的特征，避免重复计算
        current_before, current_after, current_labels, combined_before, combined_after = self._extract_combined_features_optimized(
            model_before, model_after, data_loader
        )
        
        # 在特征提取后立即清理一次
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            after_extract_memory = torch.cuda.memory_allocated() / (1024**3)
            logging.info(f"[MEMORY] After feature extraction: {after_extract_memory:.2f}GB")
        
        # 初始化特征维度缓存
        if self.feature_dim is None:
            self.feature_dim = current_after.size(1)
            # 延迟初始化缓存
            self._initialize_cached_Z(self.feature_dim)
        
        # 构建当前任务的统计量
        current_stats = self._build_gaussian_statistics(current_after, current_labels)
        
        # 更新基础变体
        self.variants["SeqFT"].update(copy.deepcopy(current_stats))
        
        # 关键：在变换计算前清理内存
        del current_before, current_after, combined_before, combined_after
        torch.cuda.empty_cache()
        
        # 更新各种变换变体
        self._update_variants_with_transforms_optimized(
            task_id, current_stats, combined_before, combined_after
        )
        
        logging.info(f"[INFO] DistributionCompensator built {len(self.variants)} variants for task {task_id}.")
        
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            logging.error("[MEMORY] OOM detected during compensation! Consider reducing batch sizes or sample counts.")
            # 尝试紧急清理
            torch.cuda.empty_cache()
            raise RuntimeError("Out of memory during distribution compensation. Try reducing memory usage.")
        else:
            raise
    
    finally:
        # 确保最后清理GPU缓存
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            final_memory = torch.cuda.memory_allocated() / (1024**3)
            logging.info(f"[MEMORY] Final memory usage: {final_memory:.2f}GB")
            
    return self.variants
```

## 📊 实施建议

### 实施顺序
1. **Week 1**: 实施修改1-2（缓存优化和特征提取优化）
2. **Week 2**: 实施修改3（注意力补偿器优化）
3. **Week 3**: 实施修改4-5（弱非线性补偿器和主流程优化）

### 测试验证
```python
def test_memory_optimization():
    """测试内存优化的效果"""
    # 记录优化前的基准
    baseline_memory = get_peak_memory()
    
    # 实施优化
    implement_optimizations()
    
    # 记录优化后的峰值
    optimized_memory = get_peak_memory()
    
    improvement = (baseline_memory - optimized_memory) / baseline_memory * 100
    logging.info(f"Memory improvement: {improvement:.1f}%")
    
    return improvement > 30  # 期望改进超过30%
```

这些修改应该能够将显存峰值从16.08GB降低到8-10GB范围内，实现30-40%的显存节省。