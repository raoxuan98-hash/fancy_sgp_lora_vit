## 代办清单

- [ ] 主要实验
- [ ] 组件消融
- [ ] 可塑性-后补偿联合作用的弥散实验
- [ ] AMDC消融
- [ ] RGDA消融

## 一、主实验（Main Experiments）方案

### 1. 基准设置（Benchmark Setup）

我们在四个广泛使用的类增量学习（Class-Incremental Learning, CIL）基准数据集上进行评估，其任务划分如下：

| 数据集     | 任务划分                                   | 初始类别数 | 每任务新增类别数 | 总任务数 |
| ---------- | ------------------------------------------ | ---------- | ---------------- | -------- |
| CIFAR-100  | 10 tasks × 10 classes                      | 10         | 10               | 10       |
| ImageNet-R | 10 tasks × 20 classes                      | 20         | 20               | 10       |
| CUB-200    | 10 tasks × 20 classes                      | 20         | 20               | 10       |
| Cars-196   | 10tasks × 20 classes + 1 task × 16 classes | 20         | 20 (last: 6)     | 10       |


> **注**：上述设置与代码库中 `main.py` 的 `smart_defaults` 函数一致，可直接复用。  
> **默认主干架构**：初始使用 `vit-b-p16-mocov3` 预训练 ViT；后续将扩展至 `vit-b-p16` 和 `vit-b-p16-clip` 等变体，以验证泛化性。

### 2. 对比方法（Baselines）

我们选取以下代表性方法进行对比：

1. **LoRA（基础版本）**：采用 `basic_lora` 配置；
2. **LoRA + 知识蒸馏**：设置 `gamma_kd=1.0`，`update_teacher_each_task=True`，`distillation_transform='identity'`；
3. **LoRA-NSP**：使用 `nsp_lora` 类型，分别测试 `nsp_weight=0.00`（退化为普通 LoRA）与 `nsp_weight=0.05`；
4. **完整方法（LoRA-SGP + RGDA + AMDC）**：我们的统一框架。

> **实验设计说明**：
>
> + 所有方法均统一使用 **RGDA 分类器** 进行评估，以确保公平比较；
> + 虽然我们也支持线性变换、弱非线性变换及 Hopfield 变换（即 AMDC）等多种分布补偿策略，但为简化主实验结果，**仅报告“无补偿”与“AMDC 补偿”两种情形**；
> + 因此，每种方法 × 两种补偿策略 = 共 **8 组结果**；
> + 对于 LoRA-SGP 中的子空间加权函数（式 (11)），默认使用 `weight_temp=1.0`、`weight_p=1.0`。初步实验表明 `weight_temp=2.0` 在部分数据集上表现更优，因此我们将对关键超参数进行**小规模网格搜索**（如 `weight_temp ∈ {1.0, 2.0, 4.0}`，`weight_p ∈ {1.0, 2.0}`），并在各数据集上报告最优配置对应的结果。

| 数据集       | weight_temp | weight_p | attention_transform + QDA |
| ------------ | ----------- | -------- | ------------------------- |
| cars196_224  | 1.0         | 1.0      | 83.41                     |
| cars196_224  | 1.0         | 2.0      | 82.89                     |
| cars196_224  | 1.0         | 4.0      | 82.54                     |
| cars196_224  | 2.0         | 2.0      | 82.5                      |
| cars196_224  | 0.5         | 4.0      | 82.94                     |
| cifar100_224 | 1.0         | 1.0      | 86.53                     |
| cifar100_224 | 1.0         | 2.0      | 86.75                     |
| cifar100_224 | 1.0         | 4.0      | 86.79                     |
| cifar100_224 | 2.0         | 2.0      | 86.84                     |
| cifar100_224 | 0.5         | 4.0      | 86.77                     |
| cub200_224   | 1.0         | 1.0      | 81.17                     |
| cub200_224   | 1.0         | 2.0      | 80.82                     |
| cub200_224   | 1.0         | 4.0      | 80.65                     |
| cub200_224   | 2.0         | 2.0      | 80.55                     |
| cub200_224   | 0.5         | 4.0      | 80.98                     |
| imagenet-r   | 1.0         | 1.0      | 74.62                     |
| imagenet-r   | 1.0         | 2.0      | 74.43                     |
| imagenet-r   | 1.0         | 4.0      | 74.23                     |
| imagenet-r   | 2.0         | 2.0      | 74.08                     |
| imagenet-r   | 0.5         | 4.0      | 74.35                     |


> 由上面的网格搜索可以看出，目前比较好的实验性能是weight_temp = 1.0，weight_p = 1.0。在实验中，我发现weight_temp = 1.0, weight_p = 1.0，补偿 + QDA效果不错，但是不补偿 + QDA效果一般；当weight_temp = 2.0, weight_p = 2.0时，补偿 + QDA效果不是最佳，但是不补偿 + QDA效果更好了，因为稳定性的提高。

### 3. 评估指标

采用以下两个核心指标：

+ **Last-Acc**：模型完成所有任务后，在全部类别上的平均分类准确率；
+ **Avg-Acc**：模型在每个任务结束后在所有已见类别上的准确率的平均值，反映整体稳定性。

---

## 二、消融研究（Ablation Study）方案

###  组件消融（Component-wise Ablation）

| 变体        | LoRA-SGP | AMDC | 实现方式说明          |
| ----------- | -------- | ---- | --------------------- |
| Full Method | ✅        | ✅    | 默认配置              |
| w/o SGP     | ❌        | ✅    | 替换为 `basic_lora`   |
| w/o AMDC    | ✅        | ❌    | 禁用分布补偿          |
| w/o Both    | ❌        | ❌    | `basic_lora` + 无补偿 |


> **注意**：仅当微调策略变更（如 `sgp_lora` → `basic_lora`）时需重新训练；否则可在同一训练轨迹下同时报告不同补偿/分类器策略的结果。  
> 如需对比 SGD-based 分类器，可在 `classifier_builder.py` 中设置 `classifier_type = ['sgd', 'lda', 'qda']`，系统将自动输出多类分类器结果。

### LoRA-SGP和AMDC的联合实验：研究可塑性与后补偿之间互补影响

+ 动机：后补偿可以弥补可塑性提高的负面影响，因此对于补偿与不补偿，QDA的效果不同。我们要定量研究可塑性-稳定性的变化对分类器的影响。
+ 在实验中，我发现weight_temp = 1.0, weight_p = 1.0，补偿 + QDA效果不错，但是不补偿 + QDA效果一般；当weight_temp = 2.0, weight_p = 2.0时，补偿 + QDA效果不是最佳，但是不补偿 + QDA效果更好了，因为稳定性的提高。

### LoRA-SGP 超参数消融

![权重函数对不同奇异值下的奇异方向信息的“过滤作用”](https://cdn.nlark.com/yuque/0/2025/png/57388852/1762315395949-fa8648ea-7484-4da8-9a58-6ad174afb703.png)

**实验 1：权重函数敏感性分析**  

+ 测试 `weight_temp ∈ {1.0, 2.0, 4.0}` 和 `weight_p ∈ {1.0, 2.0}` 对性能的影响；
+ 预期不同数据集对超参数敏感性存在差异，需分别调优。

**实验 2：权重函数形式对比（Weight Kind Ablation）**  

+ 对比不同 `weight_kind`（如对数衰减、指数衰减等）对稳定性–可塑性权衡的影响；
+ **为控制实验成本**，仅在 ImageNet-R 与 Cars-196 上进行（一为大尺度通用数据，一为细粒度场景）；
+ **统一报告有/无 AMDC 时的性能**，以体现模块协同效应。

> **呈现建议**：采用热力图或分组柱状图，清晰展示超参数–性能关系；避免堆砌表格。

###  AMDC 消融实验

#### (a) AMDC 内部机制分析

+ **仅补偿均值**（Fix Σ, update μ）
+ **仅补偿协方差**（Fix μ, update Σ）
+ **同时补偿均值与协方差**（Full AMDC）
+ **不同注意力温度 τ 的影响**

#### (c) 与其他补偿方法对比

+ 对比 **Linear / WeakNonlinear / Nonlinear Transform** 与 **AMDC** 的：
  - **精度表现**（已在主实验中部分覆盖）；
  - **计算效率**：固定类别数（如 50, 100, 150, 200），测量分布补偿阶段的运行时间，绘制 **类别数–时间曲线或者类别数-时间柱状图**，突显 AMDC 的训练无关（training-free）优势。

![画板](https://cdn.nlark.com/yuque/0/2025/jpeg/57388852/1762316876844-b5fbc399-4dea-4948-8ccc-5008e45fefaf.jpeg)

#### （c）AMDC对其他方法的促进作用

+ 对LoRA的促进
+ 对NSP_LoRA的促进
+ 对LoRA + Distillation的促进

> 这个在主实验应该就可以体现了

###  RGDA 消融实验

+ **超参数敏感性**：分析正则化系数 α₁, α₂, α₃ 的影响；

![画板](https://cdn.nlark.com/yuque/0/2025/jpeg/57388852/1762328255542-bf52ef24-e001-4838-b3bf-6b6486b3c3da.jpeg)

+ **分类器对比**：与以下基线比较：
  - Nearest Class Mean (NCM)
  - Linear Discriminant Analysis (LDA)
  - Quadratic Discriminant Analysis (QDA)
  - SGD-based refinement（需额外训练）

## 三、补充实验（Supplementary Experiments）

### 1. 长序列任务实验（Long-Sequence CIL）

+ 将 CIFAR-100 划分为 **20 个任务**（每任务 5 类），验证方法在 **长期增量学习** 中的稳定性与抗遗忘能力；
+ 特别关注 **Avg-Acc 的衰减速率** 与 **最终性能下限**。

### 2. 跨架构泛化性（Cross-Architecture Generalization）

+ 在相同任务设置下，切换主干 ViT 的预训练来源（MoCo v3 / DINO / CLIP）；
+ 验证 LoRA-SGP + RGDA + AMDC 框架是否对 **不同预训练特征空间** 具有鲁棒适应能力。