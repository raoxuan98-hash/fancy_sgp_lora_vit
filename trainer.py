import os
import sys
import logging
import torch
import random
import numpy as np
from collections.abc import Mapping, Sequence
# from models.subspace_lora import SubspaceLoRA
from models.subspace_lora import SubspaceLoRA
from utils.data_manager import DataManager
from utils.toolkit import count_parameters
import re

def train(args):
    all_results = {}
    
    for run_id, seed in enumerate(args['seed_list']):
        args['seed'], args['run_id'] = seed, run_id
        logfile_head, logfile_name = build_log_dirs(args)
        args['log_path'] = logfile_name
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(filename)s] => %(message)s',
            handlers=[
                logging.FileHandler(filename=os.path.join(logfile_name, 'record.log')),
                logging.StreamHandler(sys.stdout)])
        
        args['log_path'] = logfile_name
        results = train_single_run(args)
        all_results[f"seed_{seed}"] = results
    aggregated = aggregate_seed_results(all_results)
    return {
        'seeds': all_results,
        'aggregate': aggregated,
    }

def train_single_run(args, return_model: bool = False):
    # Setting random seed and device for reproducibility
    set_random(args['seed'])
    print_args(args)
    
    # Initialize data manager and model
    data_manager = DataManager(
        dataset_name=args['dataset'],
        shuffle=args['shuffle'],
        seed=args['seed'],
        init_cls=args['init_cls'],
        increment=args['increment'])
    
    model = SubspaceLoRA(args)
    logging.info(f'All params: {count_parameters(model.network)}')
    logging.info(f'Trainable params: {count_parameters(model.network, True)}')
    final_results = model.loop(data_manager)
    if return_model:
        return final_results, model
    return final_results


def Bayesian_evaluate(args):
    """
    Similar to `train_single_run`, but evaluates the model every 5 tasks and returns the evaluation result.
    
    Args:
        args: Configuration arguments (same as in train_single_run)
        data_manager: DataManager object that handles datasets and task splits
    
    Yields:
        Task results after every 5 tasks for evaluation.
    """
    # Setting random seed and device for reproducibility
    set_random(args['seed'])
    device = set_device(args['device'])
    args['device'] = device

    print_args(args)

    logfile_head, logfile_name = build_log_dirs(args)
    args['log_path'] = logfile_name

    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(filename)s] => %(message)s',
        handlers=[
            logging.FileHandler(filename=os.path.join(logfile_name, 'record.log')),
            logging.StreamHandler(sys.stdout)
        ]
    )


    # Initialize data manager and model
    data_manager = DataManager(
        dataset_name=args['dataset'],
        shuffle=args['shuffle'],
        seed=args['seed'],
        init_cls=args['init_cls'],
        increment=args['increment'])
    
    model = SubspaceLoRA(args)
    logging.info(f'All params: {count_parameters(model.network)}')
    logging.info(f'Trainable params: {count_parameters(model.network, True)}')

    # Initialize result storage
    task_results = {
        "original_fc": [],
        "linear_fc": []}
    
    model._eval_tasks = model._compute_eval_milestones(data_manager.nb_tasks)

    logging.info(f"Classifier refinement scheduled at tasks: {sorted(model._eval_tasks)}")

    model.data_manager = data_manager
    # Train and evaluate in tasks

    for task_id in range(data_manager.nb_tasks):
        # Incremental training
        model.incremental_train(data_manager)
        if (model._cur_task + 1) in [5, 10]:
            model.refine_classifiers()
            # logging.info(f"Evaluating after task {model._cur_task}...")
            eval_result = model.eval_task()
            # Store the evaluation results
            task_results["original_fc"].append(eval_result.original_fc)
            task_results["linear_fc"].append(eval_result.linear_fc)
            # Yield evaluation results after every 5 tasks
            logging.info(f"Evaluation after task {task_id + 1} -> Original FC: {eval_result.original_fc:.2f}% | Compensated: {eval_result.linear_fc:.2f}%")
            
            if (model._cur_task + 1) == 5:
                flag = 0
            elif (model._cur_task + 1) == 10:
                flag = 1
            yield task_results, flag

        model.after_task()
    # Return the aggregated task results after all tasks
    return task_results

def set_device(device_type):
    """Properly set the device (either CPU or GPU) based on input"""
    if isinstance(device_type, (list, tuple)):
        return [torch.device(f'cuda:{d}' if d != -1 else 'cpu') for d in device_type]
    return torch.device('cuda' if device_type != -1 else 'cpu')

def set_random(seed):
    """Set random seeds to ensure reproducibility"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def print_args(args):
    """Log the arguments for this run"""
    for key, value in args.items():
        logging.info(f'{key}: {value}')

# --------- NEW: compact float â†’ short string ----------
def _fmt(x, *, digits=4):
    """
    å‹ç¼©æ•°å€¼åˆ°çŸ­å­—ç¬¦ä¸²ï¼š0.5 -> 0p5, 1e-3 -> 1e-03, 0.200 -> 0p2
    ä½œç”¨ï¼šå‡å°‘è·¯å¾„é•¿åº¦ã€é¿å…å°æ•°ç‚¹è¿‡å¤šã€‚
    """
    if isinstance(x, bool):
        return "1" if x else "0"
    if isinstance(x, int):
        return str(x)
    try:
        s = f"{float(x):.{digits}g}"
        s = s.replace('.', 'p')
        return s
    except Exception:
        s = str(x)
        s = s.replace('.', 'p')
        return s

from pathlib import Path
import os

from pathlib import Path
import os

import os
from pathlib import Path
import hashlib
import json

def _filter_args_by_lora_type(args: dict) -> dict:
    """
    è¿‡æ»¤å‚æ•°å­—å…¸ï¼Œåªä¿ç•™ä¸å½“å‰LoRAç±»å‹ç›¸å…³çš„å‚æ•°
    è¿™æ ·å¯ä»¥é¿å…åœ¨params.jsonä¸­ä¿å­˜ä¸ç›¸å…³çš„å‚æ•°ï¼Œå¯¼è‡´æ—¥å¿—å‘½åæ··ä¹±
    """
    lora_type = args.get('lora_type', 'basic_lora')
    filtered_args = args.copy()
    
    # å®šä¹‰æ¯ç§LoRAç±»å‹ç›¸å…³çš„å‚æ•°
    sgp_lora_params = {'weight_temp', 'weight_kind', 'weight_p'}
    nsp_lora_params = {'nsp_eps', 'nsp_weight'}
    
    # ç§»é™¤ä¸å½“å‰LoRAç±»å‹ä¸ç›¸å…³çš„å‚æ•°
    if lora_type == 'sgp_lora':
        # ä¿ç•™SGPå‚æ•°ï¼Œç§»é™¤NSPå‚æ•°
        for param in nsp_lora_params:
            filtered_args.pop(param, None)
    elif lora_type == 'nsp_lora':
        # ä¿ç•™NSPå‚æ•°ï¼Œç§»é™¤SGPå‚æ•°
        for param in sgp_lora_params:
            filtered_args.pop(param, None)
    elif lora_type == 'basic_lora':
        # ç§»é™¤æ‰€æœ‰LoRAç‰¹å®šå‚æ•°
        for param in sgp_lora_params.union(nsp_lora_params):
            filtered_args.pop(param, None)
    elif lora_type == 'full':
        # ç§»é™¤æ‰€æœ‰LoRAç‰¹å®šå‚æ•°
        for param in sgp_lora_params.union(nsp_lora_params):
            filtered_args.pop(param, None)
    
    return filtered_args

def build_log_dirs(args: dict, root_dir="."):
    """æ ¹æ® args æ„å»ºå¤šçº§æ—¥å¿—ç›®å½•ï¼Œç¡®ä¿ä¸åŒ LoRA ç±»å‹çš„å‚æ•°æ­£ç¡®åˆ†ç¦»"""

    def sanitize_filename(s: str) -> str:
        """ç§»é™¤æˆ–æ›¿æ¢æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
        # Windows éæ³•å­—ç¬¦: \ / : * ? " < > |
        s = re.sub(r'[\\/:*?"<>|]', '_', str(s))
        # å¯é€‰ï¼šå‹ç¼©è¿ç»­ä¸‹åˆ’çº¿
        s = re.sub(r'_+', '_', s)
        return s.strip('_')

    def short(s: str, maxlen=40):
        """æˆªæ–­è¿‡é•¿å­—ç¬¦ä¸²ï¼Œä¸åŠ  hashï¼Œä»…ä¿ç•™å¯è¯»æ€§"""
        s = sanitize_filename(str(s))
        if len(s) <= maxlen:
            return s
        return s[:maxlen].rstrip('_')  # é¿å…æˆªæ–­åœ¨ä¸‹åˆ’çº¿å¤„

    def _get_lora_specific_params(lora_type: str, args: dict) -> list:
        """è·å–ç‰¹å®š LoRA ç±»å‹çš„å‚æ•°ï¼Œé¿å…äº¤å‰æ±¡æŸ“"""
        params = []
        
        if lora_type == 'sgp_lora':
            # SGP LoRA ç‰¹æœ‰å‚æ•°
            if 'weight_temp' in args:
                params.append(f"t-{short(args['weight_temp'])}")
            if 'weight_kind' in args:
                params.append(f"k-{short(args['weight_kind'])}")
            if 'weight_p' in args:
                params.append(f"p-{short(args['weight_p'])}")
                
        elif lora_type == 'nsp_lora':
            # NSP LoRA ç‰¹æœ‰å‚æ•°
            if 'nsp_eps' in args:
                params.append(f"eps-{short(args['nsp_eps'])}")
            if 'nsp_weight' in args:
                params.append(f"w-{short(args['nsp_weight'])}")
                
        elif lora_type == 'basic_lora':
            # Basic LoRA é€šå¸¸æ²¡æœ‰é¢å¤–å‚æ•°ï¼Œä½†å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
            pass
            
        elif lora_type == 'full':
            # Full fine-tuning å¯èƒ½æœ‰çš„å‚æ•°
            pass
            
        return params

    def _get_kd_params(args: dict) -> list:
        """è·å–çŸ¥è¯†è’¸é¦ç›¸å…³å‚æ•°ï¼Œç»Ÿä¸€å‘½åè§„åˆ™"""
        kd_params = []
        
        if args.get('gamma_kd', 0.0) > 0.0:
            kd_params.append(f"kd-{short(args['gamma_kd'])}")
            if 'kd_type' in args:
                kd_params.append(f"type-{short(args['kd_type'])}")
            if 'distillation_transform' in args:
                kd_params.append(f"dt-{short(args['distillation_transform'])}")
            if args.get('use_aux_for_kd', False):
                kd_params.append("aux")
            # æ·»åŠ update_teacher_each_taskå‚æ•°ï¼Œç®€å†™ä¸ºutt
            if 'update_teacher_each_task' in args:
                kd_params.append(f"utt-{short(args['update_teacher_each_task'])}")
                
        return kd_params

    def _validate_parameters(args: dict) -> None:
        """éªŒè¯å‚æ•°ç»„åˆçš„åˆç†æ€§"""
        lora_type = args.get('lora_type', 'basic_lora')
        
        # æ£€æŸ¥ LoRA ç‰¹å®šå‚æ•°æ˜¯å¦è¢«è¯¯ç”¨
        if lora_type != 'sgp_lora':
            sgp_params = ['weight_temp', 'weight_kind', 'weight_p']
            for param in sgp_params:
                if param in args and args[param] is not None:
                    logging.warning(f"âš ï¸ Parameter '{param}' is being used with lora_type='{lora_type}', but it's specific to sgp_lora")
        
        if lora_type != 'nsp_lora':
            nsp_params = ['nsp_eps', 'nsp_weight']
            for param in nsp_params:
                if param in args and args[param] is not None:
                    logging.warning(f"âš ï¸ Parameter '{param}' is being used with lora_type='{lora_type}', but it's specific to nsp_lora")

    # å‚æ•°éªŒè¯
    _validate_parameters(args)

    # é¡¶å±‚ï¼šæ¨¡å‹å’Œç”¨æˆ·ä¿¡æ¯
    base_dir = os.path.join(
        root_dir,
        f"{short(args['model_name'])}_logs_{short(args['user'])}",
        f"{short(args['dataset'])}_{short(args['vit_type'])}"
    )

    # äºŒçº§ï¼šä»»åŠ¡è®¾ç½®
    task_dir = os.path.join(
        base_dir,
        f"init-{short(args['init_cls'])}_inc-{short(args['increment'])}",
        f"lrank-{short(args.get('lora_rank', 'NA'))}_ltype-{short(args.get('lora_type', 'NA'))}"
    )

    # ä¸‰çº§ï¼šLoRA ç‰¹å®šå‚æ•°ï¼ˆåªåŒ…å«ç›¸å…³çš„ï¼‰
    lora_params = _get_lora_specific_params(args.get('lora_type', 'basic_lora'), args)
    
    # å››çº§ï¼šçŸ¥è¯†è’¸é¦å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
    kd_params = _get_kd_params(args)
    
    # åˆå¹¶ LoRA å’Œ KD å‚æ•°
    method_params = lora_params + kd_params
    
    # æ„å»ºæ–¹æ³•å‚æ•°ç›®å½•
    if method_params:
        method_subdir = "_".join(method_params)
        method_dir = os.path.join(task_dir, short(method_subdir, maxlen=80))
    else:
        method_dir = task_dir

    # äº”çº§ï¼šä¼˜åŒ–å™¨å’Œè®­ç»ƒå‚æ•°
    opt_params = [
        f"opt-{args['optimizer']}",
        f"lr-{short(args['lrate'])}",
        f"b-{args['batch_size']}",
        f"i-{args['iterations']}",
        f"s-{args['seed']}"
    ]
    opt_str = "_".join(opt_params)
    opt_dir = os.path.join(method_dir, short(opt_str, maxlen=80))

    # === é€çº§åˆ›å»ºç›®å½• ===
    abs_log_dir = os.path.abspath(opt_dir)
    current = Path(abs_log_dir).root
    for part in Path(abs_log_dir).parts[1:]:
        current = Path(current) / part
        current.mkdir(exist_ok=True)

    # ä¿å­˜è¿‡æ»¤åçš„å‚æ•°åˆ° JSONï¼Œé¿å…å‚æ•°äº¤å‰æ±¡æŸ“
    filtered_args = _filter_args_by_lora_type(args)
    params_json = Path(abs_log_dir) / "params.json"
    if not params_json.exists():
        with open(params_json, "w", encoding="utf-8") as f:
            json.dump(filtered_args, f, ensure_ascii=False, indent=2)

    # è®°å½•ç”Ÿæˆçš„ç›®å½•ç»“æ„ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    logging.info(f"ğŸ“ Log directory created: {abs_log_dir}")
    logging.info(f"   LoRA params: {lora_params}")
    logging.info(f"   KD params: {kd_params}")
    
    # è®°å½•è¿‡æ»¤ä¿¡æ¯
    original_params = set(args.keys())
    filtered_params = set(filtered_args.keys())
    removed_params = original_params - filtered_params
    if removed_params:
        logging.info(f"   è¿‡æ»¤æ‰çš„å‚æ•°: {sorted(removed_params)}")

    return os.path.dirname(abs_log_dir), str(abs_log_dir)

    
def aggregate_seed_results(seed_results):
    """Aggregate evaluation statistics from multiple random seeds."""

    if isinstance(seed_results, Mapping):
        records = list(seed_results.values())
    elif isinstance(seed_results, Sequence) and not isinstance(seed_results, (str, bytes)):
        records = list(seed_results)
    else:
        records = [seed_results]

    if not records:
        logging.warning("âš ï¸ No seed results provided for aggregation.")
        return {"final_task": {}, "average_across_tasks": {}}

    # Collect all variant names across all seeds
    all_variants = set()
    for res in records:
        all_variants.update(res.get("last_task_accuracies", {}).keys())
        all_variants.update(res.get("average_accuracies", {}).keys())
    all_variants = sorted(all_variants)

    # Initialize containers
    final_task_values = {variant: [] for variant in all_variants}
    avg_task_values = {variant: [] for variant in all_variants}

    # Populate with data from each seed
    for res in records:
        last_acc = res.get("last_task_accuracies", {})
        avg_acc = res.get("average_accuracies", {})

        for variant in all_variants:
            final_task_values[variant].append(last_acc.get(variant, 0.0))
            avg_task_values[variant].append(avg_acc.get(variant, 0.0))

    # Compute mean and std
    final_task_stats = {}
    avg_task_stats = {}

    if not all_variants:
        logging.warning("âš ï¸ No accuracy statistics found in seed results.")
        return {
            "final_task": final_task_stats,
            "average_across_tasks": avg_task_stats,
        }

    for variant in all_variants:
        f_vals = np.array(final_task_values[variant])
        a_vals = np.array(avg_task_values[variant])

        final_task_stats[variant] = (float(np.mean(f_vals)), float(np.std(f_vals)))
        avg_task_stats[variant] = (float(np.mean(a_vals)), float(np.std(a_vals)))

    # === ğŸ“Š Log Aggregated Results ===
    logging.info("ğŸ“ˆ Aggregated Results Across Random Seeds:")
    logging.info("  â”€â”€ Final Task Accuracy (Mean Â± Std) â”€â”€")
    for variant in all_variants:
        mean, std = final_task_stats[variant]
        logging.info(f"      {variant:<20} : {mean:.2f}% Â± {std:.2f}%")

    logging.info("  â”€â”€ Average Accuracy Across Tasks (Mean Â± Std) â”€â”€")
    for variant in all_variants:
        mean, std = avg_task_stats[variant]
        logging.info(f"      {variant:<20} : {mean:.2f}% Â± {std:.2f}%")

    # === ğŸ—‚ï¸ SAVE AGGREGATED RESULTS TO FILE ===
    # ä¿å­˜èšåˆç»“æœåˆ°JSONæ–‡ä»¶
    import time
    
    # å°è¯•ä»ç¬¬ä¸€ä¸ªç§å­çš„ç»“æœä¸­è·å–log_path
    if isinstance(seed_results, Mapping) and len(seed_results) > 0:
        first_seed_key = list(seed_results.keys())[0]
        first_seed_result = seed_results[first_seed_key]
        
        # æŸ¥æ‰¾log_path
        log_path = None
        if isinstance(first_seed_result, dict) and 'log_path' in first_seed_result:
            log_path = first_seed_result['log_path']
        elif isinstance(first_seed_result, dict) and 'per_task_results' in first_seed_result:
            # å°è¯•ä»å­ç»“æ„ä¸­æŸ¥æ‰¾
            for key, value in first_seed_result.items():
                if isinstance(value, dict) and 'log_path' in value:
                    log_path = value['log_path']
                    break
        
        if log_path:
            log_dir = Path(log_path).parent
            aggregate_file = log_dir / "aggregate_results.json"
            
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®
            save_data = {
                "final_task_stats": {k: {"mean": v[0], "std": v[1]} for k, v in final_task_stats.items()},
                "average_across_tasks_stats": {k: {"mean": v[0], "std": v[1]} for k, v in avg_task_stats.items()},
                "seed_list": list(seed_results.keys()),
                "num_seeds": len(seed_results),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "variants": all_variants
            }
            
            with open(aggregate_file, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, indent=2, ensure_ascii=False)
            
            logging.info(f"ğŸ’¾ Aggregated results saved to: {aggregate_file}")
        else:
            logging.warning("âš ï¸ Could not find log_path for saving aggregated results.")
    else:
        logging.warning("âš ï¸ No seed results available for saving aggregated results.")

    # Return structured stats
    return {
        "final_task": final_task_stats,
        "average_across_tasks": avg_task_stats,
    }
